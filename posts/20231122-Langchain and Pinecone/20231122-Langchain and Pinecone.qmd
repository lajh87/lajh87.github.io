---
title: "Using Vector Store Database"
description: "An example using pinecone and langchain"
author: "Luke Heley"
date: "22 Nov 23"
freeze: true
execute:
  eval: true
format:
  html:
    toc: true
    code-fold: show
categories:
 - large language models (LLM)
 - evidence synthesis
 - methods
 - notes
editor_options:
  chunk_output_type: console
---

# Introduction

https://docs.pinecone.io/docs/langchain

# Build the Knowledge Base

```{python}
import os
import sys

from langchain.document_loaders import PyPDFLoader
f = open("sources.txt", "r")
urls = f.readlines()
f.close()

newurls = []
for url in urls:
  newurls.append(url.replace("\n", ""))

urls = newurls
documents = []
for url in urls:
  print(url)
  documents.extend(PyPDFLoader(url).load())

print("documents:", len(urls))
print("pages:", len(documents))

```

Documents are split by pages which contain a lot of text. Our first task is therefore to identify a good preprocessing method for chunking these articles into "concise" chunks to later be embedded and stored into the pinecone database.

```{python}
import tiktoken

tiktoken.encoding_for_model('gpt-3.5-turbo')
tokenizer = tiktoken.get_encoding('cl100k_base')

# create the length function
def tiktoken_len(text):
    tokens = tokenizer.encode(
        text,
        disallowed_special=()
    )
    return len(tokens)

tiktoken_len("hello I am a chunk of text and using the tiktoken_len function "
             "we can find the length of this chunk of text in tokens")

```

```{python}
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=20,
    length_function=tiktoken_len,
    separators=["\n\n", "\n", " ", ""]
)

docs = text_splitter.split_documents(documents)

print("Length of pages: ", len(documents))
print("Length of splits: ", len(docs))

```

# Create Embeddings

```{python}
from langchain.embeddings.openai import OpenAIEmbeddings

model_name = 'text-embedding-ada-002'

embed = OpenAIEmbeddings(
    model=model_name,
    openai_api_key=os.getenv('OPENAI_API_KEY') 
)
```

```{python}
texts = [
    'this is the first chunk of text',
    'then another second chunk of text is here'
]

res = embed.embed_documents(texts)
len(res), len(res[0])
```

# Vector Database

```{python}

import pinecone

index_name = 'langchain-retrieval-augmentation'
pinecone.init(
    api_key=os.getenv("PINECONE_API_KEY"),
    environment="gcp-starter"
)

if index_name not in pinecone.list_indexes():
    # we create a new index
    pinecone.create_index(
        name=index_name,
        metric='cosine',
        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002
    )
```

Connect to the new index

```{python}
index = pinecone.GRPCIndex(index_name)
index.describe_index_stats()
```

```{python}
from tqdm.auto import tqdm
from uuid import uuid4

batch_limit = 100

texts = []
metadatas = []

for i, record in enumerate(tqdm(documents)):
    # first get metadata fields for this record
    metadata = record.metadata
    # now we create chunks from the record text
    record_texts = text_splitter.split_text(record.page_content)
    # create individual metadata dicts for each chunk
    record_metadatas = [{
        "chunk": j, "text": text, **metadata
    } for j, text in enumerate(record_texts)]
    # append these to current batches
    texts.extend(record_texts)
    metadatas.extend(record_metadatas)
    # if we have reached the batch_limit we can add texts
    if len(texts) >= batch_limit:
        ids = [str(uuid4()) for _ in range(len(texts))]
        embeds = embed.embed_documents(texts)
        index.upsert(vectors=zip(ids, embeds, metadatas))
        texts = []
        metadatas = []

if len(texts) > 0:
    ids = [str(uuid4()) for _ in range(len(texts))]
    embeds = embed.embed_documents(texts)
    index.upsert(vectors=zip(ids, embeds, metadatas))
```

```{python}
index.describe_index_stats()
```

```{python}
from langchain.vectorstores import Pinecone

text_field = "text"

# switch back to normal index for langchain
index = pinecone.Index(index_name)

vectorstore = Pinecone(
    index, embed.embed_query, text_field
)
```

```{python}
query = "Provide an example of a major project"

vectorstore.similarity_search(
    query,  # our search query
    k=3  # return 3 most relevant docs
)
```

# Generative Question Answering

```{python}
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# completion llm
llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    model_name='gpt-3.5-turbo',
    temperature=0.0
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)
```

```{python}
qa.run(query)
```

We can also include the sources of information that the LLM is using to answer our question. We can do this using a slightly different version of `RetrievalQA` called `RetrievalQAWithSourcesChain`:

```{python}
from langchain.chains import RetrievalQAWithSourcesChain

qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

qa_with_sources(query)
```
