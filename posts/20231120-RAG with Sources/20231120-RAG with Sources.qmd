---
title: "Retrieval Augmented Generation with Multiple PDF and Citation"
description: "An applied example using langchain"
author: "Luke Heley"
date: "20 Nov 23"
freeze: true
execute:
  eval: true
format:
  html:
    toc: true
    code-fold: show
categories:
 - large language models (LLM)
 - evidence synthesis
 - methods
 - notes
editor_options:
  chunk_output_type: console
---


# Introduction

https://python.langchain.com/docs/use_cases/question_answering/

# Build the Knowledge Base

```{python}

import os
import sys

from langchain.document_loaders import PyPDFLoader

pdf_url = "https://www.nao.org.uk/wp-content/uploads/2015/10/Major-Projects-Report-2015-and-the-Equipment-Plan-2015-2025.pdf" # also allowed is a file path e.g. /home/downloads/xyz.pdf
url2 = "https://www.nao.org.uk/wp-content/uploads/2010/10/1011489_I.pdf"
urls = [url2, pdf_url]

documents = []
for url in urls:
        documents.extend(PyPDFLoader(url).load())


```

# Chunk 

```{python}
# Split
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1500,
    chunk_overlap = 150
)

docs = text_splitter.split_documents(documents)

print("Length of pages: ", len(documents))
print("Length of splits: ", len(docs))
```

# Embed and Store

```{python}
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma

embeddings = OpenAIEmbeddings()

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embeddings,
    persist_directory="data"
)

print(vectorstore._collection.count()) 
```

# Retrieve

```{python}
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 6})
retrieved_docs = retriever.get_relevant_documents(
    "What are the causes of schedule variation?"
)

len(retrieved_docs)
print(retrieved_docs[0].page_content)

```

# Generate

```{python}
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

from langchain import hub
prompt = hub.pull("rlm/rag-prompt")

from langchain.schema import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

for chunk in rag_chain.stream("What are the causes of schedule variation?"):
    print(chunk, end="", flush=True)

```

# Custom Prompt

```{python}
from langchain.prompts import PromptTemplate

template = """Use the following pieces of context to answer the question at the end. 
If you don't know the answer, just say that you don't know, don't try to make up an answer. 
Use three sentences maximum and keep the answer as concise as possible. 
Always say "thanks for asking!" at the end of the answer. 
{context}
Question: {question}
Helpful Answer:"""
rag_prompt_custom = PromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | rag_prompt_custom
    | llm
    | StrOutputParser()
)

rag_chain.invoke("What are the causes of schedule variation?")

```

# Adding Sources

```{python}
from operator import itemgetter
from langchain.schema.runnable import RunnableMap

rag_chain_from_docs = (
    {
        "context": lambda input: format_docs(input["documents"]),
        "question": itemgetter("question"),
    }
    | rag_prompt_custom
    | llm
    | StrOutputParser()
)

rag_chain_with_source = RunnableMap(
    {"documents": retriever, "question": RunnablePassthrough()}
) | {
    "documents": lambda input: [doc.metadata for doc in input["documents"]],
    "answer": rag_chain_from_docs,
}

rag_chain_with_source.invoke("What are the main causes of schedule variation in MOD?")


```
