{
  "hash": "d7b3552dd5aa3e3e9eb068536c8f82a7",
  "result": {
    "markdown": "---\ntitle: \"Regression Discontinuity Design\"\ndescription: \"Notes on Regression Discontinuity Design\"\nauthor: \"Luke Heley\"\ndate: \"18 June 2023\"\nfreeze: true\nexecute:\n  eval: true\nformat:\n  html:\n    toc: true\n    code-fold: true\ncategories:\n - evaluation\n - quasi-experimental\n - methods\n - notes\neditor_options:\n  chunk_output_type: console\nbibliography: references.bib\n---\n\n```{=html}\n<meta name=\"robots\" content=\"noindex, nofollow\" />\n```\n\n\n# What is it\n\nThis method estimates the impact of an intervention by using a cut-off threshold to assign the intervention. The method relies on the assumption that the individuals just below and above a cutoff threshold will be similar, with the only significant difference between the 2 groups being whether they received the intervention or not. By comparing the value of the outcome variable for the individuals just above and below the cut-off threshold, the method infers the impact of the intervention [@mb2020].\n\n# How is it used?\n\nThe method was first used to estimate the impact of a merit-based scholarship scheme by @tc1960. Simply comparing the attainment grades of students who were given the scholarship versus those who didn't would lead to a biased estimate of the impact of the scheme, because more able students were given the scholarship so would likely have better attainment whether they received the scholarship or not. To overcome this, the students who were only just eligible for the scholarship (who only just passed the scholarship test) were compared with those who only just missed the cut-off grade for the scholarship award. These \"only just\" unsuccessful students were very similar in their ability to those who were \"only just\" successful so they made a good comparison group. By comparing the outcomes of both groups in later years, the impact of the scholarship could be estimated [@mb2020].\n\n# What are the pros and cons?\n\nPros:\n\n-   Can be useful method where randomisation is not possible in the intervention design.\n\nCons:\n\n-   The method only allows estimation of the impact of the intervention for individuals close to the cut-off (local average treatment effect). The effect of the intervention may differ for those individuals further away from the cut-off.\n-   As with other quasi-experimental methods, many assumptions must be made in order to assert attribution [@mb2020].\n\n# Example \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://mixtape.scunning.com/06-regression_discontinuity\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\n# simulate the data\ndat <- tibble(\n  x = rnorm(1000, 50, 25)\n) %>%\n  mutate(\n    x = if_else(x < 0, 0, x)\n  ) %>%\n  filter(x < 100)\n\n# cutoff at x = 50\ndat <- dat %>% \n  mutate(\n    D  = if_else(x > 50, 1, 0),\n    y1 = 25 + 0 * D + 1.5 * x + rnorm(n(), 0, 20)\n  )\n\nggplot(aes(x, y1, colour = factor(D)), data = dat) +\n  geom_point(alpha = 0.5) +\n  geom_vline(xintercept = 50, colour = \"grey\", linetype = 2)+\n  stat_smooth(method = \"lm\", se = F) +\n  labs(x = \"Test score (X)\", y = \"Potential Outcome (Y1)\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](rdd_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# rdd simluate 2\n\n# simulate the discontinuity\ndat <- dat %>%\n  mutate(\n    y2 = 25 + 40 * D + 1.5 * x + rnorm(n(), 0, 20)\n  )\n\n# figure\nggplot(aes(x, y2, colour = factor(D)), data = dat) +\n  geom_point(alpha = 0.5) +\n  geom_vline(xintercept = 50, colour = \"grey\", linetype = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  labs(x = \"Test score (X)\", y = \"Potential Outcome (Y)\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](rdd_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# rdd simulate 3\n#simultate nonlinearity\ndat <- tibble(\n  x = rnorm(1000, 100, 50)\n) %>% \n  mutate(\n    x = case_when(x < 0 ~ 0, TRUE ~ x),\n    D = case_when(x > 140 ~ 1, TRUE ~ 0),\n    x2 = x*x,\n    x3 = x*x*x,\n    y3 = 10000 + 0 * D - 100 * x + x2 + rnorm(1000, 0, 1000)\n  ) %>% \n  filter(x < 280)\n\n\nggplot(aes(x, y3, colour = factor(D)), data = dat) +\n  geom_point(alpha = 0.2) +\n  geom_vline(xintercept = 140, colour = \"grey\", linetype = 2) +\n  stat_smooth(method = \"lm\", se = F) +\n  labs(x = \"Test score (X)\", y = \"Potential Outcome (Y)\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](rdd_files/figure-html/unnamed-chunk-1-3.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(aes(x, y3, colour = factor(D)), data = dat) +\n  geom_point(alpha = 0.2) +\n  geom_vline(xintercept = 140, colour = \"grey\", linetype = 2) +\n  stat_smooth(method = \"loess\", se = F) +\n  labs(x = \"Test score (X)\", y = \"Potential Outcome (Y)\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](rdd_files/figure-html/unnamed-chunk-1-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# \nlibrary(stargazer)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nPlease cite as: \n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n```\n:::\n\n```{.r .cell-code}\ndat <- tibble(\n  x = rnorm(1000, 100, 50)\n) %>% \n  mutate(\n    x = case_when(x < 0 ~ 0, TRUE ~ x),\n    D = case_when(x > 140 ~ 1, TRUE ~ 0),\n    x2 = x*x,\n    x3 = x*x*x,\n    y3 = 10000 + 0 * D - 100 * x + x2 + rnorm(1000, 0, 1000)\n  ) %>% \n  filter(x < 280)\n\nregression <- lm(y3 ~ D*., data = dat)\n\nstargazer(regression, type = \"text\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                                y3             \n-----------------------------------------------\nD                           -33,094.350        \n                           (24,398.440)        \n                                               \nx                           -94.823***         \n                              (8.989)          \n                                               \nx2                           0.929***          \n                              (0.147)          \n                                               \nx3                            0.0002           \n                              (0.001)          \n                                               \nD:x                           546.174          \n                             (401.870)         \n                                               \nD:x2                          -2.921           \n                              (2.181)          \n                                               \nD:x3                           0.005           \n                              (0.004)          \n                                               \nConstant                   9,994.145***        \n                             (159.627)         \n                                               \n-----------------------------------------------\nObservations                   1,000           \nR2                             0.972           \nAdjusted R2                    0.972           \nResidual Std. Error     997.060 (df = 992)     \nF Statistic         4,984.954*** (df = 7; 992) \n===============================================\nNote:               *p<0.1; **p<0.05; ***p<0.01\n```\n:::\n\n```{.r .cell-code}\nggplot(aes(x, y3, colour = factor(D)), data = dat) +\n  geom_point(alpha = 0.2) +\n  geom_vline(xintercept = 140, colour = \"grey\", linetype = 2) +\n  stat_smooth(method = \"loess\", se = F) +\n  labs(x = \"Test score (X)\", y = \"Potential Outcome (Y)\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](rdd_files/figure-html/unnamed-chunk-1-5.png){width=672}\n:::\n:::\n\n\n# Notes from @gertler2016 (Chapter 6)\n\n@gertler2016 provides an introduction to RDD, stating the four main conditions that are required to apply RDD, a description of fuzzy and sharp RDD, steps on how to check the validity of RDD and limitations and interpretations of RDD.\n\n## Introduction\n\n-   Social programmes often have eligability indices that determine who can and cannot signup.\n-   Examples include targeted antipoverty programmes based on means. Tests scores.\n\n> Regression discontinuity design (RDD) is an impact evaluation method that is adequate for programs that use a continuous index to rank potential participants and that have a cutoff point along the index that determines whether or not potential participants are eligible to receive the program.\n\n## Conditions\n\nTo apply RDD 4 main conditions must be met:\n\n1.  The index must rank people in a continuous way. By contrast, variables that have discrete categories cannot be ranked.\n2.  There must be a clear defined cutoff score. The value above or below that the population is eligable for the programme.\n3.  The cutoff must be unique to the programme of interest. There must be no other programme, apart from the one to be evaluated that uses the same cutoff score.\n4.  The score of a particular individual or unit cannot be manipuated by data collectors, beneficiaries, administrators, or politicians.\n\nRDD esitmate impact around the eligability cutoff as difference between average outcome for units on the treated side of eligability cutoff and average outcome of units on the untreated (comparison) side of the cutoff.\n\nAn example based on agriculture where the scheme is determine by hectare of land is provided.\n\nIt makes the point that since the comparison group is made up of farm just above the eligability threshold, teh impact given by RDD is valid only locally. Therefore we obtain an estimate of the Local Average Treatment Effect (LATE). The RDD will not be able to identify the impact on the smallest farms.\n\nAn advantage is that when the eligability rule is applied, not eligable units need ot be left untreated for the purpose of the impact evaluation. The trade off is that observations far away from the cutoff will not be known.\n\n## Fuzzy RDD\n\nFuzzy RDD. Some units who qualify for the programme may not opt to participate. Or others may find a way to participate even if they are on the other side of the line.\n\nWhen all units comply with the assignment that corresponds to them on the basis of their eligibility index, we say that the RDD is \"sharp,\" while if there is noncompliance on either side of the cutoff , then we say that the RDD is \"fuzzy\".\n\nIf the RDD is fuzzy, we can use the instrumental variable approach to correct for the noncompliance. In the case of randomized assignment with noncompliance, we used the randomized assignment as the instrumental variable that helped us correct for noncompliance.\n\nIn the case of RDD, we can use the original assignment based on the eligibility index as the instrumental variable. Doing so has a drawback, though: our instrumental RDD impact estimate will be further localized---in the sense that it is no longer valid to all observations close to the cutoff , but instead represents the impact for the subgroup of the population that is located close to the cutoff point and that participates in the program only because of the eligibility criteria.\n\n![Gertler et al 2016 Figure 6.3](gertlerfig6_3.png)\n\n## Testing Validity of RDD\n\nFor RDD to provide unbiased estimates of LATE there must be no manipulation of the eligibility index around the cutoff.\n\nA tell tale sign of manipuation is where there is bunching around the cutoff, e.g. look at the percentage of households at the cuttoff. A second test is to plot the eligability index against the outcome index at baseline and check there are no discontinuities or jump right around the cutoff.\n\n## Limitations and Interpretations\n\nBecause the RDD method estimates the impact of the program around\nthe cutoff score, or locally, the estimate cannot necessarily be generalized to\nunits whose scores are further away from the cutoff score: that is, where\neligible and ineligible individuals may not be as similar.\n\n## Checklist\n\n-   Is the index continuous around the cutoff score at the time of the\n    baseline?\n\n-    Is there any evidence of noncompliance with the rule that determines\n    eligibility for treatment? Test whether all eligible units and no ineligible\n    units have received the treatment. If you find noncompliance, you will\n    need to combine RDD with an instrumental variable approach to correct\n    for this \\\"fuzzy discontinuity.\\\"\n\n-   Is there any evidence that index scores may have been manipulated in\n    order to influence who qualified for the program? Test whether the\n    distribution of the index score is smooth at the cutoff point. If you fi nd\n    evidence of \\\"bunching\\\" of index scores either above or below the cutoff\n    point, this might indicate manipulation.\n\n-   Is the cutoff unique to the program being evaluated, or is the cutoff used\n    by other programs as well?\n\nFor a review of practical issues in implementing RDD, see @imbens2008\n\n# Notes from @dehejia2007\n\n-   provides a summary of RDD assumptions\\\n-   notes RDD arguably has the strongest internal validity of any quasi-experimental design and external validity may be limited.\n-   identified 3 threats to RDD (other variables change discontinuously at the cutoff, discontinuities at other values of the assignment variable, manipulation of the assignment variable).\n-   provides 5 specification checks:\n    -   discontinuities in the average covariates\n    -   discontinuity in distribution of forcing variables\n    -   discontinuity in average outcome at other values\n    -   sensitivity to bandwidth choice\n    -   fuzzy RD design\n    -   regression kink design.\n-   provides a case study on fuzzy RDD (@marie2009) that shows recidivism for those who leave prison early falls.\n-   provides a case study on manipulation (@cs2010) and provide examples of the Lee and McCrary tests that cast doubt on (@lmb2004). @dehejia2007 concludes that LMB did not do enough to justify functional form or sensitivity to different bandwidth.\n-   Lesson from CS and LMB are to try and find problems in your design before someone does it for you, identify and collect accurate data that are likely to reveal something about your cut off point, automatic bandwidth algoritms do not gaurantee good results. They are just a starting point.\n-   provide guide to practise.\n-   set out 3 steps to sharp RD analysis:\n    -   graph the data showing the average outcome value over differnt bins. Need to be precise enough so that plots looks smooth on either side of the cutoff value, but at the same time small enough to make the jump around the kickoff value clear.\n    -   Estimate treatment effect by running regression on both sides of the cutoff point.\n    -   The robustness of the results should be assessed by employing various specification tests.\n-   sets out 3 steps for fuzzy rd analysis:\n    -   graph like SRD but also graph probability of treatment.\n    -   Estimate treatment effect using 2SLS which is numerically equivalet to computing the ratio in the estimate of the jump, at the cutoff.\n    -   assess robustess of the results.\n-   A checklist for evaluating a RD paper:\n    -   treatment changes discontinuously at the cutoff?\n    -   outcomes change discontinuously at the cutoff?\n    -   other covariates do not change at the cutoff?\n    -   pre-treatment outcomes do not change at the cutoff?\n    -   there is no manipuation of the assignment variable? (bunching near the cutoff) \\*are the basic results evident from a single graph?\n    -   are the results robust to different functional form assumptions around the assignment varibles?\n    -   could other possibly unobserved treatments change discontinuously at the cutoff?\n    -   external validity - are cases near the cutoff different from cases far from the cutoff in other ways? Do these differences make them more of less relevant from a theoretical or policy perspective?\n\n# Notes from @hk2021 Chapter 20\n\n@hk2021 provides an example of comparing US to Mexico based on distance from the border. \n\nTerminology\n\n* __Running variable__. The running variable, also known as a forcing variable, is the variable that determines whether you’re treated or not. For example, if a doctor takes your blood pressure and will assign you a blood-pressure reducing medicine if your systolic blood pressure is above 135, then your blood pressure is the running variable.\n\n* __Cutoff__. The cutoff is the value of the running variable that determines whether you get treatment. Using the same blood pressure example, the cutoff in this case is a systolic blood pressure of 135. If you’re above 135, you get the medicine. If you’re below 135, you don’t.\n\n* __Bandwidth__. It’s reasonable to think that people just barely to either side of the cutoff are basically the same other than for the cutoff. But people farther away (say, downtown San Diego vs. further inside of Mexico) might be different for reasons other than the cutoff. The bandwidth is how much area around the cutoff you’re willing to consider comparable. Ten feet to either side of the US/Mexico border? 1000 feet? 80 miles?\n\nExamples, money qualify for programme, test for education, geography - country and policy, politics win election or do not. \n\nWhat we’re looking for is some sort of treatment that is assigned based on a cutoff. There’s the running variable that determines treatment. And there’s some cutoff value. If you’re just to one side of the cutoff value, you don’t get treated. If you’re to the other side, you do get treated.\n\nSince our strategy is going to be assuming that people close to the cutoff are effectively randomly assigned there shouldn’t be any obvious impediments to that randomness - people shouldn’t be able to manipulate the running variable so as to choose their own treatment, and also the people who choose what the cutoff is shouldn’t be able to make that choice in response to finding out who has which running variable values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDiagrammeR::grViz(\n  \"digraph{\n    graph[rankdir = LR]\n    node[shape = none]\n    Z -> RunningVariable\n    RunningVariable -> Outcome\n    RunningVariable -> AboveCutoff\n    AboveCutoff -> Treatment\n    Z -> Outcome\n    Treatment -> Outcome\n    \n    \n\n  }\"\n)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"grViz html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-328a5d14c55966fc53ea\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-328a5d14c55966fc53ea\">{\"x\":{\"diagram\":\"digraph{\\n    graph[rankdir = LR]\\n    node[shape = none]\\n    Z -> RunningVariable\\n    RunningVariable -> Outcome\\n    RunningVariable -> AboveCutoff\\n    AboveCutoff -> Treatment\\n    Z -> Outcome\\n    Treatment -> Outcome\\n    \\n    \\n\\n  }\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n@battistin2009 are one paper of many that apply fuzzy regression discontinuity to retirement. In their paper, they look at consumption and how it changes at the point of retirement. Specifically, they want to know if retirement causes consumption to immediately drop and if it does drop, why?\n\n## How is it performed?\n\n$$Y = \\beta_0 + \\beta_1 (Running-Cutoff) + \\beta_2 Treated + \\beta_3 (Running - Cutoff) * Treated + \\varepsilon$$\n\nThis is a simple linear approach to regression discontinuity, where $\\Running$ is the running variable which we have centred around the cutoff using $(Running-Cutoff)$. This takes a negative value to the left of the cutoff, zero at the cutoff, and a positive value to the right. We’re talking about a sharp regression discontinuity here, so $Treated$ is both an indicator for being treated and an indicator for being above the cutoff - these are the same thing. The model is generally estimated using heteroskedasticity-robust standard errors, as one might expect the discontinuity and general shape of the line we’re fitting to exhibit heteroskedasticity in most cases.We’ll ignore the issue of a bandwidth for now and come back to it later - this regression approach can be applied whether you use all the data or limit yourself to a bandwidth around the cutoff.\n\nNotice the lack of control variables In most other chapters, when I do this it’s to help focus your attention on the design. Here, it’s very intentional. The whole idea of regression discontinuity is that you have nearly random assignment on either side of the cutoff. You shouldn’t need control variables because the design itself should close any back doors. No open back doors? No need for controls. Adding controls implies you don’t believe the assumptions necessary for the regression discontinuity method to work, and makes the whole thing becomes a bit suspicious to any readers.\n\n## R code\n\nLet’s code up some regression discontinuity! I’m going to do this in two ways. First I’m going to run a plain-ol’ ordinary least squares model with a bandwidth and kernel weight applied, with heteroskedasticity-robust standard errors. However, there are a number of other adjustments we’ll be talking about in this chapter, and it can be a good idea to pass this task off to a package that knows what it’s doing. I see no point in showing you code you’re unlikely to use just because we haven’t gotten to the relevant point in the chapter yet. So we’ll be using those specialized commands as well, and I’ll talk later about some of the extra stuff they’re doing for us.\n\nFor this example we’re going to use data from Government Transfers and Political Support by Manacorda, Miguel, and Vigorito (2011Manacorda, Marco, Edward Miguel, and Andrea Vigorito. 2011. “Government Transfers and Political Support.” American Economic Journal: Applied Economics 3 (3): 1–28.). This paper looks at a large poverty alleviation program in Uruguay which cut a sizeable check to a large portion of the population. They are interested in whether receiving those funds made people more likely to support the newly-installed center-left government that sent them.\n\nWho got the payments? You had to have an income low enough. But they didn’t exactly use income as a running variable; that might have been too easy to manipulate. Instead, the government used a bunch of factors - housing, work, reported income, schooling - and predicted what your income would be from that. Then, the predicted income was the running variable, and treatment was assigned based on being below a cutoff. About 14% of the population ended up getting payments.\n\nThe researchers polled a bunch of people near the income cutoff to check their support for the government afterwards. Did people just below the income cutoff support the government more than those just above?\n\nThe data set we have on government transfers comes with the predicted-income variable pre-centered so the cutoff is at zero. Then, support for the government takes three values: you think they’re better than the previous government (1), the same (1/2) or worse (0). The data only includes individuals near the cutoff - the centered-income variable in the data only goes from -.02 to .02.\n\nBefore we estimate our model, let’s do a nearly-compulsory graphical regression discontinuity check so we can confirm that there does appear to be some sort of discontinuity where we expect it.\nThis is a “plot of binned means.” There are some preprogrammed ways to do this like rdplot in R or Stata in the rdrobust package or binscatter in Stata from the binscatter package, but this is easy enough that we may as well do it by hand and flex those graphing muscles.\n\nFor one of these graphs, you generally want to (1) slice the running variable up into bins (making sure the cutoff is the edge between two bins), (2) take the mean of the outcome within each of the bins, and (3) plot the result, with a vertical line at the cutoff so you can see where the cutoff is. Then you’d generally repeat the process with treatment instead of the outcome to produce a graph like below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ngt <- causaldata::gov_transfers\n\n# Use cut() to create bins, using breaks to make sure it breaks at 0\n# (-15:15)*.02/15 gives 15 breaks from -.02 to .02\nbinned <- gt %>%\n    mutate(Inc_Bins = cut(Income_Centered,\n           breaks = (-15:15)*(.02/15))) %>%\n    group_by(Inc_Bins) %>%\n    summarize(Support = mean(Support),\n    Income = mean(Income_Centered))\n# Taking the mean of Income lets us plot data roughly at the bin midpoints\n\nggplot(binned, aes(x = Income, y = Support)) + \n    geom_line() + \n    # Add a cutoff line\n    geom_vline(aes(xintercept = 0), linetype = 'dashed')\n```\n\n::: {.cell-output-display}\n![](rdd_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nNow that we have our graph in mind, we can actually estimate our model. We’ll start doing it with OLS and a second-order polynomial, and then we’ll do a linear model with a triangular kernel weight, limiting the bandwidth around the cutoff to .01 on either side. The bandwidth in this case isn’t super necessary - the data is already limited to .02 on either side around the cutoff - but this is just a demonstration.\n\nIt’s important to note that the first step of doing regression discontinuity with OLS is to center the running variable around the cutoff. This is as simple as making the variable \n$RunningVariable - Cutoff$, translated into whatever language you use. The second step would then be to create a “treated” variable $RunningVariable < Cutoff$ (since treatment is applied below the cutoff in this instance). But in this case, the running variable comes pre-centered ($IncomeCentred$) and the below-cutoff treatment variable is already in the data ($Participation$\n) so I’ll leave those parts out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse); library(modelsummary)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'modelsummary' was built under R version 4.3.1\n```\n:::\n\n```{.r .cell-code}\ngt <- causaldata::gov_transfers\n\n# Linear term and a squared term with \"treated\" interactions\nm <- lm(Support ~ Income_Centered*Participation +\n       I(Income_Centered^2)*Participation, data = gt)\n\n# Add a triangular kernel weight\nkweight <- function(x) {\n    # To start at a weight of 0 at x = 0, and impose a bandwidth of .01, \n    # we need a \"slope\" of -1/.01 = 100, \n    # and to go in either direction use the absolute value\n    w <- 1 - 100*abs(x)\n    # if further away than .01, the weight is 0, not negative\n    w <- ifelse(w < 0, 0, w)\n    return(w)\n}\n\n# Run the same model but with the weight\nmw <- lm(Support ~ Income_Centered*Participation, data = gt,\n         weights = kweight(Income_Centered))\n\n# See the results with heteroskedasticity-robust SEs\nmsummary(list('Quadratic' = m, 'Linear with Kernel Weight' = mw), \n    stars = c('*' = .1, '**' = .05, '***' = .01), vcov = 'robust')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in residuals^2/(1 - diaghat)^2: longer object length is not a multiple\nof shorter object length\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in residuals^2/(1 - diaghat)^2: longer object length is not a multiple\nof shorter object length\n```\n:::\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-c8ef07ca15bd16c939f6\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-c8ef07ca15bd16c939f6\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"(Intercept)\",\"\",\"Income_Centered\",\"\",\"Participation\",\"\",\"I(Income_Centered^2)\",\"\",\"Income_Centered × Participation\",\"\",\"Participation × I(Income_Centered^2)\",\"\",\"Num.Obs.\",\"R2\",\"R2 Adj.\",\"AIC\",\"BIC\",\"Log.Lik.\",\"F\",\"RMSE\",\"Std.Errors\"],[\"0.769***\",\"(0.034)\",\"-11.567\",\"(8.101)\",\"0.093**\",\"(0.044)\",\"562.247\",\"(401.982)\",\"19.300*\",\"(10.322)\",\"-101.103\",\"(502.789)\",\"1948\",\"0.036\",\"0.034\",\"1007.5\",\"1046.6\",\"-496.764\",\"14.128\",\"0.31\",\"HC3\"],[\"0.819***\",\"(0.015)\",\"-23.697***\",\"(3.219)\",\"0.033\",\"(0.021)\",\"\",\"\",\"26.594***\",\"(4.433)\",\"\",\"\",\"937\",\"0.041\",\"0.038\",\"750.9\",\"775.2\",\"-370.470\",\"55.653\",\"0.34\",\"HC3\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Quadratic<\\/th>\\n      <th>Linear with Kernel Weight<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nhttps://rdpackages.github.io/\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse); library(rdrobust)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'rdrobust' was built under R version 4.3.1\n```\n:::\n\n```{.r .cell-code}\ngt <- causaldata::gov_transfers\n\n# Estimate regression discontinuity and plot it\nm <- rdrobust(gt$Support, gt$Income_Centered, c = 0)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in rdrobust(gt$Support, gt$Income_Centered, c = 0): Mass points\ndetected in the running variable.\n```\n:::\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1948\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                 1127          821\nEff. Number of Obs.             291          194\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.005        0.005\nBW bias (b)                   0.010        0.010\nrho (h/b)                     0.509        0.509\nUnique Obs.                     841          639\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.025     0.062     0.396     0.692    [-0.098 , 0.147]     \n        Robust         -         -     0.624     0.533    [-0.097 , 0.188]     \n=============================================================================\n```\n:::\n\n```{.r .cell-code}\n# Note, by default, rdrobust and rdplot use different numbers\n# of polynomial terms. You can set the p option to standardize them.\nrdplot(gt$Support, gt$Income_Centered)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Mass points detected in the running variable.\"\n```\n:::\n\n::: {.cell-output-display}\n![](rdd_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse); library(fixest); library(modelsummary)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'fixest' was built under R version 4.3.1\n```\n:::\n\n```{.r .cell-code}\nvet <- causaldata::mortgages\n\n# Create an \"above-cutoff\" variable as the instrument\nvet <- vet %>% mutate(above = qob_minus_kw > 0)\n\n# Impose a bandwidth of 12 quarters on either side\nvet <- vet %>%  filter(abs(qob_minus_kw) < 12)\n\nm <- feols(home_ownership ~\n    nonwhite  | # Control for race\n    bpl + qob | # fixed effect controls\n    qob_minus_kw*vet_wwko ~ # Instrument our standard RDD\n    qob_minus_kw*above, # with being above the cutoff\n    se = 'hetero', # heteroskedasticity-robust SEs\n    data = vet) \n\n# And look at the results\nmsummary(m, stars = c('*' = .1, '**' = .05, '***' = .01))\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-9d4c8f70d447cf74f03c\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-9d4c8f70d447cf74f03c\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"fit_qob_minus_kw\",\"\",\"fit_vet_wwko\",\"\",\"fit_qob_minus_kw × vet_wwko\",\"\",\"nonwhite\",\"\",\"Num.Obs.\",\"R2\",\"R2 Adj.\",\"R2 Within\",\"R2 Within Adj.\",\"AIC\",\"BIC\",\"RMSE\",\"Std.Errors\",\"FE: bpl\",\"FE: qob\"],[\"-0.007***\",\"(0.002)\",\"0.170***\",\"(0.046)\",\"-0.003\",\"(0.003)\",\"-0.190***\",\"(0.007)\",\"56901\",\"0.053\",\"0.052\",\"0.037\",\"0.037\",\"68659.5\",\"69187.5\",\"0.44\",\"Heteroskedasticity-robust\",\"X\",\"X\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>(1)<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse); library(rdrobust)\nvet <- causaldata::mortgages\n\n# It will apply a bandwidth anyway, but having it\n# check the whole bandwidth space will be slow. So let's\n# pre-limit it to a reasonable range of 12 quarters\nvet <- vet %>%\n    filter(abs(qob_minus_kw) <= 12)\n\n# Create our matrix of controls\ncontrols <- vet %>%\n    select(nonwhite, bpl, qob) %>%\n    mutate(qob = factor(qob))\n# and make it a matrix with dummies\nconmatrix <- model.matrix(~., data = controls)\n\n# This is fairly slow due to the controls, beware!\nm <- rdrobust(vet$home_ownership,\n              vet$qob_minus_kw,\n              fuzzy = vet$vet_wwko,\n              c = 0,\n              covs = conmatrix)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in rdrobust(vet$home_ownership, vet$qob_minus_kw, fuzzy = vet$vet_wwko,\n: Multicollinearity issue detected in covs. Redundant covariates dropped.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in rdrobust(vet$home_ownership, vet$qob_minus_kw, fuzzy = vet$vet_wwko,\n: Mass points detected in the running variable.\n```\n:::\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCovariate-adjusted Fuzzy RD estimates using local polynomial regression.\n\nNumber of Obs.                56901\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                28776        28125\nEff. Number of Obs.            6911         6756\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   3.387        3.387\nBW bias (b)                   5.354        5.354\nrho (h/b)                     0.633        0.633\nUnique Obs.                      12           12\n\nFirst-stage estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.018     0.018     0.964     0.335    [-0.018 , 0.053]     \n        Robust         -         -     2.244     0.025     [0.006 , 0.094]     \n=============================================================================\n\nTreatment effect estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P>|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    -5.377     5.716    -0.941     0.347   [-16.581 , 5.827]     \n        Robust         -         -     0.513     0.608   [-10.182 , 17.408]    \n=============================================================================\n```\n:::\n:::\n\n\n\n@heiss2020\n\n# References\n",
    "supporting": [
      "rdd_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\r\n<script src=\"../../site_libs/viz-1.8.2/viz.js\"></script>\r\n<link href=\"../../site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/grViz-binding-1.0.9/grViz.js\"></script>\r\n<link href=\"../../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/datatables-binding-0.27/datatables.js\"></script>\r\n<script src=\"../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\r\n<link href=\"../../site_libs/dt-core-1.12.1/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\r\n<link href=\"../../site_libs/dt-core-1.12.1/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/dt-core-1.12.1/js/jquery.dataTables.min.js\"></script>\r\n<link href=\"../../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}