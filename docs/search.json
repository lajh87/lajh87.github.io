[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Retrieval Augmented Generation\n\n\n\n\n\n\n\nlarge language models (LLM)\n\n\nevidence synthesis\n\n\nmethods\n\n\nnotes\n\n\n\n\nAn applied example using llama index\n\n\n\n\n\n\nNov 19, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nRetrieval Augmented Generation\n\n\n\n\n\n\n\nlarge language models (LLM)\n\n\nevidence synthesis\n\n\nmethods\n\n\nnotes\n\n\n\n\nAn applied example using pinecone and langchain\n\n\n\n\n\n\nNov 19, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nRetrieval Augmented Generation\n\n\n\n\n\n\n\nlarge language models (LLM)\n\n\nevidence synthesis\n\n\nmethods\n\n\nnotes\n\n\n\n\nAn applied example using pinecone and langchain\n\n\n\n\n\n\nNov 19, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nIran Sanctions - Sythetic Control\n\n\n\n\n\n\n\nevaluation\n\n\nsynthetic-control\n\n\nmethods\n\n\nnotes\n\n\n\n\nAn applied example of synthetic control\n\n\n\n\n\n\nAug 29, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nSynthetic Control Methods\n\n\n\n\n\n\n\nevaluation\n\n\nsynthetic-control\n\n\nmethods\n\n\nnotes\n\n\n\n\nNotes on Synthetic Control\n\n\n\n\n\n\nAug 29, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nRegression Discontinuity Design\n\n\n\n\n\n\n\nevaluation\n\n\nquasi-experimental\n\n\nmethods\n\n\nnotes\n\n\n\n\nNotes on Regression Discontinuity Design\n\n\n\n\n\n\nJun 18, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nSummarise Information in a Book\n\n\n\n\n\n\n\nNLP\n\n\n\n\nSummarise the Top 5 Setences by Section in Homo Deus\n\n\n\n\n\n\nMay 30, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nCalculate Voluntary Job Separation Rates\n\n\n\n\n\n\n\nLabour Markets\n\n\nLabour Force Survey\n\n\n\n\nCalculate voluntary job separation rates from the two quarter labour force survey\n\n\n\n\n\n\nMay 19, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nNatural Language Processing\n\n\n\n\n\n\n\nNLP\n\n\n\n\nExtract people from Guardian reporting on what we know on Ukraine.\n\n\n\n\n\n\nMay 18, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nProcess Guardian - What we know on Ukraine\n\n\n\n\n\n\n\napi\n\n\nNLP\n\n\n\n\nThis post uses API to extract Guardian What we Know data and LexR to extract top 3 points.\n\n\n\n\n\n\nMay 18, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nExtract UK Frigate Data\n\n\n\n\n\n\n\nwikipedia\n\n\n\n\nSearch and extract UK Frigate Data\n\n\n\n\n\n\nApr 1, 2023\n\n\nDefence Economist\n\n\n\n\n\n\n  \n\n\n\n\nExtract Type 23 Data\n\n\n\n\n\n\n\nwikipedia\n\n\nscrape\n\n\n\n\nExtract Type 23 Performance Data\n\n\n\n\n\n\nApr 1, 2023\n\n\nDefence Economist\n\n\n\n\n\n\n  \n\n\n\n\nUsing Wiki Data to Extract Data\n\n\n\n\n\n\n\nwikipedia\n\n\nscrape\n\n\n\n\nThis post shows how to access the wikipedia knowledge graph\n\n\n\n\n\n\nMar 26, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nExtract of UK Frigates and Their Precendents\n\n\n\n\n\n\n\nwikipedia\n\n\nscrape\n\n\n\n\nThis post shows how to access the wikipedia knowledge graph\n\n\n\n\n\n\nMar 26, 2023\n\n\nLuke Heley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20230326-using-wikidata/using-wikidata.html",
    "href": "posts/20230326-using-wikidata/using-wikidata.html",
    "title": "Using Wiki Data to Extract Data",
    "section": "",
    "text": "Data\nWe use the wikidata API to extract the knowledge graph from wiki.\nThe approach is to search wikidata for the item of interest and select the best match from the list of search results that return.\n\n# function to search wiki data and return a tibble of search results\nsearch_wikidata <- function(search){\n  base <- \"https://www.wikidata.org\" \n  path <- \"/w/api.php\"\n  query <- list(\n    action=\"query\",\n    list=\"search\",\n    format = \"json\",\n    srsearch = search\n  )\n  \n  httr::GET(base, path = path, query = query) |>\n    httr::content() |>\n    purrr::pluck(\"query\", \"search\") |>\n    purrr::map_df(~{\n      ul <- unlist(.x)\n      name <- names(ul)\n      value <- as.character(ul)\n      dplyr::tibble(name, value) |>\n        tidyr::pivot_wider()\n      })\n}\n\n(search_results <- search_wikidata(\"Type-094 submarine\"))\n\n# A tibble: 2 × 6\n  ns    title    pageid  wordcount snippet                               times…¹\n  <chr> <chr>    <chr>   <chr>     <chr>                                 <chr>  \n1 0     Q1203377 1146175 0         nuclear-powered ballistic missile su… 2023-0…\n2 0     Q7008427 6886594 0         Wikimedia category                    2022-1…\n# … with abbreviated variable name ¹​timestamp\n\n# Extract the item title for the item of interest\nroot_item_title <- search_results$title[1]\n\nWe then extract wikidata associated with the item.\n\n# extract the entity data for a chosen item.\nget_entity_data <- function(item = \"Q1203377\"){\n  base <- \"https://www.wikidata.org/\"\n  path <- glue::glue(\"wiki/Special:EntityData/{item}.json\")\n  query <- list(flavor = \"simple\")  \n  req <- httr::GET(base, path = path, query = query)\n  \n  httr::content(req) |>\n    purrr::pluck(\"entities\", item, \"claims\") |>\n    purrr::map_df(~{\n      value <- unlist(.x)\n      name <- names(value)\n      dplyr::tibble(name, value) |>\n        tidyr::pivot_wider() |>\n        tidyr::unnest()\n      })\n}\n\nentity_data <- get_entity_data(root_item_title)\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n    dplyr::group_by(name) %>%\n    dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n    dplyr::filter(n > 1L)\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(mainsnak.snaktype, mainsnak.property, mainsnak.hash, `mainsnak.datavalue.value.entity-type`, \n    `mainsnak.datavalue.value.numeric-id`, mainsnak.datavalue.value.id, \n    mainsnak.datavalue.type, mainsnak.datatype, type, id, rank)`\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n    dplyr::group_by(name) %>%\n    dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n    dplyr::filter(n > 1L)\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(mainsnak.snaktype, mainsnak.property, mainsnak.hash, `mainsnak.datavalue.value.entity-type`, \n    `mainsnak.datavalue.value.numeric-id`, mainsnak.datavalue.value.id, \n    mainsnak.datavalue.type, mainsnak.datatype, type, id, rank, \n    references.hash, references.snaks.P143.snaktype, references.snaks.P143.property, \n    references.snaks.P143.hash, `references.snaks.P143.datavalue.value.entity-type`, \n    `references.snaks.P143.datavalue.value.numeric-id`, references.snaks.P143.datavalue.value.id, \n    references.snaks.P143.datavalue.type, references.snaks.P143.datatype, \n    `references.snaks-order`)`\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n\n\nAnd the associated properties\n\nproperties <- entity_data |> \n  dplyr::pull(\"mainsnak.property\") |>\n  unique()\n\nget_entity_id <- function(id = \"P373\"){\n  if(length(id)>1) id <- paste(id, collapse = \"|\")\n  base <- \"https://www.wikidata.org/\"\n  path <- \"w/api.php\"\n  query <- list(\n    action=\"wbgetentities\",\n    ids=id,\n    languages=\"en\",\n    props=\"labels\",\n    format=\"json\"\n    )\n  \n  req <- httr::GET(base, path = path, query = query) \n  \n  if(req$status != 200) \n    return(stop(glue::glue(\"Error returned status: {req$status}\")))\n  \n  httr::content(req) |>\n    purrr::pluck(\"entities\") |>\n    purrr::map_df(~{\n      value <- unlist(.x)\n      name <- names(value)\n      dplyr::tibble(name, value) |> tidyr::pivot_wider()\n    })\n}\n\n(prop_label <- get_entity_id(properties))\n\n# A tibble: 16 × 5\n   type     datatype        id    labels.en.language labels.en.value      \n   <chr>    <chr>           <chr> <chr>              <chr>                \n 1 property string          P373  en                 Commons category     \n 2 property wikibase-item   P516  en                 powered by           \n 3 property string          P561  en                 NATO reporting name  \n 4 property wikibase-item   P31   en                 instance of          \n 5 property wikibase-item   P279  en                 subclass of          \n 6 property wikibase-item   P910  en                 topic's main category\n 7 property external-id     P646  en                 Freebase ID          \n 8 property time            P729  en                 service entry        \n 9 property wikibase-item   P156  en                 followed by          \n10 property wikibase-item   P155  en                 follows              \n11 property commonsMedia    P18   en                 image                \n12 property wikibase-item   P176  en                 manufacturer         \n13 property wikibase-item   P137  en                 operator             \n14 property monolingualtext P1813 en                 short name           \n15 property wikibase-item   P520  en                 armament             \n16 property wikibase-item   P495  en                 country of origin    \n\n\nGet the item data\n\nentity_data2 <- entity_data |>\n  dplyr::select(\n    mainsnak.property,\n    mainsnak.datavalue.type,\n    mainsnak.datavalue.value.id,\n    mainsnak.datavalue.value,\n    mainsnak.datavalue.value.text,\n    mainsnak.datavalue.value.time\n  ) |>\n  dplyr::mutate(\n    value = dplyr::case_when(\n      !is.na(mainsnak.datavalue.value.id) ~ mainsnak.datavalue.value.id,\n      !is.na(mainsnak.datavalue.value) ~ mainsnak.datavalue.value,\n      !is.na(mainsnak.datavalue.value.time) ~ mainsnak.datavalue.value.time,\n      !is.na(mainsnak.datavalue.value.text) ~ mainsnak.datavalue.value.text\n    )\n  ) |>\n  dplyr::select(property = 1, type = 2, value) |>\n  dplyr::distinct()\n\nentity_data3 <- entity_data2 |>\n  dplyr::left_join(prop_label |>\n  dplyr::select(\n    property = id, \n    property_label = labels.en.value\n  ))\n\nJoining, by = \"property\"\n\nitems <- entity_data3 |>\n  dplyr::filter(type == \"wikibase-entityid\") |>\n  dplyr::pull(value) |> \n  unique()\n\nitem_labels <- get_entity_id(items)\n\n(item_property <- entity_data3 |>\n  dplyr::left_join(\n    item_labels |>\n      dplyr::select(value = id, item_label = labels.en.value)\n  ) |>\n  dplyr::mutate(item_label = dplyr::case_when(is.na(item_label)~value, \n                                              TRUE ~ item_label)) |>\n  dplyr::select(property_label, item_label))\n\nJoining, by = \"value\"\n\n\n# A tibble: 17 × 2\n   property_label        item_label                                          \n   <chr>                 <chr>                                               \n 1 Commons category      Type 09IV submarines                                \n 2 powered by            nuclear marine propulsion                           \n 3 NATO reporting name   Jin                                                 \n 4 instance of           submarine class                                     \n 5 subclass of           ballistic missile submarine                         \n 6 subclass of           nuclear submarine                                   \n 7 topic's main category Category:Type 094 submarines                        \n 8 Freebase ID           /m/09wz47                                           \n 9 service entry         +2010-01-01T00:00:00Z                               \n10 followed by           Type 096 submarine                                  \n11 follows               Type 092 Daqingyu                                   \n12 image                 Jin (Type 094) Class Ballistic Missile Submarine.JPG\n13 manufacturer          Bohai Shipyard                                      \n14 operator              People's Liberation Army Navy                       \n15 short name            Type 094                                            \n16 armament              JL-2                                                \n17 country of origin     People's Republic of China                          \n\n\nGet wiki urls\n\nget_wikisites <- function(item = \"Q1203377\"){\n  base <- \"https://www.wikidata.org/\"\n  path <- glue::glue(\"wiki/Special:EntityData/{item}.json\")\n  query <- list(flavor = \"simple\")  \n  req <- httr::GET(base, path = path, query = query)\n  \n  cont <- httr::content(req) \n  cont |>\n    purrr::pluck(\"entities\", item, \"sitelinks\") |>\n    purrr::map_df(~{\n      value <- unlist(.x)\n      name <- names(value)\n      dplyr::tibble(name, value) |>\n        tidyr::pivot_wider()\n      })\n}\n\n(wikiurl <- get_wikisites(root_item_title) |>\n  dplyr::filter(site == \"enwiki\") |>\n  dplyr::pull(url))\n\n                                                   \n\"https://en.wikipedia.org/wiki/Type_094_submarine\" \n\n\nScrape the infobox from the wiki url\n\nscrape_infobox <- function(\n    url =\"https://en.wikipedia.org/wiki/Type_094_submarine\"\n){\n  req <- httr::GET(url)\n  req |> \n    httr::content() |> \n    xml2::xml_find_all(\"//table[@class='infobox']\") |>\n    rvest::html_table() \n}\n\nscrape_infobox(\"https://en.wikipedia.org/wiki/Type_094_submarine\")\n\n[[1]]\n# A tibble: 22 × 2\n   X1                      X2                               \n   <chr>                   <chr>                            \n 1 Profile of the Type 094 Profile of the Type 094          \n 2 Type 094 submarine      Type 094 submarine               \n 3 Class overview          Class overview                   \n 4 Name                    Type 094 (Jin class)             \n 5 Builders                Bohai Shipyard, Huludao, China[2]\n 6 Operators               People's Liberation Army Navy    \n 7 Preceded by             Type 092 (Xia class)             \n 8 Succeeded by            Type 096                         \n 9 Cost                    $750 million per unit[1]         \n10 In commission           2007–present[2]                  \n# … with 12 more rows\n\n\nScrape wikitables\n\nscrape_wikitables <- function(\n    url =\"https://en.wikipedia.org/wiki/Type_094_submarine\"\n){\n  req <- httr::GET(url)\n  req |> \n    httr::content() |> \n    xml2::xml_find_all(\"//table[@class='wikitable']\") |>\n    rvest::html_table()\n}\n\nscrape_wikitables(\"https://en.wikipedia.org/wiki/Type_094_submarine\")\n\n[[1]]\n# A tibble: 8 × 7\n  Name                `Hull no.`  Builder         Laid …¹ Launc…² Commi…³ Status\n  <chr>               <chr>       <chr>           <chr>   <chr>   <chr>   <chr> \n1 \"Type 094\"          \"Type 094\"  \"Type 094\"      \"Type … \"Type … Type 0… Type …\n2 \"\"                  \"411[2]\"    \"Bohai Shipyar… \"2001[… \"28 Ju… March … Active\n3 \"Changzheng 10[20]\" \"412[2]\"    \"Bohai Shipyar… \"2003[… \"2006[… 2010[2] Active\n4 \"Changzheng 11[20]\" \"413[2]\"    \"Bohai Shipyar… \"2004[… \"Decem… 2012[2] Active\n5 \"Changzheng 18[21]\" \"421[22]\"   \"\"              \"\"      \"\"      23 Apr… Active\n6 \"Type 094A\"         \"Type 094A\" \"Type 094A\"     \"Type … \"Type … Type 0… Type …\n7 \"\"                  \"\"          \"\"              \"\"      \"\"      2020[5] Active\n8 \"\"                  \"\"          \"\"              \"\"      \"\"      2020[5] Active\n# … with abbreviated variable names ¹​`Laid down`, ²​Launched, ³​Commissioned"
  },
  {
    "objectID": "posts/20230401-extract-uk-frigate-data/20230401-extract-uk-frigate-data.html",
    "href": "posts/20230401-extract-uk-frigate-data/20230401-extract-uk-frigate-data.html",
    "title": "Extract UK Frigate Data",
    "section": "",
    "text": "Data\ntype 26, type 23, type 22, type 21, leander class, rothesay-class, valour-class, Whitby class and Sailsbury class\n\nfrigates <- c(\n  \"Type 26\", \"Type 23\", \"Type 22\", \n  \"Type 21\", \"Leander Class\", \"Rothesay Class\", \n   \"Whitby Class\", \"Salisbury Class\"\n  ) |>\n  paste(\"Frigate\")\n\nbase <- \"https://www.wikidata.org\" \npath <- \"/w/api.php\"\nquery <- list(\n  action=\"query\",\n  list=\"search\",\n  format = \"json\"\n)\n\nsearch_results <- frigates |>\n  purrr::map(~{\n    query$srsearch = .x\n    httr::GET(base, path = path, query = query) |>\n      httr::content()\n  })\n\n\nsearch_results_tidy <- search_results |>\n  purrr::map_df(~{\n     .x$query$search[1]; # extract first response\n    } ) |>\n  dplyr::select(title, snippet) |>\n  dplyr::mutate(search = frigates) |>\n  dplyr::relocate(search)\n\nget wiki url\n\nbase <- \"https://www.wikidata.org/\"\nreq <- search_results_tidy$title |>\n  purrr::map(~{\n    path <- glue::glue(\"wiki/Special:EntityData/{.x}.json\")\n    httr::GET(url = base, path = path, query = list(flavor = \"simple\"))\n  })\n\n(wiki_url <- req |>\n  purrr::map_chr(~{\n    cnt <- httr::content(.x)\n    cnt$entities[[1]]$sitelinks$enwiki$url\n  }))\n\n[1] \"https://en.wikipedia.org/wiki/Type_26_frigate\"        \n[2] \"https://en.wikipedia.org/wiki/Type_23_frigate\"        \n[3] \"https://en.wikipedia.org/wiki/Type_22_frigate\"        \n[4] \"https://en.wikipedia.org/wiki/Type_21_frigate\"        \n[5] \"https://en.wikipedia.org/wiki/Leander-class_frigate\"  \n[6] \"https://en.wikipedia.org/wiki/Rothesay-class_frigate\" \n[7] \"https://en.wikipedia.org/wiki/Whitby-class_frigate\"   \n[8] \"https://en.wikipedia.org/wiki/Salisbury-class_frigate\"\n\n\n\ninfobox <- wiki_url |>\n  purrr::map_df(~{\n    httr::GET(.x) |>\n      httr::content() |>\n      xml2::xml_find_all(\"//table[@class='infobox']\") |>\n      rvest::html_table() |>\n      purrr::pluck(1) |>\n      dplyr::mutate(source = .x)\n  })\n\nvars <- c(\"Displacement\", \"Length\", \"Beam\", \"Draught\")\n\nperformance <- infobox |>\n  dplyr::group_by(source) |>\n  dplyr::mutate(X0 = ifelse(X1 == X2, X1, NA)) |>\n  tidyr::fill(X0) |>\n  dplyr::filter(X1 != X2) |>\n  dplyr::filter(X1 %in% vars) |>\n  dplyr::ungroup() |>\n  dplyr::mutate(key = 1L:dplyr::n()) |>\n  dplyr::group_by(source) |>\n  dplyr::mutate(X2 = stringr::str_split(X2, \"\\n\")) |>\n  tidyr::unnest(cols = \"X2\") |>\n  dplyr::ungroup() |>\n  dplyr::mutate(\n    X3 = ifelse(\n      test = stringr::str_detect(X2, \"^[0-9]\", negate = TRUE), \n      yes = substr(X2, 1, stringr::str_locate(X2, \":\")[,1]-1),\n      no = NA)\n  ) |>\n   dplyr::mutate(\n    X2 = ifelse(\n      test = stringr::str_detect(X2, \"^[0-9]\", negate = TRUE), \n      yes = substr(X2, stringr::str_locate(X2, \":\")[,1]+2, nchar(X2)),\n      no = X2)\n  )  |>\n  dplyr::mutate(\n    value = as.numeric(stringr::str_remove_all(substr(X2, 1, stringr::str_locate(X2, \"\\\\s\")[,1]-1), \",\"))\n  ) |>\n  dplyr::mutate(\n    units = substr(X2, stringr::str_locate(X2, \"\\\\s\")[,1]+1, nchar(X2))\n  ) |>\n  dplyr::mutate(\n    units = substr(units, 1, stringr::str_locate(units, \"\\\\s\")[,1]-1)\n  ) |> \n  dplyr::mutate(\n    label = stringr::str_remove_all(source, \"https://en.wikipedia.org/wiki/\") |>\n      stringr::str_replace_all(\"_\", \" \")\n    ) |>\n  dplyr::filter(!is.na(value)) |>\n  dplyr::group_by(key) |>\n  dplyr::mutate(increment = 1:dplyr::n()) |>\n  dplyr::select(key, increment, label, name = X1, value, units) |>\n  dplyr::mutate(units = dplyr::case_when(\n    units %in% c(\"tonnes\", \"ton\", \"tonnes,\", \"tons\") ~ \"t\",\n    units %in% c(\"long\") ~ \"lt\",\n    units %in% c(\"metres\") ~ \"m\",\n    TRUE ~ units\n  )) |>\n  dplyr::mutate(\n    value = dplyr::case_when(\n      units %in% \"lt\" ~ (value * 1.016047), \n      units %in% \"ft\" ~ (value * 0.3048),\n      TRUE ~ value\n    )\n  ) |>\n  dplyr::mutate(\n    units = dplyr::case_when(\n       units %in% \"lt\" ~ \"t\",\n       units %in% \"ft\" ~ \"m\",\n       TRUE ~ units\n    )\n  ) |>\n  dplyr::ungroup() |>\n  dplyr::select(-units, -key, -increment) |> \n  dplyr::group_by(label, name) |>\n  dplyr::mutate(increment = 1:dplyr::n()) |>\n  tidyr::pivot_wider(names_from = name, values_from = value) |>\n  tidyr::fill(c(Length, Beam, Draught)) \n\nx <- GGally::ggpairs(performance, lower= list(mapping = ggplot2::aes(colour = label)), columns = 3:6) \n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nplotly::ggplotly(x)\n\nWarning: Can only have one: highlight\n\nWarning: Can only have one: highlight\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\nWarning: Removed 3 rows containing non-finite values (stat_density).\n\n\nWarning: Can only have one: highlight"
  },
  {
    "objectID": "posts/20230401-wikidata-extract-preceded-by/20230401-extract-preceded-by.html",
    "href": "posts/20230401-wikidata-extract-preceded-by/20230401-extract-preceded-by.html",
    "title": "Extract of UK Frigates and Their Precendents",
    "section": "",
    "text": "Objective\nExtract list of UK Frigates and their precedents.\n\n\nData\nWe use wikidata to first extract the different types of warship\n\nq &lt;- '\nSELECT ?ship_type ?ship_typeLabel WHERE {\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n  ?ship_type wdt:P31 wd:Q2235308;\n    wdt:P279 wd:Q3114762.\n  \n  OPTIONAL {  }\n}\n  '\n\nWikidataQueryServiceR::query_wikidata(q)\n\nRows: 55 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): ship_type, ship_typeLabel\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 55 × 2\n   ship_type                              ship_typeLabel  \n   &lt;chr&gt;                                  &lt;chr&gt;           \n 1 http://www.wikidata.org/entity/Q17205  aircraft carrier\n 2 http://www.wikidata.org/entity/Q104843 cruiser         \n 3 http://www.wikidata.org/entity/Q161705 frigate         \n 4 http://www.wikidata.org/entity/Q170013 corvette        \n 5 http://www.wikidata.org/entity/Q174736 destroyer       \n 6 http://www.wikidata.org/entity/Q182531 battleship      \n 7 http://www.wikidata.org/entity/Q188924 galley          \n 8 http://www.wikidata.org/entity/Q207452 ship of the line\n 9 http://www.wikidata.org/entity/Q629990 bireme          \n10 http://www.wikidata.org/entity/Q640078 minelayer       \n# … with 45 more rows\n\n\nAnd subsequently different types of frigates operated by the Royal Navy\n\nq &lt;- '\nSELECT ?ship_class ?ship_classLabel ?service_entry ?followed_by ?followed_byLabel ?operator ?operatorLabel ?image ?service_retirement ?follows ?followsLabel ?service_entryLabel ?service_retirementLabel ?imageLabel WHERE {\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n  ?ship_class wdt:P31 wd:Q559026; # instance of ship class\n    wdt:P279 wd:Q106070703. # subclass of frigate\n  ?ship_class wdt:P137 wd:Q172771. # operated by RN\n  OPTIONAL { ?ship_class wdt:P729 ?service_entry. }\n  OPTIONAL { ?ship_class wdt:P730 ?service_retirement. }\n  OPTIONAL { ?ship_class wdt:P155 ?follows. }\n  OPTIONAL { ?ship_class wdt:P156 ?followed_by. }\n  OPTIONAL { ?ship_class wdt:P137 ?operator. }\n  OPTIONAL { ?ship_class wdt:P18 ?image. }\n}\n'\nrn_frigates &lt;- WikidataQueryServiceR::query_wikidata(q)\n\nRows: 83 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): ship_class, ship_classLabel, followed_by, followed_byLabel, opera...\ndttm  (4): service_entry, service_retirement, service_entryLabel, service_re...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nrn_frigates |&gt;\n  dplyr::filter(operatorLabel == \"Royal Navy\") |&gt;\n  dplyr::select(from = ship_classLabel, \n                to = followed_byLabel, \n                value = service_entry) |&gt;\n  dplyr::distinct() |&gt;\n  dplyr::filter(!is.na(to)) |&gt;\n  igraph::graph_from_data_frame() |&gt;\n  visNetwork::visIgraph()\n\n\n\n\n\nNext step. Extract data on type 26, type 23, type 22, type 21, leander class, rothesay-class, valour-class, whiteby-class and saisbury class.\nFuture intention is to compare to other nations where data are available."
  },
  {
    "objectID": "posts/20230401-wikidata-type23/20230401-wikidata-type23.html",
    "href": "posts/20230401-wikidata-type23/20230401-wikidata-type23.html",
    "title": "Extract Type 23 Data",
    "section": "",
    "text": "Objective\nExtract Type 23 performance data from wikipedia\n\n\nData\n\nurl &lt;- \"https://en.wikipedia.org/wiki/Type_23_frigate\"\nreq &lt;- httr::GET(url) |&gt;\n  httr::content()\n\nvars &lt;- c(\"Name\", \"Displacement\", \"Length\", \"Beam\", \"Draught\")\n(df &lt;- req |&gt;\n  xml2::xml_find_all(\"//table[@class = 'infobox']\") |&gt;\n  rvest::html_table() |&gt;\n  purrr::pluck(1) |&gt;\n  dplyr::filter(X1 %in% c(vars)) |&gt;\n  t() |&gt; dplyr::as_tibble(.name_repair = \"minimal\") |&gt;\n  janitor::row_to_names(1) |&gt;\n  dplyr::mutate(dplyr::across(-1, ~stringr::str_sub(.x, 1, stringr::str_locate(.x, \"\\\\(\")[,1]-2))) |&gt;\n  tidyr::pivot_longer(-1) |&gt;\n  dplyr::rename(id = Name) |&gt;\n  dplyr::mutate(unit = substr(value, stringr::str_locate(value, \"\\\\s\")[,1]+1, nchar(value))) |&gt;\n  dplyr::mutate(value = stringr::str_remove_all(value, \"[a-z]|,\")) |&gt;\n  dplyr::mutate(value = stringr::str_trim(value)) |&gt;\n  dplyr::mutate(value = as.numeric(value))\n)\n\n# A tibble: 4 × 4\n  id              name          value unit \n  &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;\n1 Type 23 frigate Displacement 4900   t    \n2 Type 23 frigate Length        133   m    \n3 Type 23 frigate Beam           16.1 m    \n4 Type 23 frigate Draught         7.3 m    \n\ndf |&gt; dplyr::select(-unit) |&gt; tidyr::pivot_wider()\n\n# A tibble: 1 × 5\n  id              Displacement Length  Beam Draught\n  &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Type 23 frigate         4900    133  16.1     7.3\n\n\n\nlibrary(GGally)\n\nWarning: package 'GGally' was built under R version 4.2.3\n\n\nLoading required package: ggplot2\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n# From the help page:\ndata(flea)\nggpairs(flea, columns = 2:4, ggplot2::aes(colour=species))"
  },
  {
    "objectID": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Extract-People-from-Text.html",
    "href": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Extract-People-from-Text.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Code\npeople <- purrr::map(df$fields.body,~{\n  text <- .x |>\n    xml2::read_html() |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text()\n    \n  s <- NLP::String(paste(text, collapse = \"\\n\"))\n\n  ## Need sentence and word token annotations.\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  a2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n\n## Entity recognition for persons.\n  entity_annotator <- openNLP::Maxent_Entity_Annotator()\n\n  s[entity_annotator(s, a2)]\n})\n\n\n\n\nCode\norg <- purrr::map(df$fields.body,~{\n  text <- .x |>\n    xml2::read_html() |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text()\n    \n  s <- NLP::String(paste(text, collapse = \"\\n\"))\n\n  ## Need sentence and word token annotations.\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  a2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n\n## Entity recognition for persons.\n  entity_annotator <- openNLP::Maxent_Entity_Annotator(kind = \"organization\")\n\n  s[entity_annotator(s, a2)]\n}, .progress = TRUE)\n\n\n\n\nCode\nloc <- purrr::map(df$fields.body,~{\n  text <- .x |>\n    xml2::read_html() |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text()\n    \n  s <- NLP::String(paste(text, collapse = \"\\n\"))\n\n  ## Need sentence and word token annotations.\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  a2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n\n## Entity recognition for persons.\n  entity_annotator <- openNLP::Maxent_Entity_Annotator(kind = \"location\")\n\n  s[entity_annotator(s, a2)]\n}, .progress = TRUE)"
  },
  {
    "objectID": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Ukraine-What-We-Know-Guardian-NLP.html",
    "href": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Ukraine-What-We-Know-Guardian-NLP.html",
    "title": "Process Guardian - What we know on Ukraine",
    "section": "",
    "text": "Code\npages <- readr::read_csv(\"2023-05-18-guardian-ukraine-what-we-know.csv\")\n\n\nRows: 440 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (12): id, type, sectionId, sectionName, webTitle, webUrl, apiUrl, field...\nlgl   (1): isHosted\ndttm  (1): webPublicationDate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nprocess_text <- function(x){\n  xml2:::read_html(x) |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text() |>\n    stringr::str_split(\"\\n\") |>\n    unlist() |>\n    stringr::str_trim() |>\n    stringi::stri_remove_empty()\n}\n\nbody <- purrr::map(pages$fields.body, ~process_text(.x)) \n\n\n\n\nCode\ntop3 <- purrr::map(body, ~{\n  lexRankr::lexRank(.x,\n                    # only 1 article; repeat same docid for all of input vector\n                    docId = rep(1, length(.x)),\n                    # return 3 sentences\n                    n = 3,\n                    continuous = TRUE, \n                    Verbose = FALSE)\n})\n\ntop3_sent <- purrr::map(top3, ~.x$sentence)\n\n\n\n\nCode\n# Summarise\npages |>\n  dplyr::mutate(fields.headline = stringr::str_trim(fields.headline)) |>\n  dplyr::mutate(day = stringr::str_extract(fields.headline, \"day [0-9]{1,3}\")) |> \n  dplyr::mutate(day = stringr::str_remove_all(day, \"day \") |> as.numeric()) |>\n  dplyr::select(webPublicationDate, day,fields.trailText) |>\n  reactable::reactable(\n    details = function(index){\n      top3_i <- data.frame(top3_sent[[index]], stringsAsFactors = FALSE)\n      tbl <- reactable::reactable(top3_i, outlined = TRUE, highlight = TRUE, fullWidth = TRUE)\n      htmltools::div(style = list(margin = \"12px 45px\"), tbl)\n    },\n    onClick = \"expand\",\n    rowStyle = list(cursor = \"pointer\")\n  )\n\n\n\n\n\n\n\nCode\n#write.csv(pages, paste0(Sys.Date(), \"-guardian-ukraine-what-we-know.csv\"), row.names = FALSE)"
  },
  {
    "objectID": "posts/20230519-Voluntary Job Separation Rates/20230519-vjs-rates.html",
    "href": "posts/20230519-Voluntary Job Separation Rates/20230519-vjs-rates.html",
    "title": "Calculate Voluntary Job Separation Rates",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\n\n# This script calculates voluntary and involuntary job separations from the \n# labour market. \n\n#+ Setup ----\n\n# Connect to Database\ncon <- DBI::dbConnect(\n  odbc::odbc(), driver = \"PostgreSQL ODBC Driver(Unicode)\", \n  database = \"lfs\", uid = \"lheley\", host = \"localhost\", pwd = \"lheley\", \n  port = 5432, maxvarcharsize = 0\n)\n\n# Extract Meta Data\nstudy_meta <- con |> \n    dplyr::tbl(\"2qlfs_index\") |>\n    dplyr::select(1:4) |>\n    dplyr::collect() |>\n    dplyr::left_join(\n      con |>\n        dplyr::tbl(\"study_filename_lu\") |>\n        dplyr::collect() |>\n        dplyr::rename(SN = study) |>\n        dplyr::mutate(SN = as.numeric(SN)),\n      by = \"SN\"\n    )\n\n\n#+ Define Variables ----\n# These variables were defined through comparison of the previous\n# ONS publications. \n\n# Where the question or possible responses to a question have changed\n# the variable is updated. \n\nwnleft <- c(\"WNEFT112\",\"WNLEFT2\")\nrelft <- c(\"REDYL112\",\"REDYL132\",\"REDYLFT2\")\nsector <- \"PUBLICR1\"\nemployment <- \"ILODEFR1\"\nage <- \"AGE1\"\nid <- c(\"PERSID\")\nlgwt <- c(\"LGWT\",\"LGWT18\", \"LGWT20\") # This responds to different population weights.\nindustry <- \"INDD07M1\"\nvars <- c(id, lgwt, wnleft, relft, sector, employment, age, industry)\n\ntbls <- DBI::dbListTables(con)\ntbls <- tbls[grepl(\"sn_\", tbls)]\n\nvariables <- tbls |>\n  purrr::map_df(function(tbl){\n    variables <- con |>\n      dplyr::tbl(tbl) |>\n      dplyr::select(tidyselect::any_of(vars)) |>\n      head() |>\n      dplyr::collect() |>\n      names()\n    \n    tibble::tibble(study = substr(tbl, 4, 9), variables)\n  })\n\n#+ Select tables the contain the variables we need -----\ntbls <- paste0(\"sn_\", variables |> \n                 dplyr::filter(variables %in% vars) |>\n                 dplyr::mutate(variables2 = dplyr::case_when(\n                   variables %in% relft ~ \"REDYLFT\",\n                   variables %in% wnleft ~ \"WNLEFT\",\n                   variables %in% lgwt ~ \"LGWT\",\n                   TRUE ~ variables\n                 )) |>\n                 dplyr::select(-variables) |>\n                 dplyr::mutate(value = 1) |>\n                 tidyr::pivot_wider(names_from = variables2, values_from = value)  |>\n                 na.omit() |>\n                 dplyr::select(study) |> dplyr::pull())\n\n\n#+ Calculate Statistics ------\n# Loop through the table. \n# Calculate the number of people that reason for leaving was voluntary separation\n# Calculate the overall people\n# This can be extended to include involuntary job separations\n\n\n# This code chunk loops through the selected tables\n# It selects variables which match our specified variables in 'vars'\n# It then renames variables with inconsistent names\n# Then filters between 16 and 65\n# It recode reason left to determine voluntary job separations\n# It calculates whether individual left employmnet in last three months\n# It then groups by sector and calculate the weighted and unweighted number of \n#  voluntary job separates by total sector size\nvjs <- tbls |>\n  purrr::map_df(function(tbl){\n    sql_tbl <- con |> dplyr::tbl(tbl) \n    sql_tbl |>\n      dplyr::select(tidyselect::any_of(vars)) |>\n      dplyr::rename(REDYLFT = tidyselect::any_of(relft)) |>\n      dplyr::rename(WNLEFT = tidyselect::any_of(wnleft)) |>\n      dplyr::rename(LGWT = tidyselect::any_of(lgwt))|>\n      dplyr::filter(AGE1 >= 16 & AGE1 < 65) |>\n      dplyr::collect() |>\n      dplyr::mutate(VJS = dplyr::case_when(\n        \"REDYLFT2\" %in% dplyr::tbl_vars(sql_tbl) ~  REDYLFT %in% 4:9,\n        \"REDYL112\" %in% dplyr::tbl_vars(sql_tbl) ~  REDYLFT %in% 4:10,\n        \"REDYL132\" %in% dplyr::tbl_vars(sql_tbl) ~  REDYLFT %in% c(3, 5:11)\n      )) |>\n      dplyr::mutate(LFT3M = WNLEFT == 1 & ILODEFR1 == 1) |> \n      dplyr::mutate(VJS_3M =  VJS & LFT3M) |>\n      dplyr::mutate(EMP = ILODEFR1 == 1) |>\n      dplyr::mutate(PUBLIC = PUBLICR1 == 2) |>\n      dplyr::mutate(PRIVATE = PUBLICR1 == 1) |>\n      dplyr::mutate(SECTOR = ifelse(PUBLIC, \"Public\", ifelse(PRIVATE, \"Private\", NA))) |>\n      dplyr::group_by(SECTOR) |>\n      dplyr::summarise(vjs_3m_w = crossprod(LGWT, VJS_3M)[1],\n                       vjs_3m = sum(VJS_3M),\n                       n_w = crossprod(EMP, LGWT)[1],\n                       n = sum(EMP), tbl = tbl)\n  })\n\nvjs_total <- study_meta |> \n  dplyr::select(tbl = SN, sitdate = End) |>\n  dplyr::mutate(tbl = paste(\"sn\", tbl, sep = \"_\")) |>\n  dplyr::collect() |>\n  dplyr::left_join(vjs, by = \"tbl\")  |>\n  dplyr::group_by(sitdate) |>\n  dplyr::summarise(vjs_3m_w = sum(vjs_3m_w),\n                   n_w = sum(n_w)) |>\n  dplyr::mutate(vjs_rate = vjs_3m_w / n_w) \n\nvjs_sector <- study_meta |> \n  dplyr::select(tbl = SN, sitdate = End) |>\n  dplyr::mutate(tbl = paste(\"sn\", tbl, sep = \"_\")) |>\n  dplyr::collect() |>\n  dplyr::left_join(vjs, by = \"tbl\")  |>\n  dplyr::filter(!is.na(SECTOR)) |>\n  dplyr::group_by(sitdate, sector = SECTOR) |>\n  dplyr::summarise(vjs_3m_w = sum(vjs_3m_w),\n                   n_w = sum(n_w)) |>\n  dplyr::mutate(vjs_rate = vjs_3m_w / n_w) \n\n\n`summarise()` has grouped output by 'sitdate'. You can override using the\n`.groups` argument.\n\n\nCode\nvjs_total$vjs_rate[vjs_total$vjs_rate == 0] <- NA\nvjs_sector$vjs_rate[vjs_sector$vjs_rate == 0] <- NA\n\n\nggplot(vjs_total) + \n  geom_line(aes(sitdate, vjs_rate))\n\n\nWarning: Removed 66 rows containing missing values (`geom_line()`).\n\n\n\n\n\nCode\nggplot(vjs_sector) +\n  geom_line(aes(sitdate, vjs_rate)) +\n  facet_wrap(~sector)\n\n\n\n\n\nCode\nDBI::dbDisconnect(con)"
  },
  {
    "objectID": "posts/20230530-Homo Deus NLP/homo_deus_nlp.html",
    "href": "posts/20230530-Homo Deus NLP/homo_deus_nlp.html",
    "title": "Summarise Information in a Book",
    "section": "",
    "text": "Load the Raw Data using PDFTOOLS\n\n\nCode\nraw &lt;- pdftools::pdf_text(\"homo_deus_chapter_1.pdf\")\n\n\n\n\nExtract the Manuscript\n\n\nCode\nmanuscript &lt;- paste(raw, collapse = \"\")\n\n\n\n\nSplit the manuscript into section heading and text.\n\n\nCode\nsection_text &lt;- dplyr::tibble(manuscript) |&gt; \n  dplyr::mutate(section = stringr::str_split(manuscript, \"\\n\\n\\n\")) |&gt; \n  tidyr::unnest(cols = \"section\") |&gt; \n  dplyr::select(-manuscript) |&gt; \n  dplyr::filter(!stringr::str_detect(section, \"\\n\\n\")) |&gt; \n  dplyr::slice(3:dplyr::n()) |&gt; \n  dplyr::mutate(section = stringr::str_trim(section)) |&gt; \n  tidyr::separate(section, c(\"section\", \"text\"), \"\\n\", extra = \"merge\") |&gt; \n  dplyr::mutate(section_id = 1:dplyr::n())\n\n\n\n\nLexrankr\n\n\nCode\nlex_top_3 &lt;- seq_along(section_text$text) |&gt; \n  purrr::map_df(~{\n    lexRankr::lexRank(\n      section_text$text[[.x]],\n      docId = .x, \n      n = 5, \n      continuous = TRUE, \n      Verbose = FALSE\n    ) }) |&gt; \n  tibble::as_tibble() |&gt; \n  dplyr::mutate(docId = as.numeric(docId))\n\n\n\n\nResults\n\n\nCode\ndplyr::left_join(section_text, lex_top_3, by = c(\"section_id\" = \"docId\")) |&gt; \n  dplyr::select(section_id, section, sentenceId, sentence, value) |&gt; \n  DT::datatable(options = list(pageLength = 5))"
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "This method estimates the impact of an intervention by using a cut-off threshold to assign the intervention. The method relies on the assumption that the individuals just below and above a cutoff threshold will be similar, with the only significant difference between the 2 groups being whether they received the intervention or not. By comparing the value of the outcome variable for the individuals just above and below the cut-off threshold, the method infers the impact of the intervention (Treasury 2020)."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#references",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#references",
    "title": "Learn Regression Discontinuity Design",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#introduction",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#introduction",
    "title": "Regression Discontinuity Design",
    "section": "Introduction",
    "text": "Introduction\n\nSocial programmes often have eligability indices that determine who can and cannot signup.\nExamples include targeted antipoverty programmes based on means. Tests scores.\n\n\nRegression discontinuity design (RDD) is an impact evaluation method that is adequate for programs that use a continuous index to rank potential participants and that have a cutoff point along the index that determines whether or not potential participants are eligible to receive the program."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#conditions",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#conditions",
    "title": "Regression Discontinuity Design",
    "section": "Conditions",
    "text": "Conditions\nTo apply RDD 4 main conditions must be met:\n\nThe index must rank people in a continuous way. By contrast, variables that have discrete categories cannot be ranked.\nThere must be a clear defined cutoff score. The value above or below that the population is eligable for the programme.\nThe cutoff must be unique to the programme of interest. There must be no other programme, apart from the one to be evaluated that uses the same cutoff score.\nThe score of a particular individual or unit cannot be manipuated by data collectors, beneficiaries, administrators, or politicians.\n\nRDD esitmate impact around the eligability cutoff as difference between average outcome for units on the treated side of eligability cutoff and average outcome of units on the untreated (comparison) side of the cutoff.\nAn example based on agriculture where the scheme is determine by hectare of land is provided.\nIt makes the point that since the comparison group is made up of farm just above the eligability threshold, teh impact given by RDD is valid only locally. Therefore we obtain an estimate of the Local Average Treatment Effect (LATE). The RDD will not be able to identify the impact on the smallest farms.\nAn advantage is that when the eligability rule is applied, not eligable units need ot be left untreated for the purpose of the impact evaluation. The trade off is that observations far away from the cutoff will not be known."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#fuzzy-rdd",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#fuzzy-rdd",
    "title": "Regression Discontinuity Design",
    "section": "Fuzzy RDD",
    "text": "Fuzzy RDD\nFuzzy RDD. Some units who qualify for the programme may not opt to participate. Or others may find a way to participate even if they are on the other side of the line.\nWhen all units comply with the assignment that corresponds to them on the basis of their eligibility index, we say that the RDD is “sharp,” while if there is noncompliance on either side of the cutoff , then we say that the RDD is “fuzzy”.\nIf the RDD is fuzzy, we can use the instrumental variable approach to correct for the noncompliance. In the case of randomized assignment with noncompliance, we used the randomized assignment as the instrumental variable that helped us correct for noncompliance.\nIn the case of RDD, we can use the original assignment based on the eligibility index as the instrumental variable. Doing so has a drawback, though: our instrumental RDD impact estimate will be further localized—in the sense that it is no longer valid to all observations close to the cutoff , but instead represents the impact for the subgroup of the population that is located close to the cutoff point and that participates in the program only because of the eligibility criteria.\n\n\n\nGertler et al 2016 Figure 6.3"
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#testing-validity-of-rdd",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#testing-validity-of-rdd",
    "title": "Regression Discontinuity Design",
    "section": "Testing Validity of RDD",
    "text": "Testing Validity of RDD\nFor RDD to provide unbiased estimates of LATE there must be no manipulation of the eligibility index around the cutoff.\nA tell tale sign of manipuation is where there is bunching around the cutoff, e.g. look at the percentage of households at the cuttoff. A second test is to plot the eligability index against the outcome index at baseline and check there are no discontinuities or jump right around the cutoff."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#limitations-and-interpretations",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#limitations-and-interpretations",
    "title": "Regression Discontinuity Design",
    "section": "Limitations and Interpretations",
    "text": "Limitations and Interpretations\nBecause the RDD method estimates the impact of the program around the cutoff score, or locally, the estimate cannot necessarily be generalized to units whose scores are further away from the cutoff score: that is, where eligible and ineligible individuals may not be as similar."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#checklist",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#checklist",
    "title": "Regression Discontinuity Design",
    "section": "Checklist",
    "text": "Checklist\n\nIs the index continuous around the cutoff score at the time of the baseline?\nIs there any evidence of noncompliance with the rule that determines eligibility for treatment? Test whether all eligible units and no ineligible units have received the treatment. If you find noncompliance, you will need to combine RDD with an instrumental variable approach to correct for this \"fuzzy discontinuity.\"\nIs there any evidence that index scores may have been manipulated in order to influence who qualified for the program? Test whether the distribution of the index score is smooth at the cutoff point. If you fi nd evidence of \"bunching\" of index scores either above or below the cutoff point, this might indicate manipulation.\nIs the cutoff unique to the program being evaluated, or is the cutoff used by other programs as well?\n\nFor a review of practical issues in implementing RDD, see Imbens and Lemieux (2008)"
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#how-is-it-performed",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#how-is-it-performed",
    "title": "Regression Discontinuity Design",
    "section": "How is it performed?",
    "text": "How is it performed?\n\\[Y = \\beta_0 + \\beta_1 (Running-Cutoff) + \\beta_2 Treated + \\beta_3 (Running - Cutoff) * Treated + \\varepsilon\\]\nThis is a simple linear approach to regression discontinuity, where \\(\\Running\\) is the running variable which we have centred around the cutoff using \\((Running-Cutoff)\\). This takes a negative value to the left of the cutoff, zero at the cutoff, and a positive value to the right. We’re talking about a sharp regression discontinuity here, so \\(Treated\\) is both an indicator for being treated and an indicator for being above the cutoff - these are the same thing. The model is generally estimated using heteroskedasticity-robust standard errors, as one might expect the discontinuity and general shape of the line we’re fitting to exhibit heteroskedasticity in most cases.We’ll ignore the issue of a bandwidth for now and come back to it later - this regression approach can be applied whether you use all the data or limit yourself to a bandwidth around the cutoff.\nNotice the lack of control variables In most other chapters, when I do this it’s to help focus your attention on the design. Here, it’s very intentional. The whole idea of regression discontinuity is that you have nearly random assignment on either side of the cutoff. You shouldn’t need control variables because the design itself should close any back doors. No open back doors? No need for controls. Adding controls implies you don’t believe the assumptions necessary for the regression discontinuity method to work, and makes the whole thing becomes a bit suspicious to any readers."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#r-code",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#r-code",
    "title": "Regression Discontinuity Design",
    "section": "R code",
    "text": "R code\nLet’s code up some regression discontinuity! I’m going to do this in two ways. First I’m going to run a plain-ol’ ordinary least squares model with a bandwidth and kernel weight applied, with heteroskedasticity-robust standard errors. However, there are a number of other adjustments we’ll be talking about in this chapter, and it can be a good idea to pass this task off to a package that knows what it’s doing. I see no point in showing you code you’re unlikely to use just because we haven’t gotten to the relevant point in the chapter yet. So we’ll be using those specialized commands as well, and I’ll talk later about some of the extra stuff they’re doing for us.\nFor this example we’re going to use data from Government Transfers and Political Support by Manacorda, Miguel, and Vigorito (2011Manacorda, Marco, Edward Miguel, and Andrea Vigorito. 2011. “Government Transfers and Political Support.” American Economic Journal: Applied Economics 3 (3): 1–28.). This paper looks at a large poverty alleviation program in Uruguay which cut a sizeable check to a large portion of the population. They are interested in whether receiving those funds made people more likely to support the newly-installed center-left government that sent them.\nWho got the payments? You had to have an income low enough. But they didn’t exactly use income as a running variable; that might have been too easy to manipulate. Instead, the government used a bunch of factors - housing, work, reported income, schooling - and predicted what your income would be from that. Then, the predicted income was the running variable, and treatment was assigned based on being below a cutoff. About 14% of the population ended up getting payments.\nThe researchers polled a bunch of people near the income cutoff to check their support for the government afterwards. Did people just below the income cutoff support the government more than those just above?\nThe data set we have on government transfers comes with the predicted-income variable pre-centered so the cutoff is at zero. Then, support for the government takes three values: you think they’re better than the previous government (1), the same (1/2) or worse (0). The data only includes individuals near the cutoff - the centered-income variable in the data only goes from -.02 to .02.\nBefore we estimate our model, let’s do a nearly-compulsory graphical regression discontinuity check so we can confirm that there does appear to be some sort of discontinuity where we expect it. This is a “plot of binned means.” There are some preprogrammed ways to do this like rdplot in R or Stata in the rdrobust package or binscatter in Stata from the binscatter package, but this is easy enough that we may as well do it by hand and flex those graphing muscles.\nFor one of these graphs, you generally want to (1) slice the running variable up into bins (making sure the cutoff is the edge between two bins), (2) take the mean of the outcome within each of the bins, and (3) plot the result, with a vertical line at the cutoff so you can see where the cutoff is. Then you’d generally repeat the process with treatment instead of the outcome to produce a graph like below.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ngt &lt;- causaldata::gov_transfers\n\n# Use cut() to create bins, using breaks to make sure it breaks at 0\n# (-15:15)*.02/15 gives 15 breaks from -.02 to .02\nbinned &lt;- gt %&gt;%\n    mutate(Inc_Bins = cut(Income_Centered,\n           breaks = (-15:15)*(.02/15))) %&gt;%\n    group_by(Inc_Bins) %&gt;%\n    summarize(Support = mean(Support),\n    Income = mean(Income_Centered))\n# Taking the mean of Income lets us plot data roughly at the bin midpoints\n\nggplot(binned, aes(x = Income, y = Support)) + \n    geom_line() + \n    # Add a cutoff line\n    geom_vline(aes(xintercept = 0), linetype = 'dashed')\n\n\n\n\n\nNow that we have our graph in mind, we can actually estimate our model. We’ll start doing it with OLS and a second-order polynomial, and then we’ll do a linear model with a triangular kernel weight, limiting the bandwidth around the cutoff to .01 on either side. The bandwidth in this case isn’t super necessary - the data is already limited to .02 on either side around the cutoff - but this is just a demonstration.\nIt’s important to note that the first step of doing regression discontinuity with OLS is to center the running variable around the cutoff. This is as simple as making the variable \\(RunningVariable - Cutoff\\), translated into whatever language you use. The second step would then be to create a “treated” variable \\(RunningVariable &lt; Cutoff\\) (since treatment is applied below the cutoff in this instance). But in this case, the running variable comes pre-centered (\\(IncomeCentred\\)) and the below-cutoff treatment variable is already in the data (\\(Participation\\) ) so I’ll leave those parts out.\n\n\nCode\nlibrary(tidyverse); library(modelsummary)\n\n\nWarning: package 'modelsummary' was built under R version 4.3.1\n\n\nCode\ngt &lt;- causaldata::gov_transfers\n\n# Linear term and a squared term with \"treated\" interactions\nm &lt;- lm(Support ~ Income_Centered*Participation +\n       I(Income_Centered^2)*Participation, data = gt)\n\n# Add a triangular kernel weight\nkweight &lt;- function(x) {\n    # To start at a weight of 0 at x = 0, and impose a bandwidth of .01, \n    # we need a \"slope\" of -1/.01 = 100, \n    # and to go in either direction use the absolute value\n    w &lt;- 1 - 100*abs(x)\n    # if further away than .01, the weight is 0, not negative\n    w &lt;- ifelse(w &lt; 0, 0, w)\n    return(w)\n}\n\n# Run the same model but with the weight\nmw &lt;- lm(Support ~ Income_Centered*Participation, data = gt,\n         weights = kweight(Income_Centered))\n\n# See the results with heteroskedasticity-robust SEs\nmsummary(list('Quadratic' = m, 'Linear with Kernel Weight' = mw), \n    stars = c('*' = .1, '**' = .05, '***' = .01), vcov = 'robust')\n\n\nWarning in residuals^2/(1 - diaghat)^2: longer object length is not a multiple\nof shorter object length\n\n\nWarning in residuals^2/(1 - diaghat)^2: longer object length is not a multiple\nof shorter object length\n\n\n\n\n\n\n\nhttps://rdpackages.github.io/\n\n\nCode\nlibrary(tidyverse); library(rdrobust)\n\n\nWarning: package 'rdrobust' was built under R version 4.3.1\n\n\nCode\ngt &lt;- causaldata::gov_transfers\n\n# Estimate regression discontinuity and plot it\nm &lt;- rdrobust(gt$Support, gt$Income_Centered, c = 0)\n\n\nWarning in rdrobust(gt$Support, gt$Income_Centered, c = 0): Mass points\ndetected in the running variable.\n\n\nCode\nsummary(m)\n\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1948\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                 1127          821\nEff. Number of Obs.             291          194\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.005        0.005\nBW bias (b)                   0.010        0.010\nrho (h/b)                     0.509        0.509\nUnique Obs.                     841          639\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.025     0.062     0.396     0.692    [-0.098 , 0.147]     \n        Robust         -         -     0.624     0.533    [-0.097 , 0.188]     \n=============================================================================\n\n\nCode\n# Note, by default, rdrobust and rdplot use different numbers\n# of polynomial terms. You can set the p option to standardize them.\nrdplot(gt$Support, gt$Income_Centered)\n\n\n[1] \"Mass points detected in the running variable.\"\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse); library(fixest); library(modelsummary)\n\n\nWarning: package 'fixest' was built under R version 4.3.1\n\n\nCode\nvet &lt;- causaldata::mortgages\n\n# Create an \"above-cutoff\" variable as the instrument\nvet &lt;- vet %&gt;% mutate(above = qob_minus_kw &gt; 0)\n\n# Impose a bandwidth of 12 quarters on either side\nvet &lt;- vet %&gt;%  filter(abs(qob_minus_kw) &lt; 12)\n\nm &lt;- feols(home_ownership ~\n    nonwhite  | # Control for race\n    bpl + qob | # fixed effect controls\n    qob_minus_kw*vet_wwko ~ # Instrument our standard RDD\n    qob_minus_kw*above, # with being above the cutoff\n    se = 'hetero', # heteroskedasticity-robust SEs\n    data = vet) \n\n# And look at the results\nmsummary(m, stars = c('*' = .1, '**' = .05, '***' = .01))\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse); library(rdrobust)\nvet &lt;- causaldata::mortgages\n\n# It will apply a bandwidth anyway, but having it\n# check the whole bandwidth space will be slow. So let's\n# pre-limit it to a reasonable range of 12 quarters\nvet &lt;- vet %&gt;%\n    filter(abs(qob_minus_kw) &lt;= 12)\n\n# Create our matrix of controls\ncontrols &lt;- vet %&gt;%\n    select(nonwhite, bpl, qob) %&gt;%\n    mutate(qob = factor(qob))\n# and make it a matrix with dummies\nconmatrix &lt;- model.matrix(~., data = controls)\n\n# This is fairly slow due to the controls, beware!\nm &lt;- rdrobust(vet$home_ownership,\n              vet$qob_minus_kw,\n              fuzzy = vet$vet_wwko,\n              c = 0,\n              covs = conmatrix)\n\n\nWarning in rdrobust(vet$home_ownership, vet$qob_minus_kw, fuzzy = vet$vet_wwko,\n: Multicollinearity issue detected in covs. Redundant covariates dropped.\n\n\nWarning in rdrobust(vet$home_ownership, vet$qob_minus_kw, fuzzy = vet$vet_wwko,\n: Mass points detected in the running variable.\n\n\nCode\nsummary(m)\n\n\nCovariate-adjusted Fuzzy RD estimates using local polynomial regression.\n\nNumber of Obs.                56901\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                28776        28125\nEff. Number of Obs.            6911         6756\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   3.387        3.387\nBW bias (b)                   5.354        5.354\nrho (h/b)                     0.633        0.633\nUnique Obs.                      12           12\n\nFirst-stage estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.018     0.018     0.964     0.335    [-0.018 , 0.053]     \n        Robust         -         -     2.244     0.025     [0.006 , 0.094]     \n=============================================================================\n\nTreatment effect estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    -5.377     5.716    -0.941     0.347   [-16.581 , 5.827]     \n        Robust         -         -     0.513     0.608   [-10.182 , 17.408]    \n=============================================================================\n\n\nHeiss (2020)"
  },
  {
    "objectID": "posts/20230829-Synthetic Control Methods/synthetic-control.html",
    "href": "posts/20230829-Synthetic Control Methods/synthetic-control.html",
    "title": "Synthetic Control Methods",
    "section": "",
    "text": "Use of historical data to construct a ‘synthetic clone’ of a group receiving a particular intervention. Differences between the performance of the actual group and its synthetic clone may be used as evidence that the intervention has had an effect. Most commonly applied to interventions applied at an area level (Treasury 2020)."
  },
  {
    "objectID": "posts/20230829-Synthetic Control Methods/synthetic-control.html#inference",
    "href": "posts/20230829-Synthetic Control Methods/synthetic-control.html#inference",
    "title": "Synthetic Control Methods",
    "section": "Inference",
    "text": "Inference\nFor inference, the method relies on repeating the method for every donor in the donor pool exactly as was done for the treated unit — i.e. generating placebo synthetic controls). By setting generate_placebos = TRUE when initializing the synth pipeline with synthetic_control(), placebo cases are automatically generated when constructing the synthetic control of interest. This makes it easy to explore how unique difference between the observed and synthetic unit is when compared to the placebos.\n\n\nCode\nsmoking_out %&gt;% plot_placebos()\n\n\n\n\n\nNote that the plot_placebos() function automatically prunes any placebos that poorly fit the data in the pre-intervention period. The reason for doing so is purely visual: those units tend to throw off the scale when plotting the placebos. To prune, the function looks at the pre-intervention period mean squared prediction error (MSPE) (i.e. a metric that reflects how well the synthetic control maps to the observed outcome time series in pre-intervention period). If a placebo control has a MSPE that is two times beyond the target case (e.g. “California”), then it’s dropped. To turn off this behavior, set prune = FALSE.\n\n\nCode\nsmoking_out %&gt;% plot_placebos(prune = FALSE)\n\n\n\n\n\nFinally, Adabie et al. 2010 outline a way of constructing Fisher’s Exact P-values by dividing the post-intervention MSPE by the pre-intervention MSPE and then ranking all the cases by this ratio in descending order. A p-value is then constructed by taking the rank/total.1 The idea is that if the synthetic control fits the observed time series well (low MSPE in the pre-period) and diverges in the post-period (high MSPE in the post-period) then there is a meaningful effect due to the intervention. If the intervention had no effect, then the post-period and pre-period should continue to map onto one another fairly well, yielding a ratio close to 1. If the placebo units fit the data similarly, then we can’t reject the hull hypothesis that there is no effect brought about by the intervention.\nThis ratio can be easily plotted using plot_mspe_ratio(), offering insight into the rarity of the case where the intervention actually occurred.\n\n\nCode\nsmoking_out %&gt;% plot_mspe_ratio()\n\n\n\n\n\nFor more specific information, there is a significance table that can be extracted with one of the many grab_ prefix functions.\n\n\nCode\nsmoking_out %&gt;% grab_significance()\n\n\n# A tibble: 39 × 8\n   unit_name      type  pre_mspe post_mspe mspe_ratio  rank fishers_exact_pvalue\n   &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;                &lt;dbl&gt;\n 1 California     Trea…     3.17     392.      124.       1               0.0256\n 2 Georgia        Donor     3.79     179.       47.2      2               0.0513\n 3 Indiana        Donor    25.2      770.       30.6      3               0.0769\n 4 West Virginia  Donor     9.52     284.       29.8      4               0.103 \n 5 Wisconsin      Donor    11.1      268.       24.1      5               0.128 \n 6 Missouri       Donor     3.03      67.8      22.4      6               0.154 \n 7 Texas          Donor    14.4      277.       19.3      7               0.179 \n 8 South Carolina Donor    12.6      234.       18.6      8               0.205 \n 9 Virginia       Donor     9.81      96.4       9.83     9               0.231 \n10 Nebraska       Donor     6.30      52.9       8.40    10               0.256 \n# ℹ 29 more rows\n# ℹ 1 more variable: z_score &lt;dbl&gt;"
  },
  {
    "objectID": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html",
    "href": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html",
    "title": "Iran Sanctions - Sythetic Control",
    "section": "",
    "text": "Code\nlibrary(tidysynth)\nlibrary(wbstats)"
  },
  {
    "objectID": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html#data-and-method",
    "href": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html#data-and-method",
    "title": "Iran Sanctions - Sythetic Control",
    "section": "Data and Method",
    "text": "Data and Method\nData Source: World Bank Development Indicators\nTime Period: 2003 - 2015\nCountries: Algeria, Angola, Bahrain, Ecuador, Egypt, Iran, Israel, Jordan, Lebanon, Morocco, Nigeria, Oman, and Saudi Arabia.\nOutcome Variable: military expenditure (current US$) per capita.\nControl Variables: total population, imports of goods and services (constant US$), GDP per capita (constant US$) and real GDP per capita growth rate.\nSpecial Variables: military spending per capita in years 2010, 2008, 2006 and 2004\n\nData\n\n\nCode\n# Load Data from World Bank\nwb_countries &lt;- wb_countries(\"en\")\ncountries &lt;- c(\n  \"Bahrain\", \"Ecuador\", \"Egypt, Arab Rep.\", \n  \"Iran, Islamic Rep.\", \"Israel\", \"Jordan\", \n  \"Lebanon\", \"Morocco\", \"Nigeria\", \n  \"Oman\",  \"Saudi Arabia\"\n  )\n\ncountry_iso2c &lt;- wb_countries |&gt;\n  dplyr::filter(country %in% countries) |&gt;\n  dplyr::pull(iso2c)\n\nind &lt;- wb_indicators(\"en\", include_archive = FALSE)\nvars &lt;- c(\n  \"MS.MIL.XPND.CD\",\"SP.POP.TOTL\", \n  \"NE.IMP.GNFS.KD\", \"NY.GDP.PCAP.KD\", \n  \"NY.GDP.PCAP.KD.ZG\", \"BM.GSR.GNFS.CD\"\n  )\n\nind |&gt;\n  dplyr::filter(indicator_id %in% vars)\n\n\n# A tibble: 6 × 8\n  indicator_id indicator unit  indicator_desc source_org topics source_id source\n  &lt;chr&gt;        &lt;chr&gt;     &lt;lgl&gt; &lt;chr&gt;          &lt;chr&gt;      &lt;list&gt;     &lt;dbl&gt; &lt;chr&gt; \n1 BM.GSR.GNFS… Imports … NA    Imports of go… Internati… &lt;df&gt;           2 World…\n2 MS.MIL.XPND… Military… NA    Military expe… Stockholm… &lt;df&gt;           2 World…\n3 NE.IMP.GNFS… Imports … NA    Imports of go… World Ban… &lt;df&gt;           2 World…\n4 NY.GDP.PCAP… GDP per … NA    GDP per capit… World Ban… &lt;df&gt;           2 World…\n5 NY.GDP.PCAP… GDP per … NA    Annual percen… World Ban… &lt;df&gt;           2 World…\n6 SP.POP.TOTL  Populati… NA    Total populat… (1) Unite… &lt;df&gt;           2 World…\n\n\nCode\ndf &lt;- wb_data(indicator = vars, country = country_iso2c, start_date = 2002, end_date = 2015)\n\n# Check all variables and countries are in the data frame.\nall(vars %in% names(df)) & all(country_iso2c %in% unique(df$iso2c))\n\n\n[1] TRUE\n\n\nCode\n# Process Data\nsanctions &lt;- df |&gt;\n  dplyr::mutate(\n    milspend_pc =MS.MIL.XPND.CD/SP.POP.TOTL,\n    realgdpgrowth_pc = NY.GDP.PCAP.KD/dplyr::lag(NY.GDP.PCAP.KD,1)-1,\n    country_id = match(iso2c, country_iso2c),\n    imports = ifelse(is.na(NE.IMP.GNFS.KD), BM.GSR.GNFS.CD,NE.IMP.GNFS.KD), ## TODO convert to constant prices.\n    ) |&gt;\n  dplyr::select(\n    country_id, \n    country, \n    year = date, \n    milspend_pc, \n    pop = SP.POP.TOTL, \n    imports,\n    realgdp_pc = NY.GDP.PCAP.KD,\n    realgdpgrowth_pc\n    ) |&gt;\n  dplyr::filter(year&gt;=2003) \n\n\nsanctions.csv\n\n\nMethod\n\n\nCode\nsanctions_out &lt;- sanctions %&gt;%\n  synthetic_control(outcome = milspend_pc, \n                    unit = country, \n                    time = year, \n                    i_unit = \"Iran, Islamic Rep.\", \n                    i_time = 2011, \n                    generate_placebos=T \n                    ) %&gt;%\n  generate_predictor(time_window = 2003:2011,\n                     pop = mean(pop),\n                     imports = mean(imports),\n                     realgdp_pc = mean(realgdp_pc),\n                     realgdpgrowth_pc = mean(realgdpgrowth_pc)) %&gt;%\n  generate_predictor(time_window = 2010,\n                     milspend_2010 = milspend_pc) %&gt;%\n  generate_predictor(time_window = 2008,\n                     milspend_2008 = milspend_pc) %&gt;%\n  generate_predictor(time_window = 2006,\n                     milspend_2006 = milspend_pc) %&gt;%\n  generate_predictor(time_window = 2004,\n                     milspend_2004 = milspend_pc) %&gt;%\n  generate_weights(optimization_window = 2003:2011, \n                   margin_ipop = .02,sigf_ipop = 7,bound_ipop = 6 \n  ) %&gt;%\n  generate_control()\n\n\nOnce the synthetic control is generated, one can easily assess the fit by comparing the trends of the synthetic and observed time series. The idea is that the trends in the pre-intervention period should map closely onto one another.\n\n\nCode\nsanctions_out %&gt;% plot_trends()\n\n\n\n\n\nTo capture the causal quantity (i.e. the difference between the observed and counterfactual), one can plot the differences using plot_differences()\n\n\nCode\nsanctions_out %&gt;% plot_differences()\n\n\n\n\n\nIn addition, one can easily examine the weighting of the units and variables in the fit. This allows one to see which cases were used, in part, to generate the synthetic control.\n\n\nCode\nsanctions_out %&gt;% plot_weights()\n\n\n\n\n\nAnother useful way of evaluating the synthetic control is to look at how comparable the synthetic control is to the observed covariates of the treated unit.\n\n\nCode\nsanctions_out %&gt;% grab_balance_table()\n\n\n# A tibble: 8 × 4\n  variable         `Iran, Islamic Rep.` synthetic_Iran, Islamic R…¹ donor_sample\n  &lt;chr&gt;                           &lt;dbl&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n1 imports                      1.66e+11                    5.34e+10     4.94e+10\n2 pop                          7.22e+ 7                    5.50e+ 7     3.25e+ 7\n3 realgdp_pc                   5.03e+ 3                    4.86e+ 3     1.16e+ 4\n4 realgdpgrowth_pc             2.89e- 2                    3.20e- 2     2.48e- 2\n5 milspend_2010                1.80e+ 2                    1.74e+ 2     6.17e+ 2\n6 milspend_2008                1.51e+ 2                    1.49e+ 2     5.94e+ 2\n7 milspend_2006                1.23e+ 2                    1.06e+ 2     4.77e+ 2\n8 milspend_2004                7.59e+ 1                    8.14e+ 1     4.16e+ 2\n# ℹ abbreviated name: ¹​`synthetic_Iran, Islamic Rep.`"
  },
  {
    "objectID": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html#inference",
    "href": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html#inference",
    "title": "Iran Sanctions - Sythetic Control",
    "section": "Inference",
    "text": "Inference\nFor inference, the method relies on repeating the method for every donor in the donor pool exactly as was done for the treated unit — i.e. generating placebo synthetic controls). By setting generate_placebos = TRUE when initializing the synth pipeline with synthetic_control(), placebo cases are automatically generated when constructing the synthetic control of interest. This makes it easy to explore how unique difference between the observed and synthetic unit is when compared to the placebos.\n\n\nCode\nsanctions_out %&gt;% plot_placebos()\n\n\n\n\n\nNote that the plot_placebos() function automatically prunes any placebos that poorly fit the data in the pre-intervention period. The reason for doing so is purely visual: those units tend to throw off the scale when plotting the placebos. To prune, the function looks at the pre-intervention period mean squared prediction error (MSPE) (i.e. a metric that reflects how well the synthetic control maps to the observed outcome time series in pre-intervention period). If a placebo control has a MSPE that is two times beyond the target case (e.g. “California”), then it’s dropped. To turn off this behavior, set prune = FALSE.\n\n\nCode\nsanctions_out %&gt;% plot_placebos(prune = FALSE)\n\n\n\n\n\nFinally, Adabie et al. 2010 outline a way of constructing Fisher’s Exact P-values by dividing the post-intervention MSPE by the pre-intervention MSPE and then ranking all the cases by this ratio in descending order. A p-value is then constructed by taking the rank/total.1 The idea is that if the synthetic control fits the observed time series well (low MSPE in the pre-period) and diverges in the post-period (high MSPE in the post-period) then there is a meaningful effect due to the intervention. If the intervention had no effect, then the post-period and pre-period should continue to map onto one another fairly well, yielding a ratio close to 1. If the placebo units fit the data similarly, then we can’t reject the hull hypothesis that there is no effect brought about by the intervention.\nThis ratio can be easily plotted using plot_mspe_ratio(), offering insight into the rarity of the case where the intervention actually occurred.\n\n\nCode\nsanctions_out %&gt;% plot_mspe_ratio()\n\n\n\n\n\nFor more specific information, there is a significance table that can be extracted with one of the many grab_ prefix functions.\n\n\nCode\nsanctions_out %&gt;% grab_significance()\n\n\n# A tibble: 11 × 8\n   unit_name      type  pre_mspe post_mspe mspe_ratio  rank fishers_exact_pvalue\n   &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;                &lt;dbl&gt;\n 1 Iran, Islamic… Trea…    115.      8667.      75.6      1               0.0909\n 2 Jordan         Donor    284.     13107.      46.1      2               0.182 \n 3 Saudi Arabia   Donor  16332.    706684.      43.3      3               0.273 \n 4 Oman           Donor  13643.    540223.      39.6      4               0.364 \n 5 Lebanon        Donor    218.      5839.      26.8      5               0.455 \n 6 Ecuador        Donor    118.      2857.      24.3      6               0.545 \n 7 Morocco        Donor     21.7      210.       9.69     7               0.636 \n 8 Bahrain        Donor  10509.     77008.       7.33     8               0.727 \n 9 Egypt, Arab R… Donor     57.6      365.       6.34     9               0.818 \n10 Israel         Donor 107344.    156612.       1.46    10               0.909 \n11 Nigeria        Donor   1072.      1503.       1.40    11               1     \n# ℹ 1 more variable: z_score &lt;dbl&gt;"
  },
  {
    "objectID": "posts/20231119-Retrieval Augmented Generation/20231119-Retrieval Augmented Generation.html",
    "href": "posts/20231119-Retrieval Augmented Generation/20231119-Retrieval Augmented Generation.html",
    "title": "Retrieval Augmented Generation",
    "section": "",
    "text": "Introduction\nThis examples shows how to load data from a pdf, create emeddings, and then query the data.\n\n\nLoad PDF\nWe load the major projects report 2015 into python using (smart_pdf_loader)[https://llamahub.ai/l/smart_pdf_loader].\n\nSmartPDFLoader is a super fast PDF reader that understands the layout structure of PDFs such as nested sections, nested lists, paragraphs and tables. It uses layout information to smartly chunk PDFs into optimal short contexts for LLMs.\n\n\n\nCode\nfrom llama_hub.smart_pdf_loader import SmartPDFLoader\n\nllmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\npdf_url = \"https://www.nao.org.uk/wp-content/uploads/2015/10/Major-Projects-Report-2015-and-the-Equipment-Plan-2015-2025.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n\npdf_loader = SmartPDFLoader(llmsherpa_api_url=llmsherpa_api_url)\ndocuments = pdf_loader.load_data(pdf_url)\ndocuments[0:2] # look at first 3 chunks \n\n\n[Document(id_='7adbabd4-1df8-4ea6-a5cb-ff56f550e0c6', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7b2a550370ad85d6de2541a641003a5b2971d0115121e67f6ff2a3183be0b7c8', text='Report\\nby the Comptroller and Auditor General', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='3762c995-6cbd-4f15-9e16-491bcc9f1dad', embedding=None, metadata={'chunk_type': 'list_item'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='bb8a396323dd4d9c61c79ed4882caa34151071ed1e48e554c8da9f00a2491810', text='Major Projects Report 2015 and the Equipment Plan 2015 to 2025\\nHC 488-I', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n\n\n\n\nCreate a Vector Store Index\nThe documentation on VectorStoreIndex feels a little light. My understanding (not least through reference to OpenAI billing) is that this step uses OpenAI ada model for embeddings and then stores the index.\n\n\nCode\nfrom llama_index import VectorStoreIndex\nindex = VectorStoreIndex.from_documents(documents)\nindex\n\n\n&lt;llama_index.indices.vector_store.base.VectorStoreIndex at 0x233c568edc0&gt;\n\n\n\n\nQuery\nWe then use GPT 3.5 is used to query the data.\n\n\nCode\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Summarise this document.\")\nprint(response)\nresponse = query_engine.query(\"what are the main causes of schedule variation?\")\nprint(response)\n\n\nThe document is titled \"Major Projects Report 2015 and the Equipment Plan 2015 to 2025.\" It contains an executive project summary and an overview of cost, time, and performance. The report discusses the Department's ability to fund the Equipment Plan and suggests that the Affordability Statement should provide clearer information about uncertainties in costs and the range of possible cost outcomes. It also mentions the need to quantify risks not included in cost forecasts. The document is printed on Evolution Digital Satin paper, which is sourced from responsibly managed and sustainable forests certified by the FSC.\nThe main causes of schedule variation are the net 52-month deferment of the final stage of the Core Production Capability project and the net variation of 8 months in the remaining projects.\n\n\n\n\nNext Steps\n\nBeing able to search over multiple documents\nBeing able to cite sources."
  },
  {
    "objectID": "posts/20231120(2)-RAG with Multiple PDF/20231120-RAG with Multiple PDFs.html",
    "href": "posts/20231120(2)-RAG with Multiple PDF/20231120-RAG with Multiple PDFs.html",
    "title": "Retrieval Augmented Generation",
    "section": "",
    "text": "Build the Knowledge Base\n\n\nCode\nfrom llama_hub.smart_pdf_loader import SmartPDFLoader\nllmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n\npdf_url = \"https://www.nao.org.uk/wp-content/uploads/2015/10/Major-Projects-Report-2015-and-the-Equipment-Plan-2015-2025.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n\nurl2 = \"https://www.nao.org.uk/wp-content/uploads/2010/10/1011489_I.pdf\"\nurls = [url2, pdf_url]\n\npdf_loader = SmartPDFLoader(llmsherpa_api_url=llmsherpa_api_url)\n\ndocuments = []\nfor url in urls:\n        documents.extend(pdf_loader.load_data(url))\n\ndocuments[0:2] # look at first 3 chunks \n\n\n[Document(id_='973aa561-4e5e-42d5-a10f-602c934b9453', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='6fa95392555c5abf74e02cb5d3c6d5604298d7bbae4b2ab079e79be852b9a322', text='The Major Projects Report 2010 &gt; RepoRt by the CoMptRolleR anD auDitoR GeneRal &gt; hC 489-i SeSSion 2010–2011\\nThe National Audit Office scrutinises public spending on behalf of Parliament.\\nThe Comptroller and Auditor General, Amyas Morse, is an Officer of the House of Commons.\\nHe is the head of the National Audit Office which employs some 900 staff.\\nHe and the National Audit Office are totally independent of Government.\\nHe certifies the accounts of all Government departments and a wide range of other public sector bodies; and he has statutory authority to report to Parliament on the economy, efficiency and effectiveness with which departments and other bodies have used their resources.\\nOur work leads to savings and other efficiency gains worth many millions of pounds: £890 million in 2009-10.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n Document(id_='67179a0e-b422-45cb-91df-ad32e7f19696', embedding=None, metadata={'chunk_type': 'para'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='771d33ae2a3747256565d6efe59ebf6977fa15673a504470ccf289d2882492af', text='The Major Projects Report 2010 &gt; RepoRt by the CoMptRolleR anD auDitoR GeneRal &gt; hC 489-i SeSSion 2010–2011\\nOur vision is to help the nation spend wisely.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n\n\n\n\nCode\nfrom llama_index import VectorStoreIndex\nindex = VectorStoreIndex.from_documents(documents)\nindex\n\n\n&lt;llama_index.indices.vector_store.base.VectorStoreIndex at 0x1e5c2014eb0&gt;\n\n\n\n\nCode\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"List the Major Project Reports available in this context.\")\nprint(response)\n\n\nThe Major Project Reports available in this context are the Major Projects Report 2010 and the Major Projects Report 2015."
  },
  {
    "objectID": "posts/20231120-Retrieval Augmented Generation with Citations/20231120-RAG with Citations.html",
    "href": "posts/20231120-Retrieval Augmented Generation with Citations/20231120-RAG with Citations.html",
    "title": "Retrieval Augmented Generation",
    "section": "",
    "text": "Building the Knowledge Base\n\n\nCode\nfrom datasets import load_dataset\n\ndata = load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:10000]')\ndata[6]\n\n\n{'id': '13',\n 'url': 'https://simple.wikipedia.org/wiki/Alan%20Turing',\n 'title': 'Alan Turing',\n 'text': 'Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\\n\\nEarly life and family \\nAlan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\\n\\nEducation \\nTuring went to St. Michael\\'s, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\\n\"This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\\n\\nThe Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\\n\\nEducated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.\\n\\nA brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.\\n\\nIn 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\\n\\nAlas, Turing accidentally or otherwise lost his life in 1954, having been subjected by a British court to chemical castration, thus avoiding a custodial sentence. He is known to have ended his life at the age of 41 years, by eating an apple laced with cyanide.\\n\\nCareer \\nTuring was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\\n\\nTuring was interested in artificial intelligence. He proposed the Turing test, to say when a machine could be called \"intelligent\". A computer could be said to \"think\" if a human talking with it could not tell it was a machine.\\n\\nDuring World War II, Turing worked with others to break German ciphers (secret messages). He  worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain\\'s codebreaking centre that produced Ultra intelligence.\\nUsing cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.\\n\\nFrom 1945 to 1947, Turing worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory. He presented a paper on 19 February 1946. That paper was \"the first detailed design of a stored-program computer\". Although it was possible to build ACE, there were delays in starting the project. In late 1947 he returned to Cambridge for a sabbatical year. While he was at Cambridge, the Pilot ACE was built without him. It ran its first program on 10\\xa0May 1950.\\n\\nPrivate life \\nTuring was a homosexual man. In 1952, he admitted having had sex with a man in England. At that time, homosexual acts were illegal. Turing was convicted. He had to choose between going to jail and taking hormones to lower his sex drive. He decided to take the hormones. After his punishment, he became impotent. He also grew breasts.\\n\\nIn May 2012, a private member\\'s bill was put before the House of Lords to grant Turing a statutory pardon. In July 2013, the government supported it. A royal pardon was granted on 24 December 2013.\\n\\nDeath \\nIn 1954, Turing died from cyanide poisoning. The cyanide came from either an apple which was poisoned with cyanide, or from water that had cyanide in it. The reason for the confusion is that the police never tested the apple for cyanide. It is also suspected that he committed suicide.\\n\\nThe treatment forced on him is now believed to be very wrong. It is against medical ethics and international laws of human rights. In August 2009, a petition asking the British Government to apologise to Turing for punishing him for being a homosexual was started. The petition received thousands of signatures. Prime Minister Gordon Brown acknowledged the petition. He called Turing\\'s treatment \"appalling\".\\n\\nReferences\\n\\nOther websites \\nJack Copeland 2012. Alan Turing: The codebreaker who saved \\'millions of lives\\'. BBC News / Technology \\n\\nEnglish computer scientists\\nEnglish LGBT people\\nEnglish mathematicians\\nGay men\\nLGBT scientists\\nScientists from London\\nSuicides by poison\\nSuicides in the United Kingdom\\n1912 births\\n1954 deaths\\nOfficers of the Order of the British Empire'}\n\n\n\n\nCode\nimport tiktoken\n\ntiktoken.encoding_for_model('gpt-3.5-turbo')\n\n\n&lt;Encoding 'cl100k_base'&gt;\n\n\n\n\nCode\nimport tiktoken\n\ntokenizer = tiktoken.get_encoding('cl100k_base')\n\n# create the length function\ndef tiktoken_len(text):\n    tokens = tokenizer.encode(\n        text,\n        disallowed_special=()\n    )\n    return len(tokens)\n\ntiktoken_len(\"hello I am a chunk of text and using the tiktoken_len function \"\n             \"we can find the length of this chunk of text in tokens\")\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=400,\n    chunk_overlap=20,\n    length_function=tiktoken_len,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\nchunks = text_splitter.split_text(data[6]['text'])[:3]\nchunks\ntiktoken_len(chunks[0]), tiktoken_len(chunks[1]), tiktoken_len(chunks[2])\n\n\n(299, 323, 382)\n\n\n\n\nCreating Embeddings\n\n\nCode\nimport os\n\n# get openai api key from platform.openai.com\nOPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nmodel_name = 'text-embedding-ada-002'\n\nembed = OpenAIEmbeddings(\n    model=model_name,\n    openai_api_key=OPENAI_API_KEY\n)\n\n\n\n\nCode\ntexts = [\n    'this is the first chunk of text',\n    'then another second chunk of text is here'\n]\n\nres = embed.embed_documents(texts)\nlen(res), len(res[0])\n\n\n(2, 1536)\n\n\n\n\nVector Database\n\n\nCode\nindex_name = 'langchain-retrieval-augmentation'\nimport pinecone\n\n# find API key in console at app.pinecone.io\nPINECONE_API_KEY = os.getenv('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n# find ENV (cloud region) next to API key in console\nPINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT') or 'PINECONE_ENVIRONMENT'\n\npinecone.init(\n    api_key=PINECONE_API_KEY,\n    environment=PINECONE_ENVIRONMENT\n)\n\nif index_name not in pinecone.list_indexes():\n    # we create a new index\n    pinecone.create_index(\n        name=index_name,\n        metric='cosine',\n        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n    )\n\nindex = pinecone.GRPCIndex(index_name)\n\nindex.describe_index_stats()\n\n\n{'dimension': 1536,\n 'index_fullness': 0.28422,\n 'namespaces': {'': {'vector_count': 28422}},\n 'total_vector_count': 28422}\n\n\n\n\nIndexing\n\n\nCode\nfrom tqdm.auto import tqdm\nfrom uuid import uuid4\n\nbatch_limit = 100\n\ntexts = []\nmetadatas = []\n\nfor i, record in enumerate(tqdm(data)):\n    # first get metadata fields for this record\n    metadata = {\n        'wiki-id': str(record['id']),\n        'source': record['url'],\n        'title': record['title']\n    }\n    # now we create chunks from the record text\n    record_texts = text_splitter.split_text(record['text'])\n    # create individual metadata dicts for each chunk\n    record_metadatas = [{\n        \"chunk\": j, \"text\": text, **metadata\n    } for j, text in enumerate(record_texts)]\n    # append these to current batches\n    texts.extend(record_texts)\n    metadatas.extend(record_metadatas)\n    # if we have reached the batch_limit we can add texts\n    if len(texts) &gt;= batch_limit:\n        ids = [str(uuid4()) for _ in range(len(texts))]\n        embeds = embed.embed_documents(texts)\n        index.upsert(vectors=zip(ids, embeds, metadatas))\n        texts = []\n        metadatas = []\n\nif len(texts) &gt; 0:\n    ids = [str(uuid4()) for _ in range(len(texts))]\n    embeds = embed.embed_documents(texts)\n    index.upsert(vectors=zip(ids, embeds, metadatas))\n\nindex.describe_index_stats()\n\n\n\n\n\n{'dimension': 1536,\n 'index_fullness': 0.56705,\n 'namespaces': {'': {'vector_count': 56705}},\n 'total_vector_count': 56705}\n\n\n\n\nCreating a Vector Store and Querying\n\n\nCode\nfrom langchain.vectorstores import Pinecone\n\ntext_field = \"text\"\n\n# switch back to normal index for langchain\nindex = pinecone.Index(index_name)\n\nvectorstore = Pinecone(\n    index, embed.embed_query, text_field\n)\n\n\nC:\\Users\\lukeh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain\\vectorstores\\pinecone.py:59: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n  warnings.warn(\n\n\n\n\nCode\nquery = \"who was Benito Mussolini?\"\n\nvectorstore.similarity_search(\n    query,  # our search query\n    k=3  # return 3 most relevant docs\n)\n\n\n[Document(page_content='Benito Amilcare Andrea Mussolini KSMOM GCTE (29 July 1883 – 28 April 1945) was an Italian politician and journalist. He was also the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party.\\n\\nBiography\\n\\nEarly life\\nBenito Mussolini was named after Benito Juarez, a Mexican opponent of the political power of the Roman Catholic Church, by his anticlerical (a person who opposes the political interference of the Roman Catholic Church in secular affairs) father. Mussolini\\'s father was a blacksmith. Before being involved in politics, Mussolini was a newspaper editor (where he learned all his propaganda skills) and elementary school teacher.\\n\\nAt first, Mussolini was a socialist, but when he wanted Italy to join the First World War, he was thrown out of the socialist party. He \\'invented\\' a new ideology, Fascism, much out of Nationalist\\xa0and Conservative views.\\n\\nRise to power and becoming dictator\\nIn 1922, he took power by having a large group of men, \"Black Shirts,\" march on Rome and threaten to take over the government. King Vittorio Emanuele III gave in, allowed him to form a government, and made him prime minister. In the following five years, he gained power, and in 1927 created the OVRA, his personal secret police force. Using the agency to arrest, scare, or murder people against his regime, Mussolini was dictator\\xa0of Italy by the end of 1927. Only the King and his own Fascist party could challenge his power.', metadata={'chunk': 0.0, 'source': 'https://simple.wikipedia.org/wiki/Benito%20Mussolini', 'title': 'Benito Mussolini', 'wiki-id': '6754'}),\n Document(page_content='Benito Amilcare Andrea Mussolini KSMOM GCTE (29 July 1883 – 28 April 1945) was an Italian politician and journalist. He was also the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party.\\n\\nBiography\\n\\nEarly life\\nBenito Mussolini was named after Benito Juarez, a Mexican opponent of the political power of the Roman Catholic Church, by his anticlerical (a person who opposes the political interference of the Roman Catholic Church in secular affairs) father. Mussolini\\'s father was a blacksmith. Before being involved in politics, Mussolini was a newspaper editor (where he learned all his propaganda skills) and elementary school teacher.\\n\\nAt first, Mussolini was a socialist, but when he wanted Italy to join the First World War, he was thrown out of the socialist party. He \\'invented\\' a new ideology, Fascism, much out of Nationalist\\xa0and Conservative views.\\n\\nRise to power and becoming dictator\\nIn 1922, he took power by having a large group of men, \"Black Shirts,\" march on Rome and threaten to take over the government. King Vittorio Emanuele III gave in, allowed him to form a government, and made him prime minister. In the following five years, he gained power, and in 1927 created the OVRA, his personal secret police force. Using the agency to arrest, scare, or murder people against his regime, Mussolini was dictator\\xa0of Italy by the end of 1927. Only the King and his own Fascist party could challenge his power.', metadata={'chunk': 0.0, 'source': 'https://simple.wikipedia.org/wiki/Benito%20Mussolini', 'title': 'Benito Mussolini', 'wiki-id': '6754'}),\n Document(page_content='Fascism as practiced by Mussolini\\nMussolini\\'s form of Fascism, \"Italian Fascism\"- unlike Nazism, the racist ideology that Adolf Hitler followed- was different and less destructive than Hitler\\'s. Although a believer in the superiority of the Italian nation and national unity, Mussolini, unlike Hitler, is quoted \"Race? It is a feeling, not a reality. Nothing will ever make me believe that biologically pure races can be shown to exist today\".\\n\\nMussolini wanted Italy to become a new Roman Empire. In 1923, he attacked the island of Corfu, and in 1924, he occupied the city state of Fiume. In 1935, he attacked the African country Abyssinia (now called Ethiopia). His forces occupied it in 1936. Italy was thrown out of the League of Nations because of this aggression. In 1939, he occupied the country Albania. In 1936, Mussolini signed an alliance with Adolf Hitler, the dictator of Germany.\\n\\nFall from power and death\\nIn 1940, he sent Italy into the Second World War on the side of the Axis countries. Mussolini attacked Greece, but he failed to conquer it. In 1943, the Allies landed in Southern Italy. The Fascist party and King Vittorio Emanuel III deposed Mussolini and put him in jail, but he was set free by the Germans, who made him ruler of the Italian Social Republic puppet state which was in a small part of Central Italy. When the war was almost over, Mussolini tried to escape to Switzerland with his mistress, Clara Petacci, but they were both captured and shot by partisans. Mussolini\\'s dead body was hanged upside-down, together with his mistress and some of Mussolini\\'s helpers, on a pole at a gas station in the village of Millan, which is near the border  between Italy and Switzerland.', metadata={'chunk': 1.0, 'source': 'https://simple.wikipedia.org/wiki/Benito%20Mussolini', 'title': 'Benito Mussolini', 'wiki-id': '6754'})]\n\n\n\n\nGenerative Question Answering\n\n\nCode\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\n\n# completion llm\nllm = ChatOpenAI(\n    openai_api_key=OPENAI_API_KEY,\n    model_name='gpt-3.5-turbo',\n    temperature=0.0\n)\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever()\n)\n\nqa.run(query)\n\n\n'Benito Mussolini was an Italian politician and journalist who served as the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party and played a significant role in the rise of fascism in Italy. Mussolini established a dictatorship and implemented policies that aimed to create a new Roman Empire. He allied with Adolf Hitler and led Italy into World War II as part of the Axis powers. Mussolini was eventually deposed and captured by the Allies, and he was executed by Italian partisans in 1945.'\n\n\n\n\nCode\nfrom langchain.chains import RetrievalQAWithSourcesChain\n\nqa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever()\n)\nqa_with_sources(query)\n\n\n{'question': 'who was Benito Mussolini?',\n 'answer': 'Benito Mussolini was an Italian politician and journalist who served as the Prime Minister of Italy from 1922 until 1943. He was the leader of the National Fascist Party and played a significant role in the rise of fascism in Italy. Mussolini took power through a march on Rome and established a dictatorship, using his personal secret police force to suppress opposition. He pursued expansionist policies, including the occupation of Abyssinia (now Ethiopia) and Albania. Mussolini aligned with Adolf Hitler and the Axis powers during World War II. However, he was eventually deposed and captured by the Allies, and he was executed by partisans in 1945. \\n',\n 'sources': 'https://simple.wikipedia.org/wiki/Benito%20Mussolini'}"
  }
]