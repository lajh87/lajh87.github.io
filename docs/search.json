[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "NLP\n\n\n\n\nSummarise the Top 5 Setences by Section in Homo Deus\n\n\n\n\n\n\nMay 30, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabour Markets\n\n\nLabour Force Survey\n\n\n\n\nCalculate voluntary job separation rates from the two quarter labour force survey\n\n\n\n\n\n\nMay 19, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNLP\n\n\n\n\nExtract people from Guardian reporting on what we know on Ukraine.\n\n\n\n\n\n\nMay 18, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\napi\n\n\nNLP\n\n\n\n\nThis post uses API to extract Guardian What we Know data and LexR to extract top 3 points.\n\n\n\n\n\n\nMay 18, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwikipedia\n\n\n\n\nSearch and extract UK Frigate Data\n\n\n\n\n\n\nApr 1, 2023\n\n\nDefence Economist\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwikipedia\n\n\nscrape\n\n\n\n\nExtract Type 23 Performance Data\n\n\n\n\n\n\nApr 1, 2023\n\n\nDefence Economist\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwikipedia\n\n\nscrape\n\n\n\n\nThis post shows how to access the wikipedia knowledge graph\n\n\n\n\n\n\nMar 26, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwikipedia\n\n\nscrape\n\n\n\n\nThis post shows how to access the wikipedia knowledge graph\n\n\n\n\n\n\nMar 26, 2023\n\n\nLuke Heley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20230326-using-wikidata/using-wikidata.html",
    "href": "posts/20230326-using-wikidata/using-wikidata.html",
    "title": "Using Wiki Data to Extract Data",
    "section": "",
    "text": "Data\nWe use the wikidata API to extract the knowledge graph from wiki.\nThe approach is to search wikidata for the item of interest and select the best match from the list of search results that return.\n\n# function to search wiki data and return a tibble of search results\nsearch_wikidata <- function(search){\n  base <- \"https://www.wikidata.org\" \n  path <- \"/w/api.php\"\n  query <- list(\n    action=\"query\",\n    list=\"search\",\n    format = \"json\",\n    srsearch = search\n  )\n  \n  httr::GET(base, path = path, query = query) |>\n    httr::content() |>\n    purrr::pluck(\"query\", \"search\") |>\n    purrr::map_df(~{\n      ul <- unlist(.x)\n      name <- names(ul)\n      value <- as.character(ul)\n      dplyr::tibble(name, value) |>\n        tidyr::pivot_wider()\n      })\n}\n\n(search_results <- search_wikidata(\"Type-094 submarine\"))\n\n# A tibble: 2 × 6\n  ns    title    pageid  wordcount snippet                               times…¹\n  <chr> <chr>    <chr>   <chr>     <chr>                                 <chr>  \n1 0     Q1203377 1146175 0         nuclear-powered ballistic missile su… 2023-0…\n2 0     Q7008427 6886594 0         Wikimedia category                    2022-1…\n# … with abbreviated variable name ¹​timestamp\n\n# Extract the item title for the item of interest\nroot_item_title <- search_results$title[1]\n\nWe then extract wikidata associated with the item.\n\n# extract the entity data for a chosen item.\nget_entity_data <- function(item = \"Q1203377\"){\n  base <- \"https://www.wikidata.org/\"\n  path <- glue::glue(\"wiki/Special:EntityData/{item}.json\")\n  query <- list(flavor = \"simple\")  \n  req <- httr::GET(base, path = path, query = query)\n  \n  httr::content(req) |>\n    purrr::pluck(\"entities\", item, \"claims\") |>\n    purrr::map_df(~{\n      value <- unlist(.x)\n      name <- names(value)\n      dplyr::tibble(name, value) |>\n        tidyr::pivot_wider() |>\n        tidyr::unnest()\n      })\n}\n\nentity_data <- get_entity_data(root_item_title)\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n    dplyr::group_by(name) %>%\n    dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n    dplyr::filter(n > 1L)\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(mainsnak.snaktype, mainsnak.property, mainsnak.hash, `mainsnak.datavalue.value.entity-type`, \n    `mainsnak.datavalue.value.numeric-id`, mainsnak.datavalue.value.id, \n    mainsnak.datavalue.type, mainsnak.datatype, type, id, rank)`\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n    dplyr::group_by(name) %>%\n    dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n    dplyr::filter(n > 1L)\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(mainsnak.snaktype, mainsnak.property, mainsnak.hash, `mainsnak.datavalue.value.entity-type`, \n    `mainsnak.datavalue.value.numeric-id`, mainsnak.datavalue.value.id, \n    mainsnak.datavalue.type, mainsnak.datatype, type, id, rank, \n    references.hash, references.snaks.P143.snaktype, references.snaks.P143.property, \n    references.snaks.P143.hash, `references.snaks.P143.datavalue.value.entity-type`, \n    `references.snaks.P143.datavalue.value.numeric-id`, references.snaks.P143.datavalue.value.id, \n    references.snaks.P143.datavalue.type, references.snaks.P143.datatype, \n    `references.snaks-order`)`\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n\n\nAnd the associated properties\n\nproperties <- entity_data |> \n  dplyr::pull(\"mainsnak.property\") |>\n  unique()\n\nget_entity_id <- function(id = \"P373\"){\n  if(length(id)>1) id <- paste(id, collapse = \"|\")\n  base <- \"https://www.wikidata.org/\"\n  path <- \"w/api.php\"\n  query <- list(\n    action=\"wbgetentities\",\n    ids=id,\n    languages=\"en\",\n    props=\"labels\",\n    format=\"json\"\n    )\n  \n  req <- httr::GET(base, path = path, query = query) \n  \n  if(req$status != 200) \n    return(stop(glue::glue(\"Error returned status: {req$status}\")))\n  \n  httr::content(req) |>\n    purrr::pluck(\"entities\") |>\n    purrr::map_df(~{\n      value <- unlist(.x)\n      name <- names(value)\n      dplyr::tibble(name, value) |> tidyr::pivot_wider()\n    })\n}\n\n(prop_label <- get_entity_id(properties))\n\n# A tibble: 16 × 5\n   type     datatype        id    labels.en.language labels.en.value      \n   <chr>    <chr>           <chr> <chr>              <chr>                \n 1 property string          P373  en                 Commons category     \n 2 property wikibase-item   P516  en                 powered by           \n 3 property string          P561  en                 NATO reporting name  \n 4 property wikibase-item   P31   en                 instance of          \n 5 property wikibase-item   P279  en                 subclass of          \n 6 property wikibase-item   P910  en                 topic's main category\n 7 property external-id     P646  en                 Freebase ID          \n 8 property time            P729  en                 service entry        \n 9 property wikibase-item   P156  en                 followed by          \n10 property wikibase-item   P155  en                 follows              \n11 property commonsMedia    P18   en                 image                \n12 property wikibase-item   P176  en                 manufacturer         \n13 property wikibase-item   P137  en                 operator             \n14 property monolingualtext P1813 en                 short name           \n15 property wikibase-item   P520  en                 armament             \n16 property wikibase-item   P495  en                 country of origin    \n\n\nGet the item data\n\nentity_data2 <- entity_data |>\n  dplyr::select(\n    mainsnak.property,\n    mainsnak.datavalue.type,\n    mainsnak.datavalue.value.id,\n    mainsnak.datavalue.value,\n    mainsnak.datavalue.value.text,\n    mainsnak.datavalue.value.time\n  ) |>\n  dplyr::mutate(\n    value = dplyr::case_when(\n      !is.na(mainsnak.datavalue.value.id) ~ mainsnak.datavalue.value.id,\n      !is.na(mainsnak.datavalue.value) ~ mainsnak.datavalue.value,\n      !is.na(mainsnak.datavalue.value.time) ~ mainsnak.datavalue.value.time,\n      !is.na(mainsnak.datavalue.value.text) ~ mainsnak.datavalue.value.text\n    )\n  ) |>\n  dplyr::select(property = 1, type = 2, value) |>\n  dplyr::distinct()\n\nentity_data3 <- entity_data2 |>\n  dplyr::left_join(prop_label |>\n  dplyr::select(\n    property = id, \n    property_label = labels.en.value\n  ))\n\nJoining, by = \"property\"\n\nitems <- entity_data3 |>\n  dplyr::filter(type == \"wikibase-entityid\") |>\n  dplyr::pull(value) |> \n  unique()\n\nitem_labels <- get_entity_id(items)\n\n(item_property <- entity_data3 |>\n  dplyr::left_join(\n    item_labels |>\n      dplyr::select(value = id, item_label = labels.en.value)\n  ) |>\n  dplyr::mutate(item_label = dplyr::case_when(is.na(item_label)~value, \n                                              TRUE ~ item_label)) |>\n  dplyr::select(property_label, item_label))\n\nJoining, by = \"value\"\n\n\n# A tibble: 17 × 2\n   property_label        item_label                                          \n   <chr>                 <chr>                                               \n 1 Commons category      Type 09IV submarines                                \n 2 powered by            nuclear marine propulsion                           \n 3 NATO reporting name   Jin                                                 \n 4 instance of           submarine class                                     \n 5 subclass of           ballistic missile submarine                         \n 6 subclass of           nuclear submarine                                   \n 7 topic's main category Category:Type 094 submarines                        \n 8 Freebase ID           /m/09wz47                                           \n 9 service entry         +2010-01-01T00:00:00Z                               \n10 followed by           Type 096 submarine                                  \n11 follows               Type 092 Daqingyu                                   \n12 image                 Jin (Type 094) Class Ballistic Missile Submarine.JPG\n13 manufacturer          Bohai Shipyard                                      \n14 operator              People's Liberation Army Navy                       \n15 short name            Type 094                                            \n16 armament              JL-2                                                \n17 country of origin     People's Republic of China                          \n\n\nGet wiki urls\n\nget_wikisites <- function(item = \"Q1203377\"){\n  base <- \"https://www.wikidata.org/\"\n  path <- glue::glue(\"wiki/Special:EntityData/{item}.json\")\n  query <- list(flavor = \"simple\")  \n  req <- httr::GET(base, path = path, query = query)\n  \n  cont <- httr::content(req) \n  cont |>\n    purrr::pluck(\"entities\", item, \"sitelinks\") |>\n    purrr::map_df(~{\n      value <- unlist(.x)\n      name <- names(value)\n      dplyr::tibble(name, value) |>\n        tidyr::pivot_wider()\n      })\n}\n\n(wikiurl <- get_wikisites(root_item_title) |>\n  dplyr::filter(site == \"enwiki\") |>\n  dplyr::pull(url))\n\n                                                   \n\"https://en.wikipedia.org/wiki/Type_094_submarine\" \n\n\nScrape the infobox from the wiki url\n\nscrape_infobox <- function(\n    url =\"https://en.wikipedia.org/wiki/Type_094_submarine\"\n){\n  req <- httr::GET(url)\n  req |> \n    httr::content() |> \n    xml2::xml_find_all(\"//table[@class='infobox']\") |>\n    rvest::html_table() \n}\n\nscrape_infobox(\"https://en.wikipedia.org/wiki/Type_094_submarine\")\n\n[[1]]\n# A tibble: 22 × 2\n   X1                      X2                               \n   <chr>                   <chr>                            \n 1 Profile of the Type 094 Profile of the Type 094          \n 2 Type 094 submarine      Type 094 submarine               \n 3 Class overview          Class overview                   \n 4 Name                    Type 094 (Jin class)             \n 5 Builders                Bohai Shipyard, Huludao, China[2]\n 6 Operators               People's Liberation Army Navy    \n 7 Preceded by             Type 092 (Xia class)             \n 8 Succeeded by            Type 096                         \n 9 Cost                    $750 million per unit[1]         \n10 In commission           2007–present[2]                  \n# … with 12 more rows\n\n\nScrape wikitables\n\nscrape_wikitables <- function(\n    url =\"https://en.wikipedia.org/wiki/Type_094_submarine\"\n){\n  req <- httr::GET(url)\n  req |> \n    httr::content() |> \n    xml2::xml_find_all(\"//table[@class='wikitable']\") |>\n    rvest::html_table()\n}\n\nscrape_wikitables(\"https://en.wikipedia.org/wiki/Type_094_submarine\")\n\n[[1]]\n# A tibble: 8 × 7\n  Name                `Hull no.`  Builder         Laid …¹ Launc…² Commi…³ Status\n  <chr>               <chr>       <chr>           <chr>   <chr>   <chr>   <chr> \n1 \"Type 094\"          \"Type 094\"  \"Type 094\"      \"Type … \"Type … Type 0… Type …\n2 \"\"                  \"411[2]\"    \"Bohai Shipyar… \"2001[… \"28 Ju… March … Active\n3 \"Changzheng 10[20]\" \"412[2]\"    \"Bohai Shipyar… \"2003[… \"2006[… 2010[2] Active\n4 \"Changzheng 11[20]\" \"413[2]\"    \"Bohai Shipyar… \"2004[… \"Decem… 2012[2] Active\n5 \"Changzheng 18[21]\" \"421[22]\"   \"\"              \"\"      \"\"      23 Apr… Active\n6 \"Type 094A\"         \"Type 094A\" \"Type 094A\"     \"Type … \"Type … Type 0… Type …\n7 \"\"                  \"\"          \"\"              \"\"      \"\"      2020[5] Active\n8 \"\"                  \"\"          \"\"              \"\"      \"\"      2020[5] Active\n# … with abbreviated variable names ¹​`Laid down`, ²​Launched, ³​Commissioned"
  },
  {
    "objectID": "posts/20230401-extract-uk-frigate-data/20230401-extract-uk-frigate-data.html",
    "href": "posts/20230401-extract-uk-frigate-data/20230401-extract-uk-frigate-data.html",
    "title": "Extract UK Frigate Data",
    "section": "",
    "text": "Data\ntype 26, type 23, type 22, type 21, leander class, rothesay-class, valour-class, Whitby class and Sailsbury class\n\nfrigates <- c(\n  \"Type 26\", \"Type 23\", \"Type 22\", \n  \"Type 21\", \"Leander Class\", \"Rothesay Class\", \n   \"Whitby Class\", \"Salisbury Class\"\n  ) |>\n  paste(\"Frigate\")\n\nbase <- \"https://www.wikidata.org\" \npath <- \"/w/api.php\"\nquery <- list(\n  action=\"query\",\n  list=\"search\",\n  format = \"json\"\n)\n\nsearch_results <- frigates |>\n  purrr::map(~{\n    query$srsearch = .x\n    httr::GET(base, path = path, query = query) |>\n      httr::content()\n  })\n\n\nsearch_results_tidy <- search_results |>\n  purrr::map_df(~{\n     .x$query$search[1]; # extract first response\n    } ) |>\n  dplyr::select(title, snippet) |>\n  dplyr::mutate(search = frigates) |>\n  dplyr::relocate(search)\n\nget wiki url\n\nbase <- \"https://www.wikidata.org/\"\nreq <- search_results_tidy$title |>\n  purrr::map(~{\n    path <- glue::glue(\"wiki/Special:EntityData/{.x}.json\")\n    httr::GET(url = base, path = path, query = list(flavor = \"simple\"))\n  })\n\n(wiki_url <- req |>\n  purrr::map_chr(~{\n    cnt <- httr::content(.x)\n    cnt$entities[[1]]$sitelinks$enwiki$url\n  }))\n\n[1] \"https://en.wikipedia.org/wiki/Type_26_frigate\"        \n[2] \"https://en.wikipedia.org/wiki/Type_23_frigate\"        \n[3] \"https://en.wikipedia.org/wiki/Type_22_frigate\"        \n[4] \"https://en.wikipedia.org/wiki/Type_21_frigate\"        \n[5] \"https://en.wikipedia.org/wiki/Leander-class_frigate\"  \n[6] \"https://en.wikipedia.org/wiki/Rothesay-class_frigate\" \n[7] \"https://en.wikipedia.org/wiki/Whitby-class_frigate\"   \n[8] \"https://en.wikipedia.org/wiki/Salisbury-class_frigate\"\n\n\n\ninfobox <- wiki_url |>\n  purrr::map_df(~{\n    httr::GET(.x) |>\n      httr::content() |>\n      xml2::xml_find_all(\"//table[@class='infobox']\") |>\n      rvest::html_table() |>\n      purrr::pluck(1) |>\n      dplyr::mutate(source = .x)\n  })\n\nvars <- c(\"Displacement\", \"Length\", \"Beam\", \"Draught\")\n\nperformance <- infobox |>\n  dplyr::group_by(source) |>\n  dplyr::mutate(X0 = ifelse(X1 == X2, X1, NA)) |>\n  tidyr::fill(X0) |>\n  dplyr::filter(X1 != X2) |>\n  dplyr::filter(X1 %in% vars) |>\n  dplyr::ungroup() |>\n  dplyr::mutate(key = 1L:dplyr::n()) |>\n  dplyr::group_by(source) |>\n  dplyr::mutate(X2 = stringr::str_split(X2, \"\\n\")) |>\n  tidyr::unnest(cols = \"X2\") |>\n  dplyr::ungroup() |>\n  dplyr::mutate(\n    X3 = ifelse(\n      test = stringr::str_detect(X2, \"^[0-9]\", negate = TRUE), \n      yes = substr(X2, 1, stringr::str_locate(X2, \":\")[,1]-1),\n      no = NA)\n  ) |>\n   dplyr::mutate(\n    X2 = ifelse(\n      test = stringr::str_detect(X2, \"^[0-9]\", negate = TRUE), \n      yes = substr(X2, stringr::str_locate(X2, \":\")[,1]+2, nchar(X2)),\n      no = X2)\n  )  |>\n  dplyr::mutate(\n    value = as.numeric(stringr::str_remove_all(substr(X2, 1, stringr::str_locate(X2, \"\\\\s\")[,1]-1), \",\"))\n  ) |>\n  dplyr::mutate(\n    units = substr(X2, stringr::str_locate(X2, \"\\\\s\")[,1]+1, nchar(X2))\n  ) |>\n  dplyr::mutate(\n    units = substr(units, 1, stringr::str_locate(units, \"\\\\s\")[,1]-1)\n  ) |> \n  dplyr::mutate(\n    label = stringr::str_remove_all(source, \"https://en.wikipedia.org/wiki/\") |>\n      stringr::str_replace_all(\"_\", \" \")\n    ) |>\n  dplyr::filter(!is.na(value)) |>\n  dplyr::group_by(key) |>\n  dplyr::mutate(increment = 1:dplyr::n()) |>\n  dplyr::select(key, increment, label, name = X1, value, units) |>\n  dplyr::mutate(units = dplyr::case_when(\n    units %in% c(\"tonnes\", \"ton\", \"tonnes,\", \"tons\") ~ \"t\",\n    units %in% c(\"long\") ~ \"lt\",\n    units %in% c(\"metres\") ~ \"m\",\n    TRUE ~ units\n  )) |>\n  dplyr::mutate(\n    value = dplyr::case_when(\n      units %in% \"lt\" ~ (value * 1.016047), \n      units %in% \"ft\" ~ (value * 0.3048),\n      TRUE ~ value\n    )\n  ) |>\n  dplyr::mutate(\n    units = dplyr::case_when(\n       units %in% \"lt\" ~ \"t\",\n       units %in% \"ft\" ~ \"m\",\n       TRUE ~ units\n    )\n  ) |>\n  dplyr::ungroup() |>\n  dplyr::select(-units, -key, -increment) |> \n  dplyr::group_by(label, name) |>\n  dplyr::mutate(increment = 1:dplyr::n()) |>\n  tidyr::pivot_wider(names_from = name, values_from = value) |>\n  tidyr::fill(c(Length, Beam, Draught)) \n\nx <- GGally::ggpairs(performance, lower= list(mapping = ggplot2::aes(colour = label)), columns = 3:6) \n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nplotly::ggplotly(x)\n\nWarning: Can only have one: highlight\n\nWarning: Can only have one: highlight\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\nWarning: Removed 3 rows containing non-finite values (stat_density).\n\n\nWarning: Can only have one: highlight"
  },
  {
    "objectID": "posts/20230401-wikidata-extract-preceded-by/20230401-extract-preceded-by.html",
    "href": "posts/20230401-wikidata-extract-preceded-by/20230401-extract-preceded-by.html",
    "title": "Extract of UK Frigates and Their Precendents",
    "section": "",
    "text": "Data\nWe use wikidata to first extract the different types of warship\n\nq <- '\nSELECT ?ship_type ?ship_typeLabel WHERE {\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n  ?ship_type wdt:P31 wd:Q2235308;\n    wdt:P279 wd:Q3114762.\n  \n  OPTIONAL {  }\n}\n  '\n\nWikidataQueryServiceR::query_wikidata(q)\n\nRows: 55 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): ship_type, ship_typeLabel\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 55 × 2\n   ship_type                              ship_typeLabel  \n   <chr>                                  <chr>           \n 1 http://www.wikidata.org/entity/Q17205  aircraft carrier\n 2 http://www.wikidata.org/entity/Q104843 cruiser         \n 3 http://www.wikidata.org/entity/Q161705 frigate         \n 4 http://www.wikidata.org/entity/Q170013 corvette        \n 5 http://www.wikidata.org/entity/Q174736 destroyer       \n 6 http://www.wikidata.org/entity/Q182531 battleship      \n 7 http://www.wikidata.org/entity/Q188924 galley          \n 8 http://www.wikidata.org/entity/Q207452 ship of the line\n 9 http://www.wikidata.org/entity/Q629990 bireme          \n10 http://www.wikidata.org/entity/Q640078 minelayer       \n# … with 45 more rows\n\n\nAnd subsequently different types of frigates operated by the Royal Navy\n\nq <- '\nSELECT ?ship_class ?ship_classLabel ?service_entry ?followed_by ?followed_byLabel ?operator ?operatorLabel ?image ?service_retirement ?follows ?followsLabel ?service_entryLabel ?service_retirementLabel ?imageLabel WHERE {\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n  ?ship_class wdt:P31 wd:Q559026; # instance of ship class\n    wdt:P279 wd:Q106070703. # subclass of frigate\n  ?ship_class wdt:P137 wd:Q172771. # operated by RN\n  OPTIONAL { ?ship_class wdt:P729 ?service_entry. }\n  OPTIONAL { ?ship_class wdt:P730 ?service_retirement. }\n  OPTIONAL { ?ship_class wdt:P155 ?follows. }\n  OPTIONAL { ?ship_class wdt:P156 ?followed_by. }\n  OPTIONAL { ?ship_class wdt:P137 ?operator. }\n  OPTIONAL { ?ship_class wdt:P18 ?image. }\n}\n'\nrn_frigates <- WikidataQueryServiceR::query_wikidata(q)\n\nRows: 83 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): ship_class, ship_classLabel, followed_by, followed_byLabel, opera...\ndttm  (4): service_entry, service_retirement, service_entryLabel, service_re...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nrn_frigates |>\n  dplyr::filter(operatorLabel == \"Royal Navy\") |>\n  dplyr::select(from = ship_classLabel, \n                to = followed_byLabel, \n                value = service_entry) |>\n  dplyr::distinct() |>\n  dplyr::filter(!is.na(to)) |>\n  igraph::graph_from_data_frame() |>\n  visNetwork::visIgraph()\n\n\n\n\n\nNext step. Extract data on type 26, type 23, type 22, type 21, leander class, rothesay-class, valour-class, whiteby-class and saisbury class.\nFuture intention is to compare to other nations where data are available."
  },
  {
    "objectID": "posts/20230401-wikidata-type23/20230401-wikidata-type23.html",
    "href": "posts/20230401-wikidata-type23/20230401-wikidata-type23.html",
    "title": "Extract Type 23 Data",
    "section": "",
    "text": "Data\n\nurl <- \"https://en.wikipedia.org/wiki/Type_23_frigate\"\nreq <- httr::GET(url) |>\n  httr::content()\n\nvars <- c(\"Name\", \"Displacement\", \"Length\", \"Beam\", \"Draught\")\n(df <- req |>\n  xml2::xml_find_all(\"//table[@class = 'infobox']\") |>\n  rvest::html_table() |>\n  purrr::pluck(1) |>\n  dplyr::filter(X1 %in% c(vars)) |>\n  t() |> dplyr::as_tibble(.name_repair = \"minimal\") |>\n  janitor::row_to_names(1) |>\n  dplyr::mutate(dplyr::across(-1, ~stringr::str_sub(.x, 1, stringr::str_locate(.x, \"\\\\(\")[,1]-2))) |>\n  tidyr::pivot_longer(-1) |>\n  dplyr::rename(id = Name) |>\n  dplyr::mutate(unit = substr(value, stringr::str_locate(value, \"\\\\s\")[,1]+1, nchar(value))) |>\n  dplyr::mutate(value = stringr::str_remove_all(value, \"[a-z]|,\")) |>\n  dplyr::mutate(value = stringr::str_trim(value)) |>\n  dplyr::mutate(value = as.numeric(value))\n)\n\n# A tibble: 4 × 4\n  id              name          value unit \n  <chr>           <chr>         <dbl> <chr>\n1 Type 23 frigate Displacement 4900   t    \n2 Type 23 frigate Length        133   m    \n3 Type 23 frigate Beam           16.1 m    \n4 Type 23 frigate Draught         7.3 m    \n\ndf |> dplyr::select(-unit) |> tidyr::pivot_wider()\n\n# A tibble: 1 × 5\n  id              Displacement Length  Beam Draught\n  <chr>                  <dbl>  <dbl> <dbl>   <dbl>\n1 Type 23 frigate         4900    133  16.1     7.3\n\n\n\nlibrary(GGally)\n\nWarning: package 'GGally' was built under R version 4.2.3\n\n\nLoading required package: ggplot2\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n# From the help page:\ndata(flea)\nggpairs(flea, columns = 2:4, ggplot2::aes(colour=species))"
  },
  {
    "objectID": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Extract-People-from-Text.html",
    "href": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Extract-People-from-Text.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Code\npeople <- purrr::map(df$fields.body,~{\n  text <- .x |>\n    xml2::read_html() |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text()\n    \n  s <- NLP::String(paste(text, collapse = \"\\n\"))\n\n  ## Need sentence and word token annotations.\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  a2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n\n## Entity recognition for persons.\n  entity_annotator <- openNLP::Maxent_Entity_Annotator()\n\n  s[entity_annotator(s, a2)]\n})\n\n\n\n\nCode\norg <- purrr::map(df$fields.body,~{\n  text <- .x |>\n    xml2::read_html() |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text()\n    \n  s <- NLP::String(paste(text, collapse = \"\\n\"))\n\n  ## Need sentence and word token annotations.\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  a2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n\n## Entity recognition for persons.\n  entity_annotator <- openNLP::Maxent_Entity_Annotator(kind = \"organization\")\n\n  s[entity_annotator(s, a2)]\n}, .progress = TRUE)\n\n\n\n\nCode\nloc <- purrr::map(df$fields.body,~{\n  text <- .x |>\n    xml2::read_html() |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text()\n    \n  s <- NLP::String(paste(text, collapse = \"\\n\"))\n\n  ## Need sentence and word token annotations.\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  a2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n\n## Entity recognition for persons.\n  entity_annotator <- openNLP::Maxent_Entity_Annotator(kind = \"location\")\n\n  s[entity_annotator(s, a2)]\n}, .progress = TRUE)"
  },
  {
    "objectID": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Ukraine-What-We-Know-Guardian-NLP.html",
    "href": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Ukraine-What-We-Know-Guardian-NLP.html",
    "title": "Process Guardian - What we know on Ukraine",
    "section": "",
    "text": "Code\npages <- readr::read_csv(\"2023-05-18-guardian-ukraine-what-we-know.csv\")\n\n\nRows: 440 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (12): id, type, sectionId, sectionName, webTitle, webUrl, apiUrl, field...\nlgl   (1): isHosted\ndttm  (1): webPublicationDate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nprocess_text <- function(x){\n  xml2:::read_html(x) |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text() |>\n    stringr::str_split(\"\\n\") |>\n    unlist() |>\n    stringr::str_trim() |>\n    stringi::stri_remove_empty()\n}\n\nbody <- purrr::map(pages$fields.body, ~process_text(.x)) \n\n\n\n\nCode\ntop3 <- purrr::map(body, ~{\n  lexRankr::lexRank(.x,\n                    # only 1 article; repeat same docid for all of input vector\n                    docId = rep(1, length(.x)),\n                    # return 3 sentences\n                    n = 3,\n                    continuous = TRUE, \n                    Verbose = FALSE)\n})\n\ntop3_sent <- purrr::map(top3, ~.x$sentence)\n\n\n\n\nCode\n# Summarise\npages |>\n  dplyr::mutate(fields.headline = stringr::str_trim(fields.headline)) |>\n  dplyr::mutate(day = stringr::str_extract(fields.headline, \"day [0-9]{1,3}\")) |> \n  dplyr::mutate(day = stringr::str_remove_all(day, \"day \") |> as.numeric()) |>\n  dplyr::select(webPublicationDate, day,fields.trailText) |>\n  reactable::reactable(\n    details = function(index){\n      top3_i <- data.frame(top3_sent[[index]], stringsAsFactors = FALSE)\n      tbl <- reactable::reactable(top3_i, outlined = TRUE, highlight = TRUE, fullWidth = TRUE)\n      htmltools::div(style = list(margin = \"12px 45px\"), tbl)\n    },\n    onClick = \"expand\",\n    rowStyle = list(cursor = \"pointer\")\n  )\n\n\n\n\n\n\n\nCode\n#write.csv(pages, paste0(Sys.Date(), \"-guardian-ukraine-what-we-know.csv\"), row.names = FALSE)"
  },
  {
    "objectID": "posts/20230519-Voluntary Job Separation Rates/20230519-vjs-rates.html",
    "href": "posts/20230519-Voluntary Job Separation Rates/20230519-vjs-rates.html",
    "title": "Calculate Voluntary Job Separation Rates",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\n\n# This script calculates voluntary and involuntary job separations from the \n# labour market. \n\n#+ Setup ----\n\n# Connect to Database\ncon <- DBI::dbConnect(\n  odbc::odbc(), driver = \"PostgreSQL ODBC Driver(Unicode)\", \n  database = \"lfs\", uid = \"lheley\", host = \"localhost\", pwd = \"lheley\", \n  port = 5432, maxvarcharsize = 0\n)\n\n# Extract Meta Data\nstudy_meta <- con |> \n    dplyr::tbl(\"2qlfs_index\") |>\n    dplyr::select(1:4) |>\n    dplyr::collect() |>\n    dplyr::left_join(\n      con |>\n        dplyr::tbl(\"study_filename_lu\") |>\n        dplyr::collect() |>\n        dplyr::rename(SN = study) |>\n        dplyr::mutate(SN = as.numeric(SN)),\n      by = \"SN\"\n    )\n\n\n#+ Define Variables ----\n# These variables were defined through comparison of the previous\n# ONS publications. \n\n# Where the question or possible responses to a question have changed\n# the variable is updated. \n\nwnleft <- c(\"WNEFT112\",\"WNLEFT2\")\nrelft <- c(\"REDYL112\",\"REDYL132\",\"REDYLFT2\")\nsector <- \"PUBLICR1\"\nemployment <- \"ILODEFR1\"\nage <- \"AGE1\"\nid <- c(\"PERSID\")\nlgwt <- c(\"LGWT\",\"LGWT18\", \"LGWT20\") # This responds to different population weights.\nindustry <- \"INDD07M1\"\nvars <- c(id, lgwt, wnleft, relft, sector, employment, age, industry)\n\ntbls <- DBI::dbListTables(con)\ntbls <- tbls[grepl(\"sn_\", tbls)]\n\nvariables <- tbls |>\n  purrr::map_df(function(tbl){\n    variables <- con |>\n      dplyr::tbl(tbl) |>\n      dplyr::select(tidyselect::any_of(vars)) |>\n      head() |>\n      dplyr::collect() |>\n      names()\n    \n    tibble::tibble(study = substr(tbl, 4, 9), variables)\n  })\n\n#+ Select tables the contain the variables we need -----\ntbls <- paste0(\"sn_\", variables |> \n                 dplyr::filter(variables %in% vars) |>\n                 dplyr::mutate(variables2 = dplyr::case_when(\n                   variables %in% relft ~ \"REDYLFT\",\n                   variables %in% wnleft ~ \"WNLEFT\",\n                   variables %in% lgwt ~ \"LGWT\",\n                   TRUE ~ variables\n                 )) |>\n                 dplyr::select(-variables) |>\n                 dplyr::mutate(value = 1) |>\n                 tidyr::pivot_wider(names_from = variables2, values_from = value)  |>\n                 na.omit() |>\n                 dplyr::select(study) |> dplyr::pull())\n\n\n#+ Calculate Statistics ------\n# Loop through the table. \n# Calculate the number of people that reason for leaving was voluntary separation\n# Calculate the overall people\n# This can be extended to include involuntary job separations\n\n\n# This code chunk loops through the selected tables\n# It selects variables which match our specified variables in 'vars'\n# It then renames variables with inconsistent names\n# Then filters between 16 and 65\n# It recode reason left to determine voluntary job separations\n# It calculates whether individual left employmnet in last three months\n# It then groups by sector and calculate the weighted and unweighted number of \n#  voluntary job separates by total sector size\nvjs <- tbls |>\n  purrr::map_df(function(tbl){\n    sql_tbl <- con |> dplyr::tbl(tbl) \n    sql_tbl |>\n      dplyr::select(tidyselect::any_of(vars)) |>\n      dplyr::rename(REDYLFT = tidyselect::any_of(relft)) |>\n      dplyr::rename(WNLEFT = tidyselect::any_of(wnleft)) |>\n      dplyr::rename(LGWT = tidyselect::any_of(lgwt))|>\n      dplyr::filter(AGE1 >= 16 & AGE1 < 65) |>\n      dplyr::collect() |>\n      dplyr::mutate(VJS = dplyr::case_when(\n        \"REDYLFT2\" %in% dplyr::tbl_vars(sql_tbl) ~  REDYLFT %in% 4:9,\n        \"REDYL112\" %in% dplyr::tbl_vars(sql_tbl) ~  REDYLFT %in% 4:10,\n        \"REDYL132\" %in% dplyr::tbl_vars(sql_tbl) ~  REDYLFT %in% c(3, 5:11)\n      )) |>\n      dplyr::mutate(LFT3M = WNLEFT == 1 & ILODEFR1 == 1) |> \n      dplyr::mutate(VJS_3M =  VJS & LFT3M) |>\n      dplyr::mutate(EMP = ILODEFR1 == 1) |>\n      dplyr::mutate(PUBLIC = PUBLICR1 == 2) |>\n      dplyr::mutate(PRIVATE = PUBLICR1 == 1) |>\n      dplyr::mutate(SECTOR = ifelse(PUBLIC, \"Public\", ifelse(PRIVATE, \"Private\", NA))) |>\n      dplyr::group_by(SECTOR) |>\n      dplyr::summarise(vjs_3m_w = crossprod(LGWT, VJS_3M)[1],\n                       vjs_3m = sum(VJS_3M),\n                       n_w = crossprod(EMP, LGWT)[1],\n                       n = sum(EMP), tbl = tbl)\n  })\n\nvjs_total <- study_meta |> \n  dplyr::select(tbl = SN, sitdate = End) |>\n  dplyr::mutate(tbl = paste(\"sn\", tbl, sep = \"_\")) |>\n  dplyr::collect() |>\n  dplyr::left_join(vjs, by = \"tbl\")  |>\n  dplyr::group_by(sitdate) |>\n  dplyr::summarise(vjs_3m_w = sum(vjs_3m_w),\n                   n_w = sum(n_w)) |>\n  dplyr::mutate(vjs_rate = vjs_3m_w / n_w) \n\nvjs_sector <- study_meta |> \n  dplyr::select(tbl = SN, sitdate = End) |>\n  dplyr::mutate(tbl = paste(\"sn\", tbl, sep = \"_\")) |>\n  dplyr::collect() |>\n  dplyr::left_join(vjs, by = \"tbl\")  |>\n  dplyr::filter(!is.na(SECTOR)) |>\n  dplyr::group_by(sitdate, sector = SECTOR) |>\n  dplyr::summarise(vjs_3m_w = sum(vjs_3m_w),\n                   n_w = sum(n_w)) |>\n  dplyr::mutate(vjs_rate = vjs_3m_w / n_w) \n\n\n`summarise()` has grouped output by 'sitdate'. You can override using the\n`.groups` argument.\n\n\nCode\nvjs_total$vjs_rate[vjs_total$vjs_rate == 0] <- NA\nvjs_sector$vjs_rate[vjs_sector$vjs_rate == 0] <- NA\n\n\nggplot(vjs_total) + \n  geom_line(aes(sitdate, vjs_rate))\n\n\nWarning: Removed 66 rows containing missing values (`geom_line()`).\n\n\n\n\n\nCode\nggplot(vjs_sector) +\n  geom_line(aes(sitdate, vjs_rate)) +\n  facet_wrap(~sector)\n\n\n\n\n\nCode\nDBI::dbDisconnect(con)"
  },
  {
    "objectID": "posts/20230530-Homo Deus NLP/homo_deus_nlp.html",
    "href": "posts/20230530-Homo Deus NLP/homo_deus_nlp.html",
    "title": "Summarise Information in a Book",
    "section": "",
    "text": "Extract the Manuscript\n\n\nCode\nmanuscript <- paste(raw, collapse = \"\")\n\n\n\n\nSplit the manuscript into section heading and text.\n\n\nCode\nsection_text <- dplyr::tibble(manuscript) |> \n  dplyr::mutate(section = stringr::str_split(manuscript, \"\\n\\n\\n\")) |> \n  tidyr::unnest(cols = \"section\") |> \n  dplyr::select(-manuscript) |> \n  dplyr::filter(!stringr::str_detect(section, \"\\n\\n\")) |> \n  dplyr::slice(3:dplyr::n()) |> \n  dplyr::mutate(section = stringr::str_trim(section)) |> \n  tidyr::separate(section, c(\"section\", \"text\"), \"\\n\", extra = \"merge\") |> \n  dplyr::mutate(section_id = 1:dplyr::n())\n\n\n\n\nLexrankr\n\n\nCode\nlex_top_3 <- seq_along(section_text$text) |> \n  purrr::map_df(~{\n    lexRankr::lexRank(\n      section_text$text[[.x]],\n      docId = .x, \n      n = 5, \n      continuous = TRUE, \n      Verbose = FALSE\n    ) }) |> \n  tibble::as_tibble() |> \n  dplyr::mutate(docId = as.numeric(docId))\n\n\n\n\nResults\n\n\nCode\ndplyr::left_join(section_text, lex_top_3, by = c(\"section_id\" = \"docId\")) |> \n  dplyr::select(section_id, section, sentenceId, sentence, value) |> \n  DT::datatable(options = list(pageLength = 5))"
  }
]