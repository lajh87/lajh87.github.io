[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Iran Sanctions - Sythetic Control\n\n\n\n\n\n\n\nevaluation\n\n\nsynthetic-control\n\n\nmethods\n\n\nnotes\n\n\n\n\nAn applied example of synthetic control\n\n\n\n\n\n\nAug 29, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nSynthetic Control Methods\n\n\n\n\n\n\n\nevaluation\n\n\nsynthetic-control\n\n\nmethods\n\n\nnotes\n\n\n\n\nNotes on Synthetic Control\n\n\n\n\n\n\nAug 29, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nRegression Discontinuity Design\n\n\n\n\n\n\n\nevaluation\n\n\nquasi-experimental\n\n\nmethods\n\n\nnotes\n\n\n\n\nNotes on Regression Discontinuity Design\n\n\n\n\n\n\nJun 18, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nSummarise Information in a Book\n\n\n\n\n\n\n\nNLP\n\n\n\n\nSummarise the Top 5 Setences by Section in Homo Deus\n\n\n\n\n\n\nMay 30, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nCalculate Voluntary Job Separation Rates\n\n\n\n\n\n\n\nLabour Markets\n\n\nLabour Force Survey\n\n\n\n\nCalculate voluntary job separation rates from the two quarter labour force survey\n\n\n\n\n\n\nMay 19, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nNatural Language Processing\n\n\n\n\n\n\n\nNLP\n\n\n\n\nExtract people from Guardian reporting on what we know on Ukraine.\n\n\n\n\n\n\nMay 18, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nProcess Guardian - What we know on Ukraine\n\n\n\n\n\n\n\napi\n\n\nNLP\n\n\n\n\nThis post uses API to extract Guardian What we Know data and LexR to extract top 3 points.\n\n\n\n\n\n\nMay 18, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nExtract UK Frigate Data\n\n\n\n\n\n\n\nwikipedia\n\n\n\n\nSearch and extract UK Frigate Data\n\n\n\n\n\n\nApr 1, 2023\n\n\nDefence Economist\n\n\n\n\n\n\n  \n\n\n\n\nExtract Type 23 Data\n\n\n\n\n\n\n\nwikipedia\n\n\nscrape\n\n\n\n\nExtract Type 23 Performance Data\n\n\n\n\n\n\nApr 1, 2023\n\n\nDefence Economist\n\n\n\n\n\n\n  \n\n\n\n\nUsing Wiki Data to Extract Data\n\n\n\n\n\n\n\nwikipedia\n\n\nscrape\n\n\n\n\nThis post shows how to access the wikipedia knowledge graph\n\n\n\n\n\n\nMar 26, 2023\n\n\nLuke Heley\n\n\n\n\n\n\n  \n\n\n\n\nExtract of UK Frigates and Their Precendents\n\n\n\n\n\n\n\nwikipedia\n\n\nscrape\n\n\n\n\nThis post shows how to access the wikipedia knowledge graph\n\n\n\n\n\n\nMar 26, 2023\n\n\nLuke Heley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20230326-using-wikidata/using-wikidata.html",
    "href": "posts/20230326-using-wikidata/using-wikidata.html",
    "title": "Using Wiki Data to Extract Data",
    "section": "",
    "text": "Data\nWe use the wikidata API to extract the knowledge graph from wiki.\nThe approach is to search wikidata for the item of interest and select the best match from the list of search results that return.\n\n# function to search wiki data and return a tibble of search results\nsearch_wikidata <- function(search){\n  base <- \"https://www.wikidata.org\" \n  path <- \"/w/api.php\"\n  query <- list(\n    action=\"query\",\n    list=\"search\",\n    format = \"json\",\n    srsearch = search\n  )\n  \n  httr::GET(base, path = path, query = query) |>\n    httr::content() |>\n    purrr::pluck(\"query\", \"search\") |>\n    purrr::map_df(~{\n      ul <- unlist(.x)\n      name <- names(ul)\n      value <- as.character(ul)\n      dplyr::tibble(name, value) |>\n        tidyr::pivot_wider()\n      })\n}\n\n(search_results <- search_wikidata(\"Type-094 submarine\"))\n\n# A tibble: 2 × 6\n  ns    title    pageid  wordcount snippet                               times…¹\n  <chr> <chr>    <chr>   <chr>     <chr>                                 <chr>  \n1 0     Q1203377 1146175 0         nuclear-powered ballistic missile su… 2023-0…\n2 0     Q7008427 6886594 0         Wikimedia category                    2022-1…\n# … with abbreviated variable name ¹​timestamp\n\n# Extract the item title for the item of interest\nroot_item_title <- search_results$title[1]\n\nWe then extract wikidata associated with the item.\n\n# extract the entity data for a chosen item.\nget_entity_data <- function(item = \"Q1203377\"){\n  base <- \"https://www.wikidata.org/\"\n  path <- glue::glue(\"wiki/Special:EntityData/{item}.json\")\n  query <- list(flavor = \"simple\")  \n  req <- httr::GET(base, path = path, query = query)\n  \n  httr::content(req) |>\n    purrr::pluck(\"entities\", item, \"claims\") |>\n    purrr::map_df(~{\n      value <- unlist(.x)\n      name <- names(value)\n      dplyr::tibble(name, value) |>\n        tidyr::pivot_wider() |>\n        tidyr::unnest()\n      })\n}\n\nentity_data <- get_entity_data(root_item_title)\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n    dplyr::group_by(name) %>%\n    dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n    dplyr::filter(n > 1L)\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(mainsnak.snaktype, mainsnak.property, mainsnak.hash, `mainsnak.datavalue.value.entity-type`, \n    `mainsnak.datavalue.value.numeric-id`, mainsnak.datavalue.value.id, \n    mainsnak.datavalue.type, mainsnak.datatype, type, id, rank)`\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n\n\nWarning: Values from `value` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n    dplyr::group_by(name) %>%\n    dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n    dplyr::filter(n > 1L)\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(mainsnak.snaktype, mainsnak.property, mainsnak.hash, `mainsnak.datavalue.value.entity-type`, \n    `mainsnak.datavalue.value.numeric-id`, mainsnak.datavalue.value.id, \n    mainsnak.datavalue.type, mainsnak.datatype, type, id, rank, \n    references.hash, references.snaks.P143.snaktype, references.snaks.P143.property, \n    references.snaks.P143.hash, `references.snaks.P143.datavalue.value.entity-type`, \n    `references.snaks.P143.datavalue.value.numeric-id`, references.snaks.P143.datavalue.value.id, \n    references.snaks.P143.datavalue.type, references.snaks.P143.datatype, \n    `references.snaks-order`)`\n\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n`cols` is now required when using unnest().\nPlease use `cols = c()`\n\n\nAnd the associated properties\n\nproperties <- entity_data |> \n  dplyr::pull(\"mainsnak.property\") |>\n  unique()\n\nget_entity_id <- function(id = \"P373\"){\n  if(length(id)>1) id <- paste(id, collapse = \"|\")\n  base <- \"https://www.wikidata.org/\"\n  path <- \"w/api.php\"\n  query <- list(\n    action=\"wbgetentities\",\n    ids=id,\n    languages=\"en\",\n    props=\"labels\",\n    format=\"json\"\n    )\n  \n  req <- httr::GET(base, path = path, query = query) \n  \n  if(req$status != 200) \n    return(stop(glue::glue(\"Error returned status: {req$status}\")))\n  \n  httr::content(req) |>\n    purrr::pluck(\"entities\") |>\n    purrr::map_df(~{\n      value <- unlist(.x)\n      name <- names(value)\n      dplyr::tibble(name, value) |> tidyr::pivot_wider()\n    })\n}\n\n(prop_label <- get_entity_id(properties))\n\n# A tibble: 16 × 5\n   type     datatype        id    labels.en.language labels.en.value      \n   <chr>    <chr>           <chr> <chr>              <chr>                \n 1 property string          P373  en                 Commons category     \n 2 property wikibase-item   P516  en                 powered by           \n 3 property string          P561  en                 NATO reporting name  \n 4 property wikibase-item   P31   en                 instance of          \n 5 property wikibase-item   P279  en                 subclass of          \n 6 property wikibase-item   P910  en                 topic's main category\n 7 property external-id     P646  en                 Freebase ID          \n 8 property time            P729  en                 service entry        \n 9 property wikibase-item   P156  en                 followed by          \n10 property wikibase-item   P155  en                 follows              \n11 property commonsMedia    P18   en                 image                \n12 property wikibase-item   P176  en                 manufacturer         \n13 property wikibase-item   P137  en                 operator             \n14 property monolingualtext P1813 en                 short name           \n15 property wikibase-item   P520  en                 armament             \n16 property wikibase-item   P495  en                 country of origin    \n\n\nGet the item data\n\nentity_data2 <- entity_data |>\n  dplyr::select(\n    mainsnak.property,\n    mainsnak.datavalue.type,\n    mainsnak.datavalue.value.id,\n    mainsnak.datavalue.value,\n    mainsnak.datavalue.value.text,\n    mainsnak.datavalue.value.time\n  ) |>\n  dplyr::mutate(\n    value = dplyr::case_when(\n      !is.na(mainsnak.datavalue.value.id) ~ mainsnak.datavalue.value.id,\n      !is.na(mainsnak.datavalue.value) ~ mainsnak.datavalue.value,\n      !is.na(mainsnak.datavalue.value.time) ~ mainsnak.datavalue.value.time,\n      !is.na(mainsnak.datavalue.value.text) ~ mainsnak.datavalue.value.text\n    )\n  ) |>\n  dplyr::select(property = 1, type = 2, value) |>\n  dplyr::distinct()\n\nentity_data3 <- entity_data2 |>\n  dplyr::left_join(prop_label |>\n  dplyr::select(\n    property = id, \n    property_label = labels.en.value\n  ))\n\nJoining, by = \"property\"\n\nitems <- entity_data3 |>\n  dplyr::filter(type == \"wikibase-entityid\") |>\n  dplyr::pull(value) |> \n  unique()\n\nitem_labels <- get_entity_id(items)\n\n(item_property <- entity_data3 |>\n  dplyr::left_join(\n    item_labels |>\n      dplyr::select(value = id, item_label = labels.en.value)\n  ) |>\n  dplyr::mutate(item_label = dplyr::case_when(is.na(item_label)~value, \n                                              TRUE ~ item_label)) |>\n  dplyr::select(property_label, item_label))\n\nJoining, by = \"value\"\n\n\n# A tibble: 17 × 2\n   property_label        item_label                                          \n   <chr>                 <chr>                                               \n 1 Commons category      Type 09IV submarines                                \n 2 powered by            nuclear marine propulsion                           \n 3 NATO reporting name   Jin                                                 \n 4 instance of           submarine class                                     \n 5 subclass of           ballistic missile submarine                         \n 6 subclass of           nuclear submarine                                   \n 7 topic's main category Category:Type 094 submarines                        \n 8 Freebase ID           /m/09wz47                                           \n 9 service entry         +2010-01-01T00:00:00Z                               \n10 followed by           Type 096 submarine                                  \n11 follows               Type 092 Daqingyu                                   \n12 image                 Jin (Type 094) Class Ballistic Missile Submarine.JPG\n13 manufacturer          Bohai Shipyard                                      \n14 operator              People's Liberation Army Navy                       \n15 short name            Type 094                                            \n16 armament              JL-2                                                \n17 country of origin     People's Republic of China                          \n\n\nGet wiki urls\n\nget_wikisites <- function(item = \"Q1203377\"){\n  base <- \"https://www.wikidata.org/\"\n  path <- glue::glue(\"wiki/Special:EntityData/{item}.json\")\n  query <- list(flavor = \"simple\")  \n  req <- httr::GET(base, path = path, query = query)\n  \n  cont <- httr::content(req) \n  cont |>\n    purrr::pluck(\"entities\", item, \"sitelinks\") |>\n    purrr::map_df(~{\n      value <- unlist(.x)\n      name <- names(value)\n      dplyr::tibble(name, value) |>\n        tidyr::pivot_wider()\n      })\n}\n\n(wikiurl <- get_wikisites(root_item_title) |>\n  dplyr::filter(site == \"enwiki\") |>\n  dplyr::pull(url))\n\n                                                   \n\"https://en.wikipedia.org/wiki/Type_094_submarine\" \n\n\nScrape the infobox from the wiki url\n\nscrape_infobox <- function(\n    url =\"https://en.wikipedia.org/wiki/Type_094_submarine\"\n){\n  req <- httr::GET(url)\n  req |> \n    httr::content() |> \n    xml2::xml_find_all(\"//table[@class='infobox']\") |>\n    rvest::html_table() \n}\n\nscrape_infobox(\"https://en.wikipedia.org/wiki/Type_094_submarine\")\n\n[[1]]\n# A tibble: 22 × 2\n   X1                      X2                               \n   <chr>                   <chr>                            \n 1 Profile of the Type 094 Profile of the Type 094          \n 2 Type 094 submarine      Type 094 submarine               \n 3 Class overview          Class overview                   \n 4 Name                    Type 094 (Jin class)             \n 5 Builders                Bohai Shipyard, Huludao, China[2]\n 6 Operators               People's Liberation Army Navy    \n 7 Preceded by             Type 092 (Xia class)             \n 8 Succeeded by            Type 096                         \n 9 Cost                    $750 million per unit[1]         \n10 In commission           2007–present[2]                  \n# … with 12 more rows\n\n\nScrape wikitables\n\nscrape_wikitables <- function(\n    url =\"https://en.wikipedia.org/wiki/Type_094_submarine\"\n){\n  req <- httr::GET(url)\n  req |> \n    httr::content() |> \n    xml2::xml_find_all(\"//table[@class='wikitable']\") |>\n    rvest::html_table()\n}\n\nscrape_wikitables(\"https://en.wikipedia.org/wiki/Type_094_submarine\")\n\n[[1]]\n# A tibble: 8 × 7\n  Name                `Hull no.`  Builder         Laid …¹ Launc…² Commi…³ Status\n  <chr>               <chr>       <chr>           <chr>   <chr>   <chr>   <chr> \n1 \"Type 094\"          \"Type 094\"  \"Type 094\"      \"Type … \"Type … Type 0… Type …\n2 \"\"                  \"411[2]\"    \"Bohai Shipyar… \"2001[… \"28 Ju… March … Active\n3 \"Changzheng 10[20]\" \"412[2]\"    \"Bohai Shipyar… \"2003[… \"2006[… 2010[2] Active\n4 \"Changzheng 11[20]\" \"413[2]\"    \"Bohai Shipyar… \"2004[… \"Decem… 2012[2] Active\n5 \"Changzheng 18[21]\" \"421[22]\"   \"\"              \"\"      \"\"      23 Apr… Active\n6 \"Type 094A\"         \"Type 094A\" \"Type 094A\"     \"Type … \"Type … Type 0… Type …\n7 \"\"                  \"\"          \"\"              \"\"      \"\"      2020[5] Active\n8 \"\"                  \"\"          \"\"              \"\"      \"\"      2020[5] Active\n# … with abbreviated variable names ¹​`Laid down`, ²​Launched, ³​Commissioned"
  },
  {
    "objectID": "posts/20230401-extract-uk-frigate-data/20230401-extract-uk-frigate-data.html",
    "href": "posts/20230401-extract-uk-frigate-data/20230401-extract-uk-frigate-data.html",
    "title": "Extract UK Frigate Data",
    "section": "",
    "text": "Data\ntype 26, type 23, type 22, type 21, leander class, rothesay-class, valour-class, Whitby class and Sailsbury class\n\nfrigates <- c(\n  \"Type 26\", \"Type 23\", \"Type 22\", \n  \"Type 21\", \"Leander Class\", \"Rothesay Class\", \n   \"Whitby Class\", \"Salisbury Class\"\n  ) |>\n  paste(\"Frigate\")\n\nbase <- \"https://www.wikidata.org\" \npath <- \"/w/api.php\"\nquery <- list(\n  action=\"query\",\n  list=\"search\",\n  format = \"json\"\n)\n\nsearch_results <- frigates |>\n  purrr::map(~{\n    query$srsearch = .x\n    httr::GET(base, path = path, query = query) |>\n      httr::content()\n  })\n\n\nsearch_results_tidy <- search_results |>\n  purrr::map_df(~{\n     .x$query$search[1]; # extract first response\n    } ) |>\n  dplyr::select(title, snippet) |>\n  dplyr::mutate(search = frigates) |>\n  dplyr::relocate(search)\n\nget wiki url\n\nbase <- \"https://www.wikidata.org/\"\nreq <- search_results_tidy$title |>\n  purrr::map(~{\n    path <- glue::glue(\"wiki/Special:EntityData/{.x}.json\")\n    httr::GET(url = base, path = path, query = list(flavor = \"simple\"))\n  })\n\n(wiki_url <- req |>\n  purrr::map_chr(~{\n    cnt <- httr::content(.x)\n    cnt$entities[[1]]$sitelinks$enwiki$url\n  }))\n\n[1] \"https://en.wikipedia.org/wiki/Type_26_frigate\"        \n[2] \"https://en.wikipedia.org/wiki/Type_23_frigate\"        \n[3] \"https://en.wikipedia.org/wiki/Type_22_frigate\"        \n[4] \"https://en.wikipedia.org/wiki/Type_21_frigate\"        \n[5] \"https://en.wikipedia.org/wiki/Leander-class_frigate\"  \n[6] \"https://en.wikipedia.org/wiki/Rothesay-class_frigate\" \n[7] \"https://en.wikipedia.org/wiki/Whitby-class_frigate\"   \n[8] \"https://en.wikipedia.org/wiki/Salisbury-class_frigate\"\n\n\n\ninfobox <- wiki_url |>\n  purrr::map_df(~{\n    httr::GET(.x) |>\n      httr::content() |>\n      xml2::xml_find_all(\"//table[@class='infobox']\") |>\n      rvest::html_table() |>\n      purrr::pluck(1) |>\n      dplyr::mutate(source = .x)\n  })\n\nvars <- c(\"Displacement\", \"Length\", \"Beam\", \"Draught\")\n\nperformance <- infobox |>\n  dplyr::group_by(source) |>\n  dplyr::mutate(X0 = ifelse(X1 == X2, X1, NA)) |>\n  tidyr::fill(X0) |>\n  dplyr::filter(X1 != X2) |>\n  dplyr::filter(X1 %in% vars) |>\n  dplyr::ungroup() |>\n  dplyr::mutate(key = 1L:dplyr::n()) |>\n  dplyr::group_by(source) |>\n  dplyr::mutate(X2 = stringr::str_split(X2, \"\\n\")) |>\n  tidyr::unnest(cols = \"X2\") |>\n  dplyr::ungroup() |>\n  dplyr::mutate(\n    X3 = ifelse(\n      test = stringr::str_detect(X2, \"^[0-9]\", negate = TRUE), \n      yes = substr(X2, 1, stringr::str_locate(X2, \":\")[,1]-1),\n      no = NA)\n  ) |>\n   dplyr::mutate(\n    X2 = ifelse(\n      test = stringr::str_detect(X2, \"^[0-9]\", negate = TRUE), \n      yes = substr(X2, stringr::str_locate(X2, \":\")[,1]+2, nchar(X2)),\n      no = X2)\n  )  |>\n  dplyr::mutate(\n    value = as.numeric(stringr::str_remove_all(substr(X2, 1, stringr::str_locate(X2, \"\\\\s\")[,1]-1), \",\"))\n  ) |>\n  dplyr::mutate(\n    units = substr(X2, stringr::str_locate(X2, \"\\\\s\")[,1]+1, nchar(X2))\n  ) |>\n  dplyr::mutate(\n    units = substr(units, 1, stringr::str_locate(units, \"\\\\s\")[,1]-1)\n  ) |> \n  dplyr::mutate(\n    label = stringr::str_remove_all(source, \"https://en.wikipedia.org/wiki/\") |>\n      stringr::str_replace_all(\"_\", \" \")\n    ) |>\n  dplyr::filter(!is.na(value)) |>\n  dplyr::group_by(key) |>\n  dplyr::mutate(increment = 1:dplyr::n()) |>\n  dplyr::select(key, increment, label, name = X1, value, units) |>\n  dplyr::mutate(units = dplyr::case_when(\n    units %in% c(\"tonnes\", \"ton\", \"tonnes,\", \"tons\") ~ \"t\",\n    units %in% c(\"long\") ~ \"lt\",\n    units %in% c(\"metres\") ~ \"m\",\n    TRUE ~ units\n  )) |>\n  dplyr::mutate(\n    value = dplyr::case_when(\n      units %in% \"lt\" ~ (value * 1.016047), \n      units %in% \"ft\" ~ (value * 0.3048),\n      TRUE ~ value\n    )\n  ) |>\n  dplyr::mutate(\n    units = dplyr::case_when(\n       units %in% \"lt\" ~ \"t\",\n       units %in% \"ft\" ~ \"m\",\n       TRUE ~ units\n    )\n  ) |>\n  dplyr::ungroup() |>\n  dplyr::select(-units, -key, -increment) |> \n  dplyr::group_by(label, name) |>\n  dplyr::mutate(increment = 1:dplyr::n()) |>\n  tidyr::pivot_wider(names_from = name, values_from = value) |>\n  tidyr::fill(c(Length, Beam, Draught)) \n\nx <- GGally::ggpairs(performance, lower= list(mapping = ggplot2::aes(colour = label)), columns = 3:6) \n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nplotly::ggplotly(x)\n\nWarning: Can only have one: highlight\n\nWarning: Can only have one: highlight\n\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\nWarning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, :\nRemoved 3 rows containing missing values\n\n\nWarning: Removed 3 rows containing non-finite values (stat_density).\n\n\nWarning: Can only have one: highlight"
  },
  {
    "objectID": "posts/20230401-wikidata-extract-preceded-by/20230401-extract-preceded-by.html",
    "href": "posts/20230401-wikidata-extract-preceded-by/20230401-extract-preceded-by.html",
    "title": "Extract of UK Frigates and Their Precendents",
    "section": "",
    "text": "Objective\nExtract list of UK Frigates and their precedents.\n\n\nData\nWe use wikidata to first extract the different types of warship\n\nq &lt;- '\nSELECT ?ship_type ?ship_typeLabel WHERE {\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n  ?ship_type wdt:P31 wd:Q2235308;\n    wdt:P279 wd:Q3114762.\n  \n  OPTIONAL {  }\n}\n  '\n\nWikidataQueryServiceR::query_wikidata(q)\n\nRows: 55 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): ship_type, ship_typeLabel\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 55 × 2\n   ship_type                              ship_typeLabel  \n   &lt;chr&gt;                                  &lt;chr&gt;           \n 1 http://www.wikidata.org/entity/Q17205  aircraft carrier\n 2 http://www.wikidata.org/entity/Q104843 cruiser         \n 3 http://www.wikidata.org/entity/Q161705 frigate         \n 4 http://www.wikidata.org/entity/Q170013 corvette        \n 5 http://www.wikidata.org/entity/Q174736 destroyer       \n 6 http://www.wikidata.org/entity/Q182531 battleship      \n 7 http://www.wikidata.org/entity/Q188924 galley          \n 8 http://www.wikidata.org/entity/Q207452 ship of the line\n 9 http://www.wikidata.org/entity/Q629990 bireme          \n10 http://www.wikidata.org/entity/Q640078 minelayer       \n# … with 45 more rows\n\n\nAnd subsequently different types of frigates operated by the Royal Navy\n\nq &lt;- '\nSELECT ?ship_class ?ship_classLabel ?service_entry ?followed_by ?followed_byLabel ?operator ?operatorLabel ?image ?service_retirement ?follows ?followsLabel ?service_entryLabel ?service_retirementLabel ?imageLabel WHERE {\n  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n  ?ship_class wdt:P31 wd:Q559026; # instance of ship class\n    wdt:P279 wd:Q106070703. # subclass of frigate\n  ?ship_class wdt:P137 wd:Q172771. # operated by RN\n  OPTIONAL { ?ship_class wdt:P729 ?service_entry. }\n  OPTIONAL { ?ship_class wdt:P730 ?service_retirement. }\n  OPTIONAL { ?ship_class wdt:P155 ?follows. }\n  OPTIONAL { ?ship_class wdt:P156 ?followed_by. }\n  OPTIONAL { ?ship_class wdt:P137 ?operator. }\n  OPTIONAL { ?ship_class wdt:P18 ?image. }\n}\n'\nrn_frigates &lt;- WikidataQueryServiceR::query_wikidata(q)\n\nRows: 83 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): ship_class, ship_classLabel, followed_by, followed_byLabel, opera...\ndttm  (4): service_entry, service_retirement, service_entryLabel, service_re...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nrn_frigates |&gt;\n  dplyr::filter(operatorLabel == \"Royal Navy\") |&gt;\n  dplyr::select(from = ship_classLabel, \n                to = followed_byLabel, \n                value = service_entry) |&gt;\n  dplyr::distinct() |&gt;\n  dplyr::filter(!is.na(to)) |&gt;\n  igraph::graph_from_data_frame() |&gt;\n  visNetwork::visIgraph()\n\n\n\n\n\nNext step. Extract data on type 26, type 23, type 22, type 21, leander class, rothesay-class, valour-class, whiteby-class and saisbury class.\nFuture intention is to compare to other nations where data are available."
  },
  {
    "objectID": "posts/20230401-wikidata-type23/20230401-wikidata-type23.html",
    "href": "posts/20230401-wikidata-type23/20230401-wikidata-type23.html",
    "title": "Extract Type 23 Data",
    "section": "",
    "text": "Objective\nExtract Type 23 performance data from wikipedia\n\n\nData\n\nurl &lt;- \"https://en.wikipedia.org/wiki/Type_23_frigate\"\nreq &lt;- httr::GET(url) |&gt;\n  httr::content()\n\nvars &lt;- c(\"Name\", \"Displacement\", \"Length\", \"Beam\", \"Draught\")\n(df &lt;- req |&gt;\n  xml2::xml_find_all(\"//table[@class = 'infobox']\") |&gt;\n  rvest::html_table() |&gt;\n  purrr::pluck(1) |&gt;\n  dplyr::filter(X1 %in% c(vars)) |&gt;\n  t() |&gt; dplyr::as_tibble(.name_repair = \"minimal\") |&gt;\n  janitor::row_to_names(1) |&gt;\n  dplyr::mutate(dplyr::across(-1, ~stringr::str_sub(.x, 1, stringr::str_locate(.x, \"\\\\(\")[,1]-2))) |&gt;\n  tidyr::pivot_longer(-1) |&gt;\n  dplyr::rename(id = Name) |&gt;\n  dplyr::mutate(unit = substr(value, stringr::str_locate(value, \"\\\\s\")[,1]+1, nchar(value))) |&gt;\n  dplyr::mutate(value = stringr::str_remove_all(value, \"[a-z]|,\")) |&gt;\n  dplyr::mutate(value = stringr::str_trim(value)) |&gt;\n  dplyr::mutate(value = as.numeric(value))\n)\n\n# A tibble: 4 × 4\n  id              name          value unit \n  &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;\n1 Type 23 frigate Displacement 4900   t    \n2 Type 23 frigate Length        133   m    \n3 Type 23 frigate Beam           16.1 m    \n4 Type 23 frigate Draught         7.3 m    \n\ndf |&gt; dplyr::select(-unit) |&gt; tidyr::pivot_wider()\n\n# A tibble: 1 × 5\n  id              Displacement Length  Beam Draught\n  &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Type 23 frigate         4900    133  16.1     7.3\n\n\n\nlibrary(GGally)\n\nWarning: package 'GGally' was built under R version 4.2.3\n\n\nLoading required package: ggplot2\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n# From the help page:\ndata(flea)\nggpairs(flea, columns = 2:4, ggplot2::aes(colour=species))"
  },
  {
    "objectID": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Extract-People-from-Text.html",
    "href": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Extract-People-from-Text.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "Code\npeople <- purrr::map(df$fields.body,~{\n  text <- .x |>\n    xml2::read_html() |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text()\n    \n  s <- NLP::String(paste(text, collapse = \"\\n\"))\n\n  ## Need sentence and word token annotations.\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  a2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n\n## Entity recognition for persons.\n  entity_annotator <- openNLP::Maxent_Entity_Annotator()\n\n  s[entity_annotator(s, a2)]\n})\n\n\n\n\nCode\norg <- purrr::map(df$fields.body,~{\n  text <- .x |>\n    xml2::read_html() |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text()\n    \n  s <- NLP::String(paste(text, collapse = \"\\n\"))\n\n  ## Need sentence and word token annotations.\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  a2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n\n## Entity recognition for persons.\n  entity_annotator <- openNLP::Maxent_Entity_Annotator(kind = \"organization\")\n\n  s[entity_annotator(s, a2)]\n}, .progress = TRUE)\n\n\n\n\nCode\nloc <- purrr::map(df$fields.body,~{\n  text <- .x |>\n    xml2::read_html() |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text()\n    \n  s <- NLP::String(paste(text, collapse = \"\\n\"))\n\n  ## Need sentence and word token annotations.\n  sent_token_annotator <- openNLP::Maxent_Sent_Token_Annotator()\n  word_token_annotator <- openNLP::Maxent_Word_Token_Annotator()\n  a2 <- NLP::annotate(s, list(sent_token_annotator, word_token_annotator))\n\n## Entity recognition for persons.\n  entity_annotator <- openNLP::Maxent_Entity_Annotator(kind = \"location\")\n\n  s[entity_annotator(s, a2)]\n}, .progress = TRUE)"
  },
  {
    "objectID": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Ukraine-What-We-Know-Guardian-NLP.html",
    "href": "posts/20230518-Guardian-What-We-Know-Ukraine/20230518-Ukraine-What-We-Know-Guardian-NLP.html",
    "title": "Process Guardian - What we know on Ukraine",
    "section": "",
    "text": "Code\npages <- readr::read_csv(\"2023-05-18-guardian-ukraine-what-we-know.csv\")\n\n\nRows: 440 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (12): id, type, sectionId, sectionName, webTitle, webUrl, apiUrl, field...\nlgl   (1): isHosted\ndttm  (1): webPublicationDate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nprocess_text <- function(x){\n  xml2:::read_html(x) |>\n    xml2::xml_find_all(\"//p\") |>\n    xml2::xml_text() |>\n    stringr::str_split(\"\\n\") |>\n    unlist() |>\n    stringr::str_trim() |>\n    stringi::stri_remove_empty()\n}\n\nbody <- purrr::map(pages$fields.body, ~process_text(.x)) \n\n\n\n\nCode\ntop3 <- purrr::map(body, ~{\n  lexRankr::lexRank(.x,\n                    # only 1 article; repeat same docid for all of input vector\n                    docId = rep(1, length(.x)),\n                    # return 3 sentences\n                    n = 3,\n                    continuous = TRUE, \n                    Verbose = FALSE)\n})\n\ntop3_sent <- purrr::map(top3, ~.x$sentence)\n\n\n\n\nCode\n# Summarise\npages |>\n  dplyr::mutate(fields.headline = stringr::str_trim(fields.headline)) |>\n  dplyr::mutate(day = stringr::str_extract(fields.headline, \"day [0-9]{1,3}\")) |> \n  dplyr::mutate(day = stringr::str_remove_all(day, \"day \") |> as.numeric()) |>\n  dplyr::select(webPublicationDate, day,fields.trailText) |>\n  reactable::reactable(\n    details = function(index){\n      top3_i <- data.frame(top3_sent[[index]], stringsAsFactors = FALSE)\n      tbl <- reactable::reactable(top3_i, outlined = TRUE, highlight = TRUE, fullWidth = TRUE)\n      htmltools::div(style = list(margin = \"12px 45px\"), tbl)\n    },\n    onClick = \"expand\",\n    rowStyle = list(cursor = \"pointer\")\n  )\n\n\n\n\n\n\n\nCode\n#write.csv(pages, paste0(Sys.Date(), \"-guardian-ukraine-what-we-know.csv\"), row.names = FALSE)"
  },
  {
    "objectID": "posts/20230519-Voluntary Job Separation Rates/20230519-vjs-rates.html",
    "href": "posts/20230519-Voluntary Job Separation Rates/20230519-vjs-rates.html",
    "title": "Calculate Voluntary Job Separation Rates",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\n\n# This script calculates voluntary and involuntary job separations from the \n# labour market. \n\n#+ Setup ----\n\n# Connect to Database\ncon <- DBI::dbConnect(\n  odbc::odbc(), driver = \"PostgreSQL ODBC Driver(Unicode)\", \n  database = \"lfs\", uid = \"lheley\", host = \"localhost\", pwd = \"lheley\", \n  port = 5432, maxvarcharsize = 0\n)\n\n# Extract Meta Data\nstudy_meta <- con |> \n    dplyr::tbl(\"2qlfs_index\") |>\n    dplyr::select(1:4) |>\n    dplyr::collect() |>\n    dplyr::left_join(\n      con |>\n        dplyr::tbl(\"study_filename_lu\") |>\n        dplyr::collect() |>\n        dplyr::rename(SN = study) |>\n        dplyr::mutate(SN = as.numeric(SN)),\n      by = \"SN\"\n    )\n\n\n#+ Define Variables ----\n# These variables were defined through comparison of the previous\n# ONS publications. \n\n# Where the question or possible responses to a question have changed\n# the variable is updated. \n\nwnleft <- c(\"WNEFT112\",\"WNLEFT2\")\nrelft <- c(\"REDYL112\",\"REDYL132\",\"REDYLFT2\")\nsector <- \"PUBLICR1\"\nemployment <- \"ILODEFR1\"\nage <- \"AGE1\"\nid <- c(\"PERSID\")\nlgwt <- c(\"LGWT\",\"LGWT18\", \"LGWT20\") # This responds to different population weights.\nindustry <- \"INDD07M1\"\nvars <- c(id, lgwt, wnleft, relft, sector, employment, age, industry)\n\ntbls <- DBI::dbListTables(con)\ntbls <- tbls[grepl(\"sn_\", tbls)]\n\nvariables <- tbls |>\n  purrr::map_df(function(tbl){\n    variables <- con |>\n      dplyr::tbl(tbl) |>\n      dplyr::select(tidyselect::any_of(vars)) |>\n      head() |>\n      dplyr::collect() |>\n      names()\n    \n    tibble::tibble(study = substr(tbl, 4, 9), variables)\n  })\n\n#+ Select tables the contain the variables we need -----\ntbls <- paste0(\"sn_\", variables |> \n                 dplyr::filter(variables %in% vars) |>\n                 dplyr::mutate(variables2 = dplyr::case_when(\n                   variables %in% relft ~ \"REDYLFT\",\n                   variables %in% wnleft ~ \"WNLEFT\",\n                   variables %in% lgwt ~ \"LGWT\",\n                   TRUE ~ variables\n                 )) |>\n                 dplyr::select(-variables) |>\n                 dplyr::mutate(value = 1) |>\n                 tidyr::pivot_wider(names_from = variables2, values_from = value)  |>\n                 na.omit() |>\n                 dplyr::select(study) |> dplyr::pull())\n\n\n#+ Calculate Statistics ------\n# Loop through the table. \n# Calculate the number of people that reason for leaving was voluntary separation\n# Calculate the overall people\n# This can be extended to include involuntary job separations\n\n\n# This code chunk loops through the selected tables\n# It selects variables which match our specified variables in 'vars'\n# It then renames variables with inconsistent names\n# Then filters between 16 and 65\n# It recode reason left to determine voluntary job separations\n# It calculates whether individual left employmnet in last three months\n# It then groups by sector and calculate the weighted and unweighted number of \n#  voluntary job separates by total sector size\nvjs <- tbls |>\n  purrr::map_df(function(tbl){\n    sql_tbl <- con |> dplyr::tbl(tbl) \n    sql_tbl |>\n      dplyr::select(tidyselect::any_of(vars)) |>\n      dplyr::rename(REDYLFT = tidyselect::any_of(relft)) |>\n      dplyr::rename(WNLEFT = tidyselect::any_of(wnleft)) |>\n      dplyr::rename(LGWT = tidyselect::any_of(lgwt))|>\n      dplyr::filter(AGE1 >= 16 & AGE1 < 65) |>\n      dplyr::collect() |>\n      dplyr::mutate(VJS = dplyr::case_when(\n        \"REDYLFT2\" %in% dplyr::tbl_vars(sql_tbl) ~  REDYLFT %in% 4:9,\n        \"REDYL112\" %in% dplyr::tbl_vars(sql_tbl) ~  REDYLFT %in% 4:10,\n        \"REDYL132\" %in% dplyr::tbl_vars(sql_tbl) ~  REDYLFT %in% c(3, 5:11)\n      )) |>\n      dplyr::mutate(LFT3M = WNLEFT == 1 & ILODEFR1 == 1) |> \n      dplyr::mutate(VJS_3M =  VJS & LFT3M) |>\n      dplyr::mutate(EMP = ILODEFR1 == 1) |>\n      dplyr::mutate(PUBLIC = PUBLICR1 == 2) |>\n      dplyr::mutate(PRIVATE = PUBLICR1 == 1) |>\n      dplyr::mutate(SECTOR = ifelse(PUBLIC, \"Public\", ifelse(PRIVATE, \"Private\", NA))) |>\n      dplyr::group_by(SECTOR) |>\n      dplyr::summarise(vjs_3m_w = crossprod(LGWT, VJS_3M)[1],\n                       vjs_3m = sum(VJS_3M),\n                       n_w = crossprod(EMP, LGWT)[1],\n                       n = sum(EMP), tbl = tbl)\n  })\n\nvjs_total <- study_meta |> \n  dplyr::select(tbl = SN, sitdate = End) |>\n  dplyr::mutate(tbl = paste(\"sn\", tbl, sep = \"_\")) |>\n  dplyr::collect() |>\n  dplyr::left_join(vjs, by = \"tbl\")  |>\n  dplyr::group_by(sitdate) |>\n  dplyr::summarise(vjs_3m_w = sum(vjs_3m_w),\n                   n_w = sum(n_w)) |>\n  dplyr::mutate(vjs_rate = vjs_3m_w / n_w) \n\nvjs_sector <- study_meta |> \n  dplyr::select(tbl = SN, sitdate = End) |>\n  dplyr::mutate(tbl = paste(\"sn\", tbl, sep = \"_\")) |>\n  dplyr::collect() |>\n  dplyr::left_join(vjs, by = \"tbl\")  |>\n  dplyr::filter(!is.na(SECTOR)) |>\n  dplyr::group_by(sitdate, sector = SECTOR) |>\n  dplyr::summarise(vjs_3m_w = sum(vjs_3m_w),\n                   n_w = sum(n_w)) |>\n  dplyr::mutate(vjs_rate = vjs_3m_w / n_w) \n\n\n`summarise()` has grouped output by 'sitdate'. You can override using the\n`.groups` argument.\n\n\nCode\nvjs_total$vjs_rate[vjs_total$vjs_rate == 0] <- NA\nvjs_sector$vjs_rate[vjs_sector$vjs_rate == 0] <- NA\n\n\nggplot(vjs_total) + \n  geom_line(aes(sitdate, vjs_rate))\n\n\nWarning: Removed 66 rows containing missing values (`geom_line()`).\n\n\n\n\n\nCode\nggplot(vjs_sector) +\n  geom_line(aes(sitdate, vjs_rate)) +\n  facet_wrap(~sector)\n\n\n\n\n\nCode\nDBI::dbDisconnect(con)"
  },
  {
    "objectID": "posts/20230530-Homo Deus NLP/homo_deus_nlp.html",
    "href": "posts/20230530-Homo Deus NLP/homo_deus_nlp.html",
    "title": "Summarise Information in a Book",
    "section": "",
    "text": "Load the Raw Data using PDFTOOLS\n\n\nCode\nraw &lt;- pdftools::pdf_text(\"homo_deus_chapter_1.pdf\")\n\n\n\n\nExtract the Manuscript\n\n\nCode\nmanuscript &lt;- paste(raw, collapse = \"\")\n\n\n\n\nSplit the manuscript into section heading and text.\n\n\nCode\nsection_text &lt;- dplyr::tibble(manuscript) |&gt; \n  dplyr::mutate(section = stringr::str_split(manuscript, \"\\n\\n\\n\")) |&gt; \n  tidyr::unnest(cols = \"section\") |&gt; \n  dplyr::select(-manuscript) |&gt; \n  dplyr::filter(!stringr::str_detect(section, \"\\n\\n\")) |&gt; \n  dplyr::slice(3:dplyr::n()) |&gt; \n  dplyr::mutate(section = stringr::str_trim(section)) |&gt; \n  tidyr::separate(section, c(\"section\", \"text\"), \"\\n\", extra = \"merge\") |&gt; \n  dplyr::mutate(section_id = 1:dplyr::n())\n\n\n\n\nLexrankr\n\n\nCode\nlex_top_3 &lt;- seq_along(section_text$text) |&gt; \n  purrr::map_df(~{\n    lexRankr::lexRank(\n      section_text$text[[.x]],\n      docId = .x, \n      n = 5, \n      continuous = TRUE, \n      Verbose = FALSE\n    ) }) |&gt; \n  tibble::as_tibble() |&gt; \n  dplyr::mutate(docId = as.numeric(docId))\n\n\n\n\nResults\n\n\nCode\ndplyr::left_join(section_text, lex_top_3, by = c(\"section_id\" = \"docId\")) |&gt; \n  dplyr::select(section_id, section, sentenceId, sentence, value) |&gt; \n  DT::datatable(options = list(pageLength = 5))"
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "This method estimates the impact of an intervention by using a cut-off threshold to assign the intervention. The method relies on the assumption that the individuals just below and above a cutoff threshold will be similar, with the only significant difference between the 2 groups being whether they received the intervention or not. By comparing the value of the outcome variable for the individuals just above and below the cut-off threshold, the method infers the impact of the intervention (Treasury 2020)."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#references",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#references",
    "title": "Learn Regression Discontinuity Design",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#introduction",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#introduction",
    "title": "Regression Discontinuity Design",
    "section": "Introduction",
    "text": "Introduction\n\nSocial programmes often have eligability indices that determine who can and cannot signup.\nExamples include targeted antipoverty programmes based on means. Tests scores.\n\n\nRegression discontinuity design (RDD) is an impact evaluation method that is adequate for programs that use a continuous index to rank potential participants and that have a cutoff point along the index that determines whether or not potential participants are eligible to receive the program."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#conditions",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#conditions",
    "title": "Regression Discontinuity Design",
    "section": "Conditions",
    "text": "Conditions\nTo apply RDD 4 main conditions must be met:\n\nThe index must rank people in a continuous way. By contrast, variables that have discrete categories cannot be ranked.\nThere must be a clear defined cutoff score. The value above or below that the population is eligable for the programme.\nThe cutoff must be unique to the programme of interest. There must be no other programme, apart from the one to be evaluated that uses the same cutoff score.\nThe score of a particular individual or unit cannot be manipuated by data collectors, beneficiaries, administrators, or politicians.\n\nRDD esitmate impact around the eligability cutoff as difference between average outcome for units on the treated side of eligability cutoff and average outcome of units on the untreated (comparison) side of the cutoff.\nAn example based on agriculture where the scheme is determine by hectare of land is provided.\nIt makes the point that since the comparison group is made up of farm just above the eligability threshold, teh impact given by RDD is valid only locally. Therefore we obtain an estimate of the Local Average Treatment Effect (LATE). The RDD will not be able to identify the impact on the smallest farms.\nAn advantage is that when the eligability rule is applied, not eligable units need ot be left untreated for the purpose of the impact evaluation. The trade off is that observations far away from the cutoff will not be known."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#fuzzy-rdd",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#fuzzy-rdd",
    "title": "Regression Discontinuity Design",
    "section": "Fuzzy RDD",
    "text": "Fuzzy RDD\nFuzzy RDD. Some units who qualify for the programme may not opt to participate. Or others may find a way to participate even if they are on the other side of the line.\nWhen all units comply with the assignment that corresponds to them on the basis of their eligibility index, we say that the RDD is “sharp,” while if there is noncompliance on either side of the cutoff , then we say that the RDD is “fuzzy”.\nIf the RDD is fuzzy, we can use the instrumental variable approach to correct for the noncompliance. In the case of randomized assignment with noncompliance, we used the randomized assignment as the instrumental variable that helped us correct for noncompliance.\nIn the case of RDD, we can use the original assignment based on the eligibility index as the instrumental variable. Doing so has a drawback, though: our instrumental RDD impact estimate will be further localized—in the sense that it is no longer valid to all observations close to the cutoff , but instead represents the impact for the subgroup of the population that is located close to the cutoff point and that participates in the program only because of the eligibility criteria.\n\n\n\nGertler et al 2016 Figure 6.3"
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#testing-validity-of-rdd",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#testing-validity-of-rdd",
    "title": "Regression Discontinuity Design",
    "section": "Testing Validity of RDD",
    "text": "Testing Validity of RDD\nFor RDD to provide unbiased estimates of LATE there must be no manipulation of the eligibility index around the cutoff.\nA tell tale sign of manipuation is where there is bunching around the cutoff, e.g. look at the percentage of households at the cuttoff. A second test is to plot the eligability index against the outcome index at baseline and check there are no discontinuities or jump right around the cutoff."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#limitations-and-interpretations",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#limitations-and-interpretations",
    "title": "Regression Discontinuity Design",
    "section": "Limitations and Interpretations",
    "text": "Limitations and Interpretations\nBecause the RDD method estimates the impact of the program around the cutoff score, or locally, the estimate cannot necessarily be generalized to units whose scores are further away from the cutoff score: that is, where eligible and ineligible individuals may not be as similar."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#checklist",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#checklist",
    "title": "Regression Discontinuity Design",
    "section": "Checklist",
    "text": "Checklist\n\nIs the index continuous around the cutoff score at the time of the baseline?\nIs there any evidence of noncompliance with the rule that determines eligibility for treatment? Test whether all eligible units and no ineligible units have received the treatment. If you find noncompliance, you will need to combine RDD with an instrumental variable approach to correct for this \"fuzzy discontinuity.\"\nIs there any evidence that index scores may have been manipulated in order to influence who qualified for the program? Test whether the distribution of the index score is smooth at the cutoff point. If you fi nd evidence of \"bunching\" of index scores either above or below the cutoff point, this might indicate manipulation.\nIs the cutoff unique to the program being evaluated, or is the cutoff used by other programs as well?\n\nFor a review of practical issues in implementing RDD, see Imbens and Lemieux (2008)"
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#how-is-it-performed",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#how-is-it-performed",
    "title": "Regression Discontinuity Design",
    "section": "How is it performed?",
    "text": "How is it performed?\n\\[Y = \\beta_0 + \\beta_1 (Running-Cutoff) + \\beta_2 Treated + \\beta_3 (Running - Cutoff) * Treated + \\varepsilon\\]\nThis is a simple linear approach to regression discontinuity, where \\(\\Running\\) is the running variable which we have centred around the cutoff using \\((Running-Cutoff)\\). This takes a negative value to the left of the cutoff, zero at the cutoff, and a positive value to the right. We’re talking about a sharp regression discontinuity here, so \\(Treated\\) is both an indicator for being treated and an indicator for being above the cutoff - these are the same thing. The model is generally estimated using heteroskedasticity-robust standard errors, as one might expect the discontinuity and general shape of the line we’re fitting to exhibit heteroskedasticity in most cases.We’ll ignore the issue of a bandwidth for now and come back to it later - this regression approach can be applied whether you use all the data or limit yourself to a bandwidth around the cutoff.\nNotice the lack of control variables In most other chapters, when I do this it’s to help focus your attention on the design. Here, it’s very intentional. The whole idea of regression discontinuity is that you have nearly random assignment on either side of the cutoff. You shouldn’t need control variables because the design itself should close any back doors. No open back doors? No need for controls. Adding controls implies you don’t believe the assumptions necessary for the regression discontinuity method to work, and makes the whole thing becomes a bit suspicious to any readers."
  },
  {
    "objectID": "posts/20230618-Regression Discontinuity Design/rdd.html#r-code",
    "href": "posts/20230618-Regression Discontinuity Design/rdd.html#r-code",
    "title": "Regression Discontinuity Design",
    "section": "R code",
    "text": "R code\nLet’s code up some regression discontinuity! I’m going to do this in two ways. First I’m going to run a plain-ol’ ordinary least squares model with a bandwidth and kernel weight applied, with heteroskedasticity-robust standard errors. However, there are a number of other adjustments we’ll be talking about in this chapter, and it can be a good idea to pass this task off to a package that knows what it’s doing. I see no point in showing you code you’re unlikely to use just because we haven’t gotten to the relevant point in the chapter yet. So we’ll be using those specialized commands as well, and I’ll talk later about some of the extra stuff they’re doing for us.\nFor this example we’re going to use data from Government Transfers and Political Support by Manacorda, Miguel, and Vigorito (2011Manacorda, Marco, Edward Miguel, and Andrea Vigorito. 2011. “Government Transfers and Political Support.” American Economic Journal: Applied Economics 3 (3): 1–28.). This paper looks at a large poverty alleviation program in Uruguay which cut a sizeable check to a large portion of the population. They are interested in whether receiving those funds made people more likely to support the newly-installed center-left government that sent them.\nWho got the payments? You had to have an income low enough. But they didn’t exactly use income as a running variable; that might have been too easy to manipulate. Instead, the government used a bunch of factors - housing, work, reported income, schooling - and predicted what your income would be from that. Then, the predicted income was the running variable, and treatment was assigned based on being below a cutoff. About 14% of the population ended up getting payments.\nThe researchers polled a bunch of people near the income cutoff to check their support for the government afterwards. Did people just below the income cutoff support the government more than those just above?\nThe data set we have on government transfers comes with the predicted-income variable pre-centered so the cutoff is at zero. Then, support for the government takes three values: you think they’re better than the previous government (1), the same (1/2) or worse (0). The data only includes individuals near the cutoff - the centered-income variable in the data only goes from -.02 to .02.\nBefore we estimate our model, let’s do a nearly-compulsory graphical regression discontinuity check so we can confirm that there does appear to be some sort of discontinuity where we expect it. This is a “plot of binned means.” There are some preprogrammed ways to do this like rdplot in R or Stata in the rdrobust package or binscatter in Stata from the binscatter package, but this is easy enough that we may as well do it by hand and flex those graphing muscles.\nFor one of these graphs, you generally want to (1) slice the running variable up into bins (making sure the cutoff is the edge between two bins), (2) take the mean of the outcome within each of the bins, and (3) plot the result, with a vertical line at the cutoff so you can see where the cutoff is. Then you’d generally repeat the process with treatment instead of the outcome to produce a graph like below.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ngt &lt;- causaldata::gov_transfers\n\n# Use cut() to create bins, using breaks to make sure it breaks at 0\n# (-15:15)*.02/15 gives 15 breaks from -.02 to .02\nbinned &lt;- gt %&gt;%\n    mutate(Inc_Bins = cut(Income_Centered,\n           breaks = (-15:15)*(.02/15))) %&gt;%\n    group_by(Inc_Bins) %&gt;%\n    summarize(Support = mean(Support),\n    Income = mean(Income_Centered))\n# Taking the mean of Income lets us plot data roughly at the bin midpoints\n\nggplot(binned, aes(x = Income, y = Support)) + \n    geom_line() + \n    # Add a cutoff line\n    geom_vline(aes(xintercept = 0), linetype = 'dashed')\n\n\n\n\n\nNow that we have our graph in mind, we can actually estimate our model. We’ll start doing it with OLS and a second-order polynomial, and then we’ll do a linear model with a triangular kernel weight, limiting the bandwidth around the cutoff to .01 on either side. The bandwidth in this case isn’t super necessary - the data is already limited to .02 on either side around the cutoff - but this is just a demonstration.\nIt’s important to note that the first step of doing regression discontinuity with OLS is to center the running variable around the cutoff. This is as simple as making the variable \\(RunningVariable - Cutoff\\), translated into whatever language you use. The second step would then be to create a “treated” variable \\(RunningVariable &lt; Cutoff\\) (since treatment is applied below the cutoff in this instance). But in this case, the running variable comes pre-centered (\\(IncomeCentred\\)) and the below-cutoff treatment variable is already in the data (\\(Participation\\) ) so I’ll leave those parts out.\n\n\nCode\nlibrary(tidyverse); library(modelsummary)\n\n\nWarning: package 'modelsummary' was built under R version 4.3.1\n\n\nCode\ngt &lt;- causaldata::gov_transfers\n\n# Linear term and a squared term with \"treated\" interactions\nm &lt;- lm(Support ~ Income_Centered*Participation +\n       I(Income_Centered^2)*Participation, data = gt)\n\n# Add a triangular kernel weight\nkweight &lt;- function(x) {\n    # To start at a weight of 0 at x = 0, and impose a bandwidth of .01, \n    # we need a \"slope\" of -1/.01 = 100, \n    # and to go in either direction use the absolute value\n    w &lt;- 1 - 100*abs(x)\n    # if further away than .01, the weight is 0, not negative\n    w &lt;- ifelse(w &lt; 0, 0, w)\n    return(w)\n}\n\n# Run the same model but with the weight\nmw &lt;- lm(Support ~ Income_Centered*Participation, data = gt,\n         weights = kweight(Income_Centered))\n\n# See the results with heteroskedasticity-robust SEs\nmsummary(list('Quadratic' = m, 'Linear with Kernel Weight' = mw), \n    stars = c('*' = .1, '**' = .05, '***' = .01), vcov = 'robust')\n\n\nWarning in residuals^2/(1 - diaghat)^2: longer object length is not a multiple\nof shorter object length\n\n\nWarning in residuals^2/(1 - diaghat)^2: longer object length is not a multiple\nof shorter object length\n\n\n\n\n\n\n\nhttps://rdpackages.github.io/\n\n\nCode\nlibrary(tidyverse); library(rdrobust)\n\n\nWarning: package 'rdrobust' was built under R version 4.3.1\n\n\nCode\ngt &lt;- causaldata::gov_transfers\n\n# Estimate regression discontinuity and plot it\nm &lt;- rdrobust(gt$Support, gt$Income_Centered, c = 0)\n\n\nWarning in rdrobust(gt$Support, gt$Income_Centered, c = 0): Mass points\ndetected in the running variable.\n\n\nCode\nsummary(m)\n\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1948\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                 1127          821\nEff. Number of Obs.             291          194\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.005        0.005\nBW bias (b)                   0.010        0.010\nrho (h/b)                     0.509        0.509\nUnique Obs.                     841          639\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.025     0.062     0.396     0.692    [-0.098 , 0.147]     \n        Robust         -         -     0.624     0.533    [-0.097 , 0.188]     \n=============================================================================\n\n\nCode\n# Note, by default, rdrobust and rdplot use different numbers\n# of polynomial terms. You can set the p option to standardize them.\nrdplot(gt$Support, gt$Income_Centered)\n\n\n[1] \"Mass points detected in the running variable.\"\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse); library(fixest); library(modelsummary)\n\n\nWarning: package 'fixest' was built under R version 4.3.1\n\n\nCode\nvet &lt;- causaldata::mortgages\n\n# Create an \"above-cutoff\" variable as the instrument\nvet &lt;- vet %&gt;% mutate(above = qob_minus_kw &gt; 0)\n\n# Impose a bandwidth of 12 quarters on either side\nvet &lt;- vet %&gt;%  filter(abs(qob_minus_kw) &lt; 12)\n\nm &lt;- feols(home_ownership ~\n    nonwhite  | # Control for race\n    bpl + qob | # fixed effect controls\n    qob_minus_kw*vet_wwko ~ # Instrument our standard RDD\n    qob_minus_kw*above, # with being above the cutoff\n    se = 'hetero', # heteroskedasticity-robust SEs\n    data = vet) \n\n# And look at the results\nmsummary(m, stars = c('*' = .1, '**' = .05, '***' = .01))\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse); library(rdrobust)\nvet &lt;- causaldata::mortgages\n\n# It will apply a bandwidth anyway, but having it\n# check the whole bandwidth space will be slow. So let's\n# pre-limit it to a reasonable range of 12 quarters\nvet &lt;- vet %&gt;%\n    filter(abs(qob_minus_kw) &lt;= 12)\n\n# Create our matrix of controls\ncontrols &lt;- vet %&gt;%\n    select(nonwhite, bpl, qob) %&gt;%\n    mutate(qob = factor(qob))\n# and make it a matrix with dummies\nconmatrix &lt;- model.matrix(~., data = controls)\n\n# This is fairly slow due to the controls, beware!\nm &lt;- rdrobust(vet$home_ownership,\n              vet$qob_minus_kw,\n              fuzzy = vet$vet_wwko,\n              c = 0,\n              covs = conmatrix)\n\n\nWarning in rdrobust(vet$home_ownership, vet$qob_minus_kw, fuzzy = vet$vet_wwko,\n: Multicollinearity issue detected in covs. Redundant covariates dropped.\n\n\nWarning in rdrobust(vet$home_ownership, vet$qob_minus_kw, fuzzy = vet$vet_wwko,\n: Mass points detected in the running variable.\n\n\nCode\nsummary(m)\n\n\nCovariate-adjusted Fuzzy RD estimates using local polynomial regression.\n\nNumber of Obs.                56901\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                28776        28125\nEff. Number of Obs.            6911         6756\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   3.387        3.387\nBW bias (b)                   5.354        5.354\nrho (h/b)                     0.633        0.633\nUnique Obs.                      12           12\n\nFirst-stage estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     0.018     0.018     0.964     0.335    [-0.018 , 0.053]     \n        Robust         -         -     2.244     0.025     [0.006 , 0.094]     \n=============================================================================\n\nTreatment effect estimates.\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional    -5.377     5.716    -0.941     0.347   [-16.581 , 5.827]     \n        Robust         -         -     0.513     0.608   [-10.182 , 17.408]    \n=============================================================================\n\n\nHeiss (2020)"
  },
  {
    "objectID": "posts/20230829-Synthetic Control Methods/synthetic-control.html",
    "href": "posts/20230829-Synthetic Control Methods/synthetic-control.html",
    "title": "Synthetic Control Methods",
    "section": "",
    "text": "Use of historical data to construct a ‘synthetic clone’ of a group receiving a particular intervention. Differences between the performance of the actual group and its synthetic clone may be used as evidence that the intervention has had an effect. Most commonly applied to interventions applied at an area level (Treasury 2020)."
  },
  {
    "objectID": "posts/20230829-Synthetic Control Methods/synthetic-control.html#inference",
    "href": "posts/20230829-Synthetic Control Methods/synthetic-control.html#inference",
    "title": "Synthetic Control Methods",
    "section": "Inference",
    "text": "Inference\nFor inference, the method relies on repeating the method for every donor in the donor pool exactly as was done for the treated unit — i.e. generating placebo synthetic controls). By setting generate_placebos = TRUE when initializing the synth pipeline with synthetic_control(), placebo cases are automatically generated when constructing the synthetic control of interest. This makes it easy to explore how unique difference between the observed and synthetic unit is when compared to the placebos.\n\n\nCode\nsmoking_out %&gt;% plot_placebos()\n\n\n\n\n\nNote that the plot_placebos() function automatically prunes any placebos that poorly fit the data in the pre-intervention period. The reason for doing so is purely visual: those units tend to throw off the scale when plotting the placebos. To prune, the function looks at the pre-intervention period mean squared prediction error (MSPE) (i.e. a metric that reflects how well the synthetic control maps to the observed outcome time series in pre-intervention period). If a placebo control has a MSPE that is two times beyond the target case (e.g. “California”), then it’s dropped. To turn off this behavior, set prune = FALSE.\n\n\nCode\nsmoking_out %&gt;% plot_placebos(prune = FALSE)\n\n\n\n\n\nFinally, Adabie et al. 2010 outline a way of constructing Fisher’s Exact P-values by dividing the post-intervention MSPE by the pre-intervention MSPE and then ranking all the cases by this ratio in descending order. A p-value is then constructed by taking the rank/total.1 The idea is that if the synthetic control fits the observed time series well (low MSPE in the pre-period) and diverges in the post-period (high MSPE in the post-period) then there is a meaningful effect due to the intervention. If the intervention had no effect, then the post-period and pre-period should continue to map onto one another fairly well, yielding a ratio close to 1. If the placebo units fit the data similarly, then we can’t reject the hull hypothesis that there is no effect brought about by the intervention.\nThis ratio can be easily plotted using plot_mspe_ratio(), offering insight into the rarity of the case where the intervention actually occurred.\n\n\nCode\nsmoking_out %&gt;% plot_mspe_ratio()\n\n\n\n\n\nFor more specific information, there is a significance table that can be extracted with one of the many grab_ prefix functions.\n\n\nCode\nsmoking_out %&gt;% grab_significance()\n\n\n# A tibble: 39 × 8\n   unit_name      type  pre_mspe post_mspe mspe_ratio  rank fishers_exact_pvalue\n   &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;                &lt;dbl&gt;\n 1 California     Trea…     3.17     392.      124.       1               0.0256\n 2 Georgia        Donor     3.79     179.       47.2      2               0.0513\n 3 Indiana        Donor    25.2      770.       30.6      3               0.0769\n 4 West Virginia  Donor     9.52     284.       29.8      4               0.103 \n 5 Wisconsin      Donor    11.1      268.       24.1      5               0.128 \n 6 Missouri       Donor     3.03      67.8      22.4      6               0.154 \n 7 Texas          Donor    14.4      277.       19.3      7               0.179 \n 8 South Carolina Donor    12.6      234.       18.6      8               0.205 \n 9 Virginia       Donor     9.81      96.4       9.83     9               0.231 \n10 Nebraska       Donor     6.30      52.9       8.40    10               0.256 \n# ℹ 29 more rows\n# ℹ 1 more variable: z_score &lt;dbl&gt;"
  },
  {
    "objectID": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html",
    "href": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html",
    "title": "Iran Sanctions - Sythetic Control",
    "section": "",
    "text": "Code\nlibrary(tidysynth)\nlibrary(wbstats)"
  },
  {
    "objectID": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html#data-and-method",
    "href": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html#data-and-method",
    "title": "Iran Sanctions - Sythetic Control",
    "section": "Data and Method",
    "text": "Data and Method\nData Source: World Bank Development Indicators\nTime Period: 2003 - 2015\nCountries: Algeria, Angola, Bahrain, Ecuador, Egypt, Iran, Israel, Jordan, Lebanon, Morocco, Nigeria, Oman, and Saudi Arabia.\nOutcome Variable: military expenditure (current US$) per capita.\nControl Variables: total population, imports of goods and services (constant US$), GDP per capita (constant US$) and real GDP per capita growth rate.\nSpecial Variables: military spending per capita in years 2010, 2008, 2006 and 2004\n\nData\n\n\nCode\n# Load Data from World Bank\nwb_countries &lt;- wb_countries(\"en\")\ncountries &lt;- c(\n  \"Bahrain\", \"Ecuador\", \"Egypt, Arab Rep.\", \n  \"Iran, Islamic Rep.\", \"Israel\", \"Jordan\", \n  \"Lebanon\", \"Morocco\", \"Nigeria\", \n  \"Oman\",  \"Saudi Arabia\"\n  )\n\ncountry_iso2c &lt;- wb_countries |&gt;\n  dplyr::filter(country %in% countries) |&gt;\n  dplyr::pull(iso2c)\n\nind &lt;- wb_indicators(\"en\", include_archive = FALSE)\nvars &lt;- c(\n  \"MS.MIL.XPND.CD\",\"SP.POP.TOTL\", \n  \"NE.IMP.GNFS.KD\", \"NY.GDP.PCAP.KD\", \n  \"NY.GDP.PCAP.KD.ZG\", \"BM.GSR.GNFS.CD\"\n  )\n\nind |&gt;\n  dplyr::filter(indicator_id %in% vars)\n\n\n# A tibble: 6 × 8\n  indicator_id indicator unit  indicator_desc source_org topics source_id source\n  &lt;chr&gt;        &lt;chr&gt;     &lt;lgl&gt; &lt;chr&gt;          &lt;chr&gt;      &lt;list&gt;     &lt;dbl&gt; &lt;chr&gt; \n1 BM.GSR.GNFS… Imports … NA    Imports of go… Internati… &lt;df&gt;           2 World…\n2 MS.MIL.XPND… Military… NA    Military expe… Stockholm… &lt;df&gt;           2 World…\n3 NE.IMP.GNFS… Imports … NA    Imports of go… World Ban… &lt;df&gt;           2 World…\n4 NY.GDP.PCAP… GDP per … NA    GDP per capit… World Ban… &lt;df&gt;           2 World…\n5 NY.GDP.PCAP… GDP per … NA    Annual percen… World Ban… &lt;df&gt;           2 World…\n6 SP.POP.TOTL  Populati… NA    Total populat… (1) Unite… &lt;df&gt;           2 World…\n\n\nCode\ndf &lt;- wb_data(indicator = vars, country = country_iso2c, start_date = 2002, end_date = 2015)\n\n# Check all variables and countries are in the data frame.\nall(vars %in% names(df)) & all(country_iso2c %in% unique(df$iso2c))\n\n\n[1] TRUE\n\n\nCode\n# Process Data\nsanctions &lt;- df |&gt;\n  dplyr::mutate(\n    milspend_pc =MS.MIL.XPND.CD/SP.POP.TOTL,\n    realgdpgrowth_pc = NY.GDP.PCAP.KD/dplyr::lag(NY.GDP.PCAP.KD,1)-1,\n    country_id = match(iso2c, country_iso2c),\n    imports = ifelse(is.na(NE.IMP.GNFS.KD), BM.GSR.GNFS.CD,NE.IMP.GNFS.KD), ## TODO convert to constant prices.\n    ) |&gt;\n  dplyr::select(\n    country_id, \n    country, \n    year = date, \n    milspend_pc, \n    pop = SP.POP.TOTL, \n    imports,\n    realgdp_pc = NY.GDP.PCAP.KD,\n    realgdpgrowth_pc\n    ) |&gt;\n  dplyr::filter(year&gt;=2003) \n\n\nsanctions.csv\n\n\nMethod\n\n\nCode\nsanctions_out &lt;- sanctions %&gt;%\n  synthetic_control(outcome = milspend_pc, \n                    unit = country, \n                    time = year, \n                    i_unit = \"Iran, Islamic Rep.\", \n                    i_time = 2011, \n                    generate_placebos=T \n                    ) %&gt;%\n  generate_predictor(time_window = 2003:2011,\n                     pop = mean(pop),\n                     imports = mean(imports),\n                     realgdp_pc = mean(realgdp_pc),\n                     realgdpgrowth_pc = mean(realgdpgrowth_pc)) %&gt;%\n  generate_predictor(time_window = 2010,\n                     milspend_2010 = milspend_pc) %&gt;%\n  generate_predictor(time_window = 2008,\n                     milspend_2008 = milspend_pc) %&gt;%\n  generate_predictor(time_window = 2006,\n                     milspend_2006 = milspend_pc) %&gt;%\n  generate_predictor(time_window = 2004,\n                     milspend_2004 = milspend_pc) %&gt;%\n  generate_weights(optimization_window = 2003:2011, \n                   margin_ipop = .02,sigf_ipop = 7,bound_ipop = 6 \n  ) %&gt;%\n  generate_control()\n\n\nOnce the synthetic control is generated, one can easily assess the fit by comparing the trends of the synthetic and observed time series. The idea is that the trends in the pre-intervention period should map closely onto one another.\n\n\nCode\nsanctions_out %&gt;% plot_trends()\n\n\n\n\n\nTo capture the causal quantity (i.e. the difference between the observed and counterfactual), one can plot the differences using plot_differences()\n\n\nCode\nsanctions_out %&gt;% plot_differences()\n\n\n\n\n\nIn addition, one can easily examine the weighting of the units and variables in the fit. This allows one to see which cases were used, in part, to generate the synthetic control.\n\n\nCode\nsanctions_out %&gt;% plot_weights()\n\n\n\n\n\nAnother useful way of evaluating the synthetic control is to look at how comparable the synthetic control is to the observed covariates of the treated unit.\n\n\nCode\nsanctions_out %&gt;% grab_balance_table()\n\n\n# A tibble: 8 × 4\n  variable         `Iran, Islamic Rep.` synthetic_Iran, Islamic R…¹ donor_sample\n  &lt;chr&gt;                           &lt;dbl&gt;                       &lt;dbl&gt;        &lt;dbl&gt;\n1 imports                      1.66e+11                    5.34e+10     4.94e+10\n2 pop                          7.22e+ 7                    5.50e+ 7     3.25e+ 7\n3 realgdp_pc                   5.03e+ 3                    4.86e+ 3     1.16e+ 4\n4 realgdpgrowth_pc             2.89e- 2                    3.20e- 2     2.48e- 2\n5 milspend_2010                1.80e+ 2                    1.74e+ 2     6.17e+ 2\n6 milspend_2008                1.51e+ 2                    1.49e+ 2     5.94e+ 2\n7 milspend_2006                1.23e+ 2                    1.06e+ 2     4.77e+ 2\n8 milspend_2004                7.59e+ 1                    8.14e+ 1     4.16e+ 2\n# ℹ abbreviated name: ¹​`synthetic_Iran, Islamic Rep.`"
  },
  {
    "objectID": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html#inference",
    "href": "posts/20230828-Iran Sanctions SCM/iran-sanctions-scm.html#inference",
    "title": "Iran Sanctions - Sythetic Control",
    "section": "Inference",
    "text": "Inference\nFor inference, the method relies on repeating the method for every donor in the donor pool exactly as was done for the treated unit — i.e. generating placebo synthetic controls). By setting generate_placebos = TRUE when initializing the synth pipeline with synthetic_control(), placebo cases are automatically generated when constructing the synthetic control of interest. This makes it easy to explore how unique difference between the observed and synthetic unit is when compared to the placebos.\n\n\nCode\nsanctions_out %&gt;% plot_placebos()\n\n\n\n\n\nNote that the plot_placebos() function automatically prunes any placebos that poorly fit the data in the pre-intervention period. The reason for doing so is purely visual: those units tend to throw off the scale when plotting the placebos. To prune, the function looks at the pre-intervention period mean squared prediction error (MSPE) (i.e. a metric that reflects how well the synthetic control maps to the observed outcome time series in pre-intervention period). If a placebo control has a MSPE that is two times beyond the target case (e.g. “California”), then it’s dropped. To turn off this behavior, set prune = FALSE.\n\n\nCode\nsanctions_out %&gt;% plot_placebos(prune = FALSE)\n\n\n\n\n\nFinally, Adabie et al. 2010 outline a way of constructing Fisher’s Exact P-values by dividing the post-intervention MSPE by the pre-intervention MSPE and then ranking all the cases by this ratio in descending order. A p-value is then constructed by taking the rank/total.1 The idea is that if the synthetic control fits the observed time series well (low MSPE in the pre-period) and diverges in the post-period (high MSPE in the post-period) then there is a meaningful effect due to the intervention. If the intervention had no effect, then the post-period and pre-period should continue to map onto one another fairly well, yielding a ratio close to 1. If the placebo units fit the data similarly, then we can’t reject the hull hypothesis that there is no effect brought about by the intervention.\nThis ratio can be easily plotted using plot_mspe_ratio(), offering insight into the rarity of the case where the intervention actually occurred.\n\n\nCode\nsanctions_out %&gt;% plot_mspe_ratio()\n\n\n\n\n\nFor more specific information, there is a significance table that can be extracted with one of the many grab_ prefix functions.\n\n\nCode\nsanctions_out %&gt;% grab_significance()\n\n\n# A tibble: 11 × 8\n   unit_name      type  pre_mspe post_mspe mspe_ratio  rank fishers_exact_pvalue\n   &lt;chr&gt;          &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;                &lt;dbl&gt;\n 1 Iran, Islamic… Trea…    115.      8667.      75.6      1               0.0909\n 2 Jordan         Donor    284.     13107.      46.1      2               0.182 \n 3 Saudi Arabia   Donor  16332.    706684.      43.3      3               0.273 \n 4 Oman           Donor  13643.    540223.      39.6      4               0.364 \n 5 Lebanon        Donor    218.      5839.      26.8      5               0.455 \n 6 Ecuador        Donor    118.      2857.      24.3      6               0.545 \n 7 Morocco        Donor     21.7      210.       9.69     7               0.636 \n 8 Bahrain        Donor  10509.     77008.       7.33     8               0.727 \n 9 Egypt, Arab R… Donor     57.6      365.       6.34     9               0.818 \n10 Israel         Donor 107344.    156612.       1.46    10               0.909 \n11 Nigeria        Donor   1072.      1503.       1.40    11               1     \n# ℹ 1 more variable: z_score &lt;dbl&gt;"
  }
]